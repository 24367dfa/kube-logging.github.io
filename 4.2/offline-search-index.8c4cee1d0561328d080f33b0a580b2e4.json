[{"body":"YAML files for simple logging flows with filter examples.\nGeoIP filter Parser and tag normalizer Dedot filter Multiple format ","categories":"","description":"","excerpt":"YAML files for simple logging flows with filter examples.\nGeoIP filter …","ref":"/4.2/docs/examples/filters-in-flows/","tags":"","title":"Filter examples in Flows"},{"body":"The Logging operator supports Fluentd and syslog-ng as log forwarders. The log forwarder instance receives, filters, and transforms the incoming the logs, and transfers them to one or more destination outputs. Which one to use depends on your logging requirements.\nThe following points help you decide which forwarder to use.\nThe forwarders support different outputs. If the output you want to use is supported only by one forwarder, use that. If the volume of incoming log messages is high, use syslog-ng, as its multithreaded processing provides higher performance. If you have lots of logging flows or need complex routing or log message processing, use syslog-ng. Note: Depending on which log forwarder you use, some of the CRDs you have to create and configure are different.\nsyslog-ng is supported only in Logging operator 4.0 or newer.\n","categories":"","description":"","excerpt":"The Logging operator supports Fluentd and syslog-ng as log forwarders. …","ref":"/4.2/docs/configuration/fluentd-vs-syslog-ng/","tags":"","title":"Which log forwarder to use"},{"body":"Outputs are the destinations where your log forwarder sends the log messages, for example, to Sumo Logic, or to a file. Depending on which log forwarder you use, you have to configure different custom resources.\nFluentd outputs The Output resource defines an output where your Fluentd Flows can send the log messages. The output is a namespaced resource which means only a Flow within the same namespace can access it. You can use secrets in these definitions, but they must also be in the same namespace. Outputs are the final stage for a logging flow. You can define multiple outputs and attach them to multiple flows. ClusterOutput defines an Output without namespace restrictions. It is only evaluated in the controlNamespace by default unless allowClusterResourcesFromAllNamespaces is set to true. Note: Flow can be connected to Output and ClusterOutput, but ClusterFlow can be attached only to ClusterOutput.\nFor the details of the supported output plugins, see Outputs. For the details of Output custom resource, see OutputSpec. For the details of ClusterOutput custom resource, see ClusterOutput. Fluentd S3 output example The following snippet defines an Amazon S3 bucket as an output.\napiVersion: logging.banzaicloud.io/v1beta1 kind: Output metadata: name: s3-output-sample spec: s3: aws_key_id: valueFrom: secretKeyRef: name: s3-secret key: awsAccessKeyId namespace: default aws_sec_key: valueFrom: secretKeyRef: name: s3-secret key: awsSecretAccessKey namespace: default s3_bucket: example-logging-bucket s3_region: eu-west-1 path: logs/${tag}/%Y/%m/%d/ buffer: timekey: 1m timekey_wait: 10s timekey_use_utc: true syslog-ng outputs The SyslogNGOutput resource defines an output for syslog-ng where your SyslogNGFlows can send the log messages. The output is a namespaced resource which means only a SyslogNGFlow within the same namespace can access it. You can use secrets in these definitions, but they must also be in the same namespace. Outputs are the final stage for a logging flow. You can define multiple SyslogNGoutputs and attach them to multiple SyslogNGFlows. SyslogNGClusterOutput defines a SyslogNGOutput without namespace restrictions. It is only evaluated in the controlNamespace by default unless allowClusterResourcesFromAllNamespaces is set to true. Note: SyslogNGFlow can be connected to SyslogNGOutput and SyslogNGClusterOutput, but SyslogNGClusterFlow can be attached only to SyslogNGClusterOutput.\nRFC5424 syslog-ng output example The following example defines a simple SyslogNGOutput resource that sends the logs to the specified syslog server using the RFC5424 Syslog protocol in a TLS-encrypted connection.\napiVersion: logging.banzaicloud.io/v1beta1 kind: SyslogNGOutput metadata: name: syslog-output namespace: default spec: syslog: host: 10.20.9.89 port: 601 template: \"$(format-json --subkeys json. --exclude json.kubernetes.labels.* json.kubernetes.labels=literal($(format-flat-json --subkeys json.kubernetes.labels.)))\\n\" tls: ca_file: mountFrom: secretKeyRef: key: ca.crt name: syslog-tls-cert cert_file: mountFrom: secretKeyRef: key: tls.crt name: syslog-tls-cert key_file: mountFrom: secretKeyRef: key: tls.key name: syslog-tls-cert transport: tls For the details of the supported output plugins, see syslog-ng outputs. For the details of SyslogNGOutput custom resource, see SyslogNGOutputSpec. For the details of SyslogNGClusterOutput custom resource, see SyslogNGClusterOutput. ","categories":"","description":"","excerpt":"Outputs are the destinations where your log forwarder sends the log …","ref":"/4.2/docs/configuration/output/","tags":"","title":"Output and ClusterOutput"},{"body":"\nThis guide describes how to collect application and container logs in Kubernetes using the Logging operator, and how to send them to CloudWatch.\nThe following figure gives you an overview about how the system works. The Logging operator collects the logs from the application, selects which logs to forward to the output, and sends the selected log messages to the output. For more details about the Logging operator, see the Logging operator overview.\nDeploy the Logging operator and a demo Application Install the Logging operator and a demo application using Helm.\nDeploy the Logging operator with Helm To install the Logging operator using Helm, complete these steps.\nNote: For the Helm-based installation you need Helm v3.2.1 or later.\nAdd the chart repository of the Logging operator using the following commands:\nhelm repo add kube-logging https://kube-logging.dev/helm-charts helm repo update Install the Logging operator.\nhelm upgrade --install --wait --create-namespace --namespace logging logging-operator kube-logging/logging-operator Create AWS secret\nIf you have your $AWS_ACCESS_KEY_ID and $AWS_SECRET_ACCESS_KEY set you can use the following snippet.\nkubectl -n logging create secret generic logging-cloudwatch --from-literal \"awsAccessKeyId=$AWS_ACCESS_KEY_ID\" --from-literal \"awsSecretAccessKey=$AWS_SECRET_ACCESS_KEY\" Or set up the secret manually.\nkubectl -n logging apply -f - \u003c\u003c\"EOF\" apiVersion: v1 kind: Secret metadata: name: logging-cloudwatch type: Opaque data: awsAccessKeyId: \u003cbase64encoded\u003e awsSecretAccessKey: \u003cbase64encoded\u003e EOF Create the logging resource.\nkubectl -n logging apply -f - \u003c\u003c\"EOF\" apiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: default-logging-simple spec: fluentd: {} fluentbit: {} controlNamespace: logging EOF Note: You can use the ClusterOutput and ClusterFlow resources only in the controlNamespace.\nCreate an CloudWatch output definition.\nkubectl -n logging apply -f - \u003c\u003c\"EOF\" apiVersion: logging.banzaicloud.io/v1beta1 kind: Output metadata: name: cloudwatch-output namespace: logging spec: cloudwatch: aws_key_id: valueFrom: secretKeyRef: name: logging-cloudwatch key: awsAccessKeyId aws_sec_key: valueFrom: secretKeyRef: name: logging-cloudwatch key: awsSecretAccessKey log_group_name: operator-log-group log_stream_name: operator-log-stream region: us-east-1 auto_create_stream: true buffer: timekey: 30s timekey_wait: 30s timekey_use_utc: true EOF Note: In production environment, use a longer timekey interval to avoid generating too many objects.\nCreate a flow resource.\nkubectl -n logging apply -f - \u003c\u003c\"EOF\" apiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: cloudwatch-flow spec: filters: - tag_normaliser: {} - parser: remove_key_name_field: true reserve_data: true parse: type: nginx match: - select: labels: app.kubernetes.io/name: log-generator localOutputRefs: - cloudwatch-output EOF Install log-generator to produce logs with the label app.kubernetes.io/name: log-generator\nhelm upgrade --install --wait --create-namespace --namespace logging log-generator kube-logging/log-generator Validate your deployment.\nValidate the deployment If you don’t get the expected result you can find help in the troubleshooting section.\n","categories":"","description":"","excerpt":"\nThis guide describes how to collect application and container logs in …","ref":"/4.2/docs/examples/cloudwatch-nginx/","tags":"","title":"Store Nginx Access Logs in Amazon CloudWatch with Logging Operator"},{"body":"Flows route the selected log messages to the specified outputs. Depending on which log forwarder you use, you can use different filters and outputs, and have to configure different custom resources.\nFluentd flows Flow defines a logging flow for Fluentd with filters and outputs.\nThe Flow is a namespaced resource, so only logs from the same namespaces are collected. You can specify match statements to select or exclude logs according to Kubernetes labels, container and host names. (Match statements are evaluated in the order they are defined and processed only until the first matching select or exclude rule applies.) For detailed examples on using the match statement, see log routing.\nYou can define one or more filters within a Flow. Filters can perform various actions on the logs, for example, add additional data, transform the logs, or parse values from the records. The filters in the flow are applied in the order in the definition. You can find the list of supported filters here.\nAt the end of the Flow, you can attach one or more outputs, which may also be Output or ClusterOutput resources.\nFlow resources are namespaced, the selector only select Pod logs within namespace. ClusterFlow defines a Flow without namespace restrictions. It is also only effective in the controlNamespace. ClusterFlow selects logs from ALL namespace.\nThe following example transforms the log messages from the default namespace and sends them to an S3 output.\napiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: flow-sample namespace: default spec: filters: - parser: remove_key_name_field: true parse: type: nginx - tag_normaliser: format: ${namespace_name}.${pod_name}.${container_name} localOutputRefs: - s3-output match: - select: labels: app: nginx Note: In a multi-cluster setup you cannot easily determine which cluster the logs come from. You can append your own labels to each log using the record modifier filter.\nFor the details of Flow custom resource, see FlowSpec. For the details of ClusterFlow custom resource, see ClusterFlow. For details on selecting messages, see Routing your logs with Fluentd match directives See the list of supported filters. syslog-ng flows SyslogNGFlow defines a logging flow for syslog-ng with filters and outputs.\nsyslog-ng is supported only in Logging operator 4.0 or newer.\nThe Flow is a namespaced resource, so only logs from the same namespaces are collected. You can specify match statements to select or exclude logs according to Kubernetes labels, container and host names. For detailed examples on using the match statement, see log routing with syslog-ng.\nYou can define one or more filters within a Flow. Filters can perform various actions on the logs, for example, add additional data, transform the logs, or parse values from the records. The filters in the flow are applied in the order in the definition. You can find the list of supported filters here.\nAt the end of the Flow, you can attach one or more outputs, which may also be Output or ClusterOutput resources.\nSyslogNGFlow resources are namespaced, the selector only select Pod logs within namespace. SyslogNGClusterFlow defines a SyslogNGFlow without namespace restrictions. It is also only effective in the controlNamespace. SyslogNGClusterFlow selects logs from ALL namespace.\nThe following example selects only messages sent by the log-generator application and forwards them to a syslog output.\napiVersion: logging.banzaicloud.io/v1beta1 kind: SyslogNGFlow metadata: name: TestFlow namespace: default spec: match: and: - regexp: value: json.kubernetes.labels.app.kubernetes.io/instance pattern: log-generator type: string - regexp: value: json.kubernetes.labels.app.kubernetes.io/name pattern: log-generator type: string localOutputRefs: - syslog-output For the details of the SyslogNGFlow custom resource, see SyslogNGFlowSpec. For the details of the SyslogNGClusterFlow custom resource, see SyslogNGClusterFlow. For details on selecting messages, see Routing your logs with syslog-ng See the list of supported filters. ","categories":"","description":"","excerpt":"Flows route the selected log messages to the specified outputs. …","ref":"/4.2/docs/configuration/flow/","tags":"","title":"Flow and ClusterFlow"},{"body":" Caution: The master branch is under heavy development. Use releases instead of the master branch to get stable software.\nPrerequisites Logging operator requires Kubernetes v1.14.x or later. For the Helm-based installation you need Helm v3.2.1 or later. Deploy Logging operator with Helm To install the Logging operator using Helm, complete these steps.\nNote: For the Helm-based installation you need Helm v3.2.1 or later.\nAdd the chart repository of the Logging operator using the following commands:\nhelm repo add kube-logging https://kube-logging.dev/helm-charts helm repo update Install the Logging operator.\nhelm upgrade --install --wait --create-namespace --namespace logging logging-operator kube-logging/logging-operator You can install the logging resource with built-in TLS generation using the Helm chart.\nValidate the deployment To verify that the installation was successful, complete the following steps.\nCheck the status of the pods. You should see a new logging-operator pod.\n$ kubectl -n logging get pods NAME READY STATUS RESTARTS AGE logging-logging-operator-599c9cf846-5nw2n 1/1 Running 0 52s Check the CRDs. You should see the following five new CRDs.\n$ kubectl get crd NAME CREATED AT clusterflows.logging.banzaicloud.io 2019-11-01T21:30:18Z clusteroutputs.logging.banzaicloud.io 2019-11-01T21:30:18Z flows.logging.banzaicloud.io 2019-11-01T21:30:18Z loggings.logging.banzaicloud.io 2019-11-01T21:30:18Z outputs.logging.banzaicloud.io 2019-11-01T21:30:18Z ","categories":"","description":"","excerpt":" Caution: The master branch is under heavy development. Use releases …","ref":"/4.2/docs/install/","tags":"","title":"Install"},{"body":"Kubernetes events are objects that provide insight into what is happening inside a cluster, such as what decisions were made by the scheduler or why some pods were evicted from the node. Event tailer listens for Kubernetes events and transmits their changes to stdout, so the Logging operator can process them.\nThe operator handles this CR and generates the following required resources:\nServiceAccount: new account for event-tailer ClusterRole: sets the event-tailer's roles ClusterRoleBinding: links the account with the roles ConfigMap: contains the configuration for the event-tailer pod StatefulSet: manages the lifecycle of the event-tailer pod, which uses the banzaicloud/eventrouter:v0.1.0 image to tail events Create event tailer The simplest way to init an event-tailer is to create a new event-tailer resource with a name and controlNamespace field specified. The following command creates an event tailer called sample:\nkubectl apply -f - \u003c\u003cEOF apiVersion: logging-extensions.banzaicloud.io/v1alpha1 kind: EventTailer metadata: name: sample spec: controlNamespace: default EOF Check that the new object has been created by running:\nkubectl get eventtailer Expected output:\nNAME AGE sample 22m You can see the events in JSON format by checking the log of the event-tailer pod. This way Logging operator can collect the events, and handle them as any other log. Run:\nkubectl logs -l app.kubernetes.io/instance=sample-event-tailer | head -1 | jq The output should be similar to:\n{ \"verb\": \"UPDATED\", \"event\": { \"metadata\": { \"name\": \"kube-scheduler-kind-control-plane.17145dad77f0e528\", \"namespace\": \"kube-system\", \"uid\": \"c2416fa6-7b7f-4a7d-a5f1-b2f2241bd599\", \"resourceVersion\": \"424\", \"creationTimestamp\": \"2022-09-13T08:19:22Z\", \"managedFields\": [ { \"manager\": \"kube-controller-manager\", \"operation\": \"Update\", \"apiVersion\": \"v1\", \"time\": \"2022-09-13T08:19:22Z\" } ] }, \"involvedObject\": { \"kind\": \"Pod\", \"namespace\": \"kube-system\", \"name\": \"kube-scheduler-kind-control-plane\", \"uid\": \"7bd2c626-84f2-49c3-8e8e-8a7c0514b686\", \"apiVersion\": \"v1\", \"resourceVersion\": \"322\" }, \"reason\": \"NodeNotReady\", \"message\": \"Node is not ready\", \"source\": { \"component\": \"node-controller\" }, \"firstTimestamp\": \"2022-09-13T08:19:22Z\", \"lastTimestamp\": \"2022-09-13T08:19:22Z\", \"count\": 1, \"type\": \"Warning\", \"eventTime\": null, \"reportingComponent\": \"\", \"reportingInstance\": \"\" },... Once you have an event-tailer, you can bind your events to a specific logging flow. The following example configures a flow to route the previously created sample-eventtailer to the sample-output.\nkubectl apply -f - \u003c\u003cEOF apiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: eventtailer-flow namespace: default spec: filters: - tag_normaliser: {} match: # keeps data matching to label, the rest of the data will be discarded by this flow implicitly - select: labels: app.kubernetes.io/name: sample-event-tailer outputRefs: - sample-output EOF Delete event tailer To remove an unwanted tailer, delete the related event-tailer custom resource. This terminates the event-tailer pod. For example, run the following command to delete the event tailer called sample:\nkubectl delete eventtailer sample \u0026\u0026 kubectl get pod Expected output:\neventtailer.logging-extensions.banzaicloud.io \"sample\" deleted NAME READY STATUS RESTARTS AGE sample-event-tailer-0 1/1 Terminating 0 12s Persist event logs Event-tailer supports persist mode. In this case, the logs generated from events are stored on a persistent volume. Add the following configuration to your event-tailer spec. In this example, the event tailer is called sample:\nkubectl apply -f - \u003c\u003cEOF apiVersion: logging-extensions.banzaicloud.io/v1alpha1 kind: EventTailer metadata: name: sample spec: controlNamespace: default positionVolume: pvc: spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi volumeMode: Filesystem EOF Logging operator manages the persistent volume of event-tailer automatically, you don’t have any further task with it. To check that the persistent volume has been created, run:\nkubectl get pvc \u0026\u0026 kubectl get pv The output should be similar to:\nNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE sample-event-tailer-sample-event-tailer-0 Bound pvc-6af02cb2-3a62-4d24-8201-dc749034651e 1Gi RWO standard 43s NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-6af02cb2-3a62-4d24-8201-dc749034651e 1Gi RWO Delete Bound default/sample-event-tailer-sample-event-tailer-0 standard 42s Configuration options For the detailed list of configuration options, see the EventTailer CRD reference.\n","categories":"","description":"","excerpt":"Kubernetes events are objects that provide insight into what is …","ref":"/4.2/docs/configuration/extensions/kubernetes-event-tailer/","tags":"","title":"Kubernetes Event Tailer"},{"body":"The logging resource defines the logging infrastructure for your cluster that collects and transports your log messages, and also contains configurations for the Fluent Bit log collector and the Fluentd and syslog-ng log forwarders. It also establishes the controlNamespace, the administrative namespace of the Logging operator. The Fluentd and syslog-ng statefulsets and the Fluent Bit daemonset are deployed in this namespace, and global resources like ClusterOutput and ClusterFlow are evaluated only in this namespace by default - they are ignored in any other namespace unless allowClusterResourcesFromAllNamespaces is set to true.\nYou can define multiple logging resources if needed, for example, if you want to run multiple Fluentd instances with separate configurations.\nYou can customize the configuration of Fluentd, syslog-ng, and Fluent Bit in the logging resource. It also declares watchNamespaces if applicable to narrow down the namespaces in which the logging operator should evaluate and incorporate Flow and Output resources into fluentd’s configuration.\nYou can install a logging resource with built-in TLS generation using the logging Helm chart.\nYou can customize the following sections of the logging resource:\nGeneric parameters of the logging resource. For the list of available parameters, see LoggingSpec. The fluentd statefulset that Logging operator deploys. For a list of parameters, see FluentdSpec. For examples on customizing the Fluentd configuration, see Configure Fluentd. The syslogNG statefulset that Logging operator deploys. For a list of parameters, see SyslogNGSpec. For examples on customizing the Fluentd configuration, see Configure syslog-ng. The fluent-bit that Logging operator deploys. For a list of parameters, see FluentbitSpec. For examples on customizing the Fluent-bit configuration, see Fluent Bit log collector. The following example snippets use the logging namespace. To create this namespace if it does not already exist, run:\nkubectl create ns logging A simple logging example apiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: default-logging-simple namespace: logging spec: fluentd: {} fluentbit: {} controlNamespace: logging Filter namespaces In the following example, the watchNamespaces option is set, so logs are collected only from the prod and test namespaces.\napiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: default-logging-namespaced namespace: logging spec: fluentd: {} fluentbit: {} controlNamespace: logging watchNamespaces: [\"prod\", \"test\"] ","categories":"","description":"","excerpt":"The logging resource defines the logging infrastructure for your …","ref":"/4.2/docs/logging-infrastructure/logging/","tags":"","title":"The Logging custom resource"},{"body":"Aliyun OSS plugin for Fluentd Overview Fluent OSS output plugin buffers event logs in local files and uploads them to OSS periodically in background threads.\nThis plugin splits events by using the timestamp of event logs. For example, a log ‘2019-04-09 message Hello’ is reached, and then another log ‘2019-04-10 message World’ is reached in this order, the former is stored in “20190409.gz” file, and latter in “20190410.gz” file.\nFluent OSS input plugin reads data from OSS periodically.\nThis plugin uses MNS on the same region of the OSS bucket. We must setup MNS and OSS event notification before using this plugin.\nThis document shows how to setup MNS and OSS event notification.\nThis plugin will poll events from MNS queue and extract object keys from these events, and then will read those objects from OSS. More info at https://github.com/aliyun/fluent-plugin-oss\nConfiguration Output Config endpoint (string, required) OSS endpoint to connect to’\nDefault: -\nbucket (string, required) Your bucket name\nDefault: -\naccess_key_id (*secret.Secret, required) Your access key id Secret\nDefault: -\naaccess_key_secret (*secret.Secret, required) Your access secret key Secret\nDefault: -\npath (string, optional) Path prefix of the files on OSS\nDefault: fluent/logs\nupload_crc_enable (bool, optional) Upload crc enabled\nDefault: true\ndownload_crc_enable (bool, optional) Download crc enabled\nDefault: true\nopen_timeout (int, optional) Timeout for open connections\nDefault: 10\nread_timeout (int, optional) Timeout for read response\nDefault: 120\noss_sdk_log_dir (string, optional) OSS SDK log directory\nDefault: /var/log/td-agent\nkey_format (string, optional) The format of OSS object keys\nDefault: %{path}/%{time_slice}%{index}%{thread_id}.%{file_extension}\nstore_as (string, optional) Archive format on OSS: gzip, json, text, lzo, lzma2\nDefault: gzip\nauto_create_bucket (bool, optional) desc ‘Create OSS bucket if it does not exists\nDefault: false\noverwrite (bool, optional) Overwrite already existing path\nDefault: false\ncheck_bucket (bool, optional) Check bucket if exists or not\nDefault: true\ncheck_object (bool, optional) Check object before creation\nDefault: true\nhex_random_length (int, optional) The length of %{hex_random} placeholder(4-16)\nDefault: 4\nindex_format (string, optional) sprintf format for %{index}\nDefault: %d\nwarn_for_delay (string, optional) Given a threshold to treat events as delay, output warning logs if delayed events were put into OSS\nDefault: -\nformat (*Format, optional) Format\nDefault: -\nbuffer (*Buffer, optional) Buffer\nDefault: -\nslow_flush_log_threshold (string, optional) The threshold for chunk flush performance check. Parameter type is float, not time, default: 20.0 (seconds) If chunk flush takes longer time than this threshold, fluentd logs warning message and increases metric fluentd_output_status_slow_flush_count.\nDefault: -\n","categories":"","description":"","excerpt":"Aliyun OSS plugin for Fluentd Overview Fluent OSS output plugin …","ref":"/4.2/docs/configuration/plugins/outputs/oss/","tags":"","title":"Alibaba Cloud"},{"body":"CloudWatch output plugin for Fluentd Overview This plugin has been designed to output logs or metrics to Amazon CloudWatch. More info at https://github.com/fluent-plugins-nursery/fluent-plugin-cloudwatch-logs.\nExample output configurations spec: cloudwatch: aws_key_id: valueFrom: secretKeyRef: name: logging-s3 key: awsAccessKeyId aws_sec_key: valueFrom: secretKeyRef: name: logging-s3 key: awsSecretAccessKey log_group_name: operator-log-group log_stream_name: operator-log-stream region: us-east-1 auto_create_stream true buffer: timekey: 30s timekey_wait: 30s timekey_use_utc: true Configuration Output Config auto_create_stream (bool, optional) Create log group and stream automatically.\nDefault: false\naws_key_id (*secret.Secret, optional) AWS access key id Secret\nDefault: -\naws_sec_key (*secret.Secret, optional) AWS secret key. Secret\nDefault: -\naws_instance_profile_credentials_retries (int, optional) Instance Profile Credentials call retries\nDefault: nil\naws_use_sts (bool, optional) Enable AssumeRoleCredentials to authenticate, rather than the default credential hierarchy. See ‘Cross-Account Operation’ below for more detail.\nDefault: -\naws_sts_role_arn (string, optional) The role ARN to assume when using cross-account sts authentication\nDefault: -\naws_sts_session_name (string, optional) The session name to use with sts authentication\nDefault: ‘fluentd’\nconcurrency (int, optional) Use to set the number of threads pushing data to CloudWatch.\nDefault: 1\nendpoint (string, optional) Use this parameter to connect to the local API endpoint (for testing)\nDefault: -\nhttp_proxy (string, optional) Use to set an optional HTTP proxy\nDefault: -\ninclude_time_key (bool, optional) Include time key as part of the log entry\nDefault: UTC\njson_handler (string, optional) Name of the library to be used to handle JSON data. For now, supported libraries are json (default) and yajl\nDefault: -\nlocaltime (bool, optional) Use localtime timezone for include_time_key output (overrides UTC default)\nDefault: -\nlog_group_aws_tags (string, optional) Set a hash with keys and values to tag the log group resource\nDefault: -\nlog_group_aws_tags_key (string, optional) Specified field of records as AWS tags for the log group\nDefault: -\nlog_group_name (string, optional) Name of log group to store logs\nDefault: -\nlog_group_name_key (string, optional) Specified field of records as log group name\nDefault: -\nlog_rejected_request (string, optional) Output rejected_log_events_info request log.\nDefault: false\nlog_stream_name (string, optional) Name of log stream to store logs\nDefault: -\nlog_stream_name_key (string, optional) Specified field of records as log stream name\nDefault: -\nmax_events_per_batch (int, optional) Maximum number of events to send at once\nDefault: 10000\nmax_message_length (int, optional) Maximum length of the message\nDefault: -\nmessage_keys (string, optional) Keys to send messages as events\nDefault: -\nput_log_events_disable_retry_limit (bool, optional) If true, put_log_events_retry_limit will be ignored\nDefault: -\nput_log_events_retry_limit (int, optional) Maximum count of retry (if exceeding this, the events will be discarded)\nDefault: -\nput_log_events_retry_wait (string, optional) Time before retrying PutLogEvents (retry interval increases exponentially like put_log_events_retry_wait * (2 ^ retry_count))\nDefault: -\nregion (string, required) AWS Region\nDefault: -\nremove_log_group_aws_tags_key (string, optional) Remove field specified by log_group_aws_tags_key\nDefault: -\nremove_log_group_name_key (string, optional) Remove field specified by log_group_name_key\nDefault: -\nremove_log_stream_name_key (string, optional) Remove field specified by log_stream_name_key\nDefault: -\nremove_retention_in_days (string, optional) Remove field specified by retention_in_days\nDefault: -\nretention_in_days (string, optional) Use to set the expiry time for log group when created with auto_create_stream. (default to no expiry)\nDefault: -\nretention_in_days_key (string, optional) Use specified field of records as retention period\nDefault: -\nuse_tag_as_group (bool, optional) Use tag as a group name\nDefault: -\nuse_tag_as_stream (bool, optional) Use tag as a stream name\nDefault: -\nbuffer (*Buffer, optional) Buffer\nDefault: -\nslow_flush_log_threshold (string, optional) The threshold for chunk flush performance check. Parameter type is float, not time, default: 20.0 (seconds) If chunk flush takes longer time than this threshold, fluentd logs warning message and increases metric fluentd_output_status_slow_flush_count.\nDefault: -\nformat (*Format, optional) Format\nDefault: -\n","categories":"","description":"","excerpt":"CloudWatch output plugin for Fluentd Overview This plugin has been …","ref":"/4.2/docs/configuration/plugins/outputs/cloudwatch/","tags":"","title":"Amazon CloudWatch"},{"body":"Amazon Elasticsearch output plugin for Fluentd Overview More info at https://github.com/atomita/fluent-plugin-aws-elasticsearch-service\nExample output configurations spec: awsElasticsearch: logstash_format: true include_tag_key: true tag_key: \"@log_name\" flush_interval: 1s endpoint: url: https://CLUSTER_ENDPOINT_URL region: eu-west-1 access_key_id: value: aws-key secret_access_key: value: aws_secret Configuration Amazon Elasticsearch Send your logs to a Amazon Elasticsearch Service\nflush_interval (string, optional) flush_interval\nDefault: -\nendpoint (*EndpointCredentials, optional) AWS Endpoint Credentials\nDefault: -\nformat (*Format, optional) Format\nDefault: -\nbuffer (*Buffer, optional) Buffer\nDefault: -\n(*ElasticsearchOutput, optional) ElasticSearch\nDefault: -\nEndpoint Credentials endpoint\nregion (string, optional) AWS region. It should be in form like us-east-1, us-west-2. Default nil, which means try to find from environment variable AWS_REGION.\nDefault: -\nurl (string, optional) AWS connection url.\nDefault: -\naccess_key_id (*secret.Secret, optional) AWS access key id. This parameter is required when your agent is not running on EC2 instance with an IAM Role.\nDefault: -\nsecret_access_key (*secret.Secret, optional) AWS secret key. This parameter is required when your agent is not running on EC2 instance with an IAM Role.\nDefault: -\nassume_role_arn (*secret.Secret, optional) Typically, you can use AssumeRole for cross-account access or federation.\nDefault: -\necs_container_credentials_relative_uri (*secret.Secret, optional) Set with AWS_CONTAINER_CREDENTIALS_RELATIVE_URI environment variable value\nDefault: -\nassume_role_session_name (*secret.Secret, optional) AssumeRoleWithWebIdentity https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithWebIdentity.html\nDefault: -\nassume_role_web_identity_token_file (*secret.Secret, optional) AssumeRoleWithWebIdentity https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithWebIdentity.html\nDefault: -\nsts_credentials_region (*secret.Secret, optional) By default, the AWS Security Token Service (AWS STS) is available as a global service, and all AWS STS requests go to a single endpoint at https://sts.amazonaws.com. AWS recommends using Regional AWS STS endpoints instead of the global endpoint to reduce latency, build in redundancy, and increase session token validity. https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_enable-regions.html\nDefault: -\n","categories":"","description":"","excerpt":"Amazon Elasticsearch output plugin for Fluentd Overview More info at …","ref":"/4.2/docs/configuration/plugins/outputs/aws_elasticsearch/","tags":"","title":"Amazon Elasticsearch"},{"body":"Kinesis Firehose output plugin for Fluentd Overview More info at https://github.com/awslabs/aws-fluent-plugin-kinesis#configuration-kinesis_firehose\nExample output configurations spec: kinesisFirehose: delivery_stream_name: example-stream-name region: us-east-1 format: type: json Configuration KinesisStream Send your logs to a Kinesis Stream\ndelivery_stream_name (string, required) Name of the delivery stream to put data.\nDefault: -\nappend_new_line (*bool, optional) If it is enabled, the plugin adds new line character (\\n) to each serialized record. Before appending \\n, plugin calls chomp and removes separator from the end of each record as chomp_record is true. Therefore, you don’t need to enable chomp_record option when you use kinesis_firehose output with default configuration (append_new_line is true). If you want to set append_new_line false, you can choose chomp_record false (default) or true (compatible format with plugin v2). (Default:true)\nDefault: -\naws_key_id (*secret.Secret, optional) AWS access key id. This parameter is required when your agent is not running on EC2 instance with an IAM Role.\nDefault: -\naws_sec_key (*secret.Secret, optional) AWS secret key. This parameter is required when your agent is not running on EC2 instance with an IAM Role.\nDefault: -\naws_ses_token (*secret.Secret, optional) AWS session token. This parameter is optional, but can be provided if using MFA or temporary credentials when your agent is not running on EC2 instance with an IAM Role.\nDefault: -\naws_iam_retries (int, optional) The number of attempts to make (with exponential backoff) when loading instance profile credentials from the EC2 metadata service using an IAM role. Defaults to 5 retries.\nDefault: -\nassume_role_credentials (*KinesisFirehoseAssumeRoleCredentials, optional) Typically, you can use AssumeRole for cross-account access or federation.\nDefault: -\nprocess_credentials (*KinesisFirehoseProcessCredentials, optional) This loads AWS access credentials from an external process.\nDefault: -\nregion (string, optional) AWS region of your stream. It should be in form like us-east-1, us-west-2. Default nil, which means try to find from environment variable AWS_REGION.\nDefault: -\nretries_on_batch_request (int, optional) The plugin will put multiple records to Amazon Kinesis Data Streams in batches using PutRecords. A set of records in a batch may fail for reasons documented in the Kinesis Service API Reference for PutRecords. Failed records will be retried retries_on_batch_request times\nDefault: -\nreset_backoff_if_success (bool, optional) Boolean, default true. If enabled, when after retrying, the next retrying checks the number of succeeded records on the former batch request and reset exponential backoff if there is any success. Because batch request could be composed by requests across shards, simple exponential backoff for the batch request wouldn’t work some cases.\nDefault: -\nbatch_request_max_count (int, optional) Integer, default 500. The number of max count of making batch request from record chunk. It can’t exceed the default value because it’s API limit.\nDefault: -\nbatch_request_max_size (int, optional) Integer. The number of max size of making batch request from record chunk. It can’t exceed the default value because it’s API limit.\nDefault: -\nformat (*Format, optional) Format\nDefault: -\nbuffer (*Buffer, optional) Buffer\nDefault: -\nslow_flush_log_threshold (string, optional) The threshold for chunk flush performance check. Parameter type is float, not time, default: 20.0 (seconds) If chunk flush takes longer time than this threshold, fluentd logs warning message and increases metric fluentd_output_status_slow_flush_count.\nDefault: -\nAssume Role Credentials assume_role_credentials\nrole_arn (string, required) {#assume role-credentials-role_arn} The Amazon Resource Name (ARN) of the role to assume\nDefault: -\nrole_session_name (string, required) {#assume role-credentials-role_session_name} An identifier for the assumed role session\nDefault: -\npolicy (string, optional) {#assume role-credentials-policy} An IAM policy in JSON format\nDefault: -\nduration_seconds (string, optional) {#assume role-credentials-duration_seconds} The duration, in seconds, of the role session (900-3600)\nDefault: -\nexternal_id (string, optional) {#assume role-credentials-external_id} A unique identifier that is used by third parties when assuming roles in their customers’ accounts.\nDefault: -\nProcess Credentials process_credentials\nprocess (string, required) Command more info: https://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/ProcessCredentials.html\nDefault: -\n","categories":"","description":"","excerpt":"Kinesis Firehose output plugin for Fluentd Overview More info at …","ref":"/4.2/docs/configuration/plugins/outputs/kinesis_firehose/","tags":"","title":"Amazon Kinesis"},{"body":"Kinesis Stream output plugin for Fluentd Overview More info at https://github.com/awslabs/aws-fluent-plugin-kinesis#configuration-kinesis_streams\nExample output configurations spec: kinesisStream: stream_name: example-stream-name region: us-east-1 format: type: json Configuration KinesisStream Send your logs to a Kinesis Stream\nstream_name (string, required) Name of the stream to put data.\nDefault: -\npartition_key (string, optional) A key to extract partition key from JSON object. Default nil, which means partition key will be generated randomly.\nDefault: -\naws_key_id (*secret.Secret, optional) AWS access key id. This parameter is required when your agent is not running on EC2 instance with an IAM Role.\nDefault: -\naws_sec_key (*secret.Secret, optional) AWS secret key. This parameter is required when your agent is not running on EC2 instance with an IAM Role.\nDefault: -\naws_ses_token (*secret.Secret, optional) AWS session token. This parameter is optional, but can be provided if using MFA or temporary credentials when your agent is not running on EC2 instance with an IAM Role.\nDefault: -\naws_iam_retries (int, optional) The number of attempts to make (with exponential backoff) when loading instance profile credentials from the EC2 metadata service using an IAM role. Defaults to 5 retries.\nDefault: -\nassume_role_credentials (*KinesisStreamAssumeRoleCredentials, optional) Typically, you can use AssumeRole for cross-account access or federation.\nDefault: -\nprocess_credentials (*KinesisStreamProcessCredentials, optional) This loads AWS access credentials from an external process.\nDefault: -\nregion (string, optional) AWS region of your stream. It should be in form like us-east-1, us-west-2. Default nil, which means try to find from environment variable AWS_REGION.\nDefault: -\nretries_on_batch_request (int, optional) The plugin will put multiple records to Amazon Kinesis Data Streams in batches using PutRecords. A set of records in a batch may fail for reasons documented in the Kinesis Service API Reference for PutRecords. Failed records will be retried retries_on_batch_request times\nDefault: -\nreset_backoff_if_success (bool, optional) Boolean, default true. If enabled, when after retrying, the next retrying checks the number of succeeded records on the former batch request and reset exponential backoff if there is any success. Because batch request could be composed by requests across shards, simple exponential backoff for the batch request wouldn’t work some cases.\nDefault: -\nbatch_request_max_count (int, optional) Integer, default 500. The number of max count of making batch request from record chunk. It can’t exceed the default value because it’s API limit.\nDefault: -\nbatch_request_max_size (int, optional) Integer. The number of max size of making batch request from record chunk. It can’t exceed the default value because it’s API limit.\nDefault: -\nformat (*Format, optional) Format\nDefault: -\nbuffer (*Buffer, optional) Buffer\nDefault: -\nslow_flush_log_threshold (string, optional) The threshold for chunk flush performance check. Parameter type is float, not time, default: 20.0 (seconds) If chunk flush takes longer time than this threshold, fluentd logs warning message and increases metric fluentd_output_status_slow_flush_count.\nDefault: -\nAssume Role Credentials assume_role_credentials\nrole_arn (string, required) The Amazon Resource Name (ARN) of the role to assume\nDefault: -\nrole_session_name (string, required) An identifier for the assumed role session\nDefault: -\npolicy (string, optional) An IAM policy in JSON format\nDefault: -\nduration_seconds (string, optional) The duration, in seconds, of the role session (900-3600)\nDefault: -\nexternal_id (string, optional) A unique identifier that is used by third parties when assuming roles in their customers’ accounts.\nDefault: -\nProcess Credentials process_credentials\nprocess (string, required) Command more info: https://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/ProcessCredentials.html\nDefault: -\n","categories":"","description":"","excerpt":"Kinesis Stream output plugin for Fluentd Overview More info at …","ref":"/4.2/docs/configuration/plugins/outputs/kinesis_stream/","tags":"","title":"Amazon Kinesis"},{"body":"Amazon S3 plugin for Fluentd Overview s3 output plugin buffers event logs in local file and upload it to S3 periodically. This plugin splits files exactly by using the time of event logs (not the time when the logs are received). For example, a log ‘2011-01-02 message B’ is reached, and then another log ‘2011-01-03 message B’ is reached in this order, the former one is stored in “20110102.gz” file, and latter one in “20110103.gz” file.\nExample: S3 Output Deployment\nExample output configurations spec: s3: aws_key_id: valueFrom: secretKeyRef: name: logging-s3 key: awsAccessKeyId aws_sec_key: valueFrom: secretKeyRef: name: logging-s3 key: awsSecretAccessKey s3_bucket: logging-amazon-s3 s3_region: eu-central-1 path: logs/${tag}/%Y/%m/%d/ buffer: timekey: 10m timekey_wait: 30s timekey_use_utc: true Configuration Output Config aws_key_id (*secret.Secret, optional) AWS access key id Secret\nDefault: -\naws_sec_key (*secret.Secret, optional) AWS secret key. Secret\nDefault: -\ncheck_apikey_on_start (string, optional) Check AWS key on start\nDefault: -\ngrant_read (string, optional) Allows grantee to read the object data and its metadata\nDefault: -\noverwrite (string, optional) Overwrite already existing path\nDefault: -\npath (string, optional) Path prefix of the files on S3\nDefault: -\ngrant_write_acp (string, optional) Allows grantee to write the ACL for the applicable object\nDefault: -\ncheck_bucket (string, optional) Check bucket if exists or not\nDefault: -\nsse_customer_key (string, optional) Specifies the customer-provided encryption key for Amazon S3 to use in encrypting data\nDefault: -\nsse_customer_key_md5 (string, optional) Specifies the 128-bit MD5 digest of the encryption key according to RFC 1321\nDefault: -\ncompute_checksums (string, optional) AWS SDK uses MD5 for API request/response by default\nDefault: -\nwarn_for_delay (string, optional) Given a threshold to treat events as delay, output warning logs if delayed events were put into s3\nDefault: -\nuse_bundled_cert (string, optional) Use aws-sdk-ruby bundled cert\nDefault: -\ns3_endpoint (string, optional) Custom S3 endpoint (like minio)\nDefault: -\nssekms_key_id (string, optional) Specifies the AWS KMS key ID to use for object encryption\nDefault: -\ns3_metadata (string, optional) Arbitrary S3 metadata headers to set for the object\nDefault: -\nforce_path_style (string, optional) If true, the bucket name is always left in the request URI and never moved to the host as a sub-domain\nDefault: -\nauto_create_bucket (string, optional) Create S3 bucket if it does not exists\nDefault: -\nindex_format (string, optional) sprintf format for %{index}\nDefault: -\nsignature_version (string, optional) Signature version for API Request (s3,v4)\nDefault: -\nenable_transfer_acceleration (string, optional) If true, S3 Transfer Acceleration will be enabled for uploads. IMPORTANT: You must first enable this feature on your destination S3 bucket\nDefault: -\nssl_verify_peer (string, optional) If false, the certificate of endpoint will not be verified\nDefault: -\nproxy_uri (string, optional) URI of proxy environment\nDefault: -\ngrant_read_acp (string, optional) Allows grantee to read the object ACL\nDefault: -\ncheck_object (string, optional) Check object before creation\nDefault: -\nsse_customer_algorithm (string, optional) Specifies the algorithm to use to when encrypting the object\nDefault: -\nuse_server_side_encryption (string, optional) The Server-side encryption algorithm used when storing this object in S3 (AES256, aws:kms)\nDefault: -\ns3_region (string, optional) S3 region name\nDefault: -\nacl (string, optional) Permission for the object in S3\nDefault: -\ngrant_full_control (string, optional) Allows grantee READ, READ_ACP, and WRITE_ACP permissions on the object\nDefault: -\nhex_random_length (string, optional) The length of %{hex_random} placeholder(4-16)\nDefault: -\ns3_object_key_format (string, optional) The format of S3 object keys (default: %{path}%{time_slice}%{uuid_hash}%{index}.%{file_extension})\nDefault: %{path}%{time_slice}%{uuid_hash}%{index}.%{file_extension}\ns3_bucket (string, required) S3 bucket name\nDefault: -\nstore_as (string, optional) Archive format on S3\nDefault: -\nstorage_class (string, optional) The type of storage to use for the object, for example STANDARD, REDUCED_REDUNDANCY, STANDARD_IA, ONEZONE_IA, INTELLIGENT_TIERING, GLACIER, DEEP_ARCHIVE, OUTPOSTS, GLACIER_IR For a complete list of possible values, see the Amazon S3 API reference.\nDefault: -\naws_iam_retries (string, optional) The number of attempts to load instance profile credentials from the EC2 metadata service using IAM role\nDefault: -\nbuffer (*Buffer, optional) Buffer\nDefault: -\nslow_flush_log_threshold (string, optional) The threshold for chunk flush performance check. Parameter type is float, not time, default: 20.0 (seconds) If chunk flush takes longer time than this threshold, fluentd logs warning message and increases metric fluentd_output_status_slow_flush_count.\nDefault: -\nformat (*Format, optional) Format\nDefault: -\nassume_role_credentials (*S3AssumeRoleCredentials, optional) Assume Role Credentials\nDefault: -\ninstance_profile_credentials (*S3InstanceProfileCredentials, optional) Instance Profile Credentials\nDefault: -\nshared_credentials (*S3SharedCredentials, optional) Shared Credentials\nDefault: -\ncompress (*Compress, optional) Parquet compressor\nDefault: -\noneeye_format (bool, optional) One-eye format trigger\nDefault: false\nclustername (string, optional) Custom cluster name\nDefault: one-eye\nAssume Role Credentials assume_role_credentials\nrole_arn (string, required) {#assume role-credentials-role_arn} The Amazon Resource Name (ARN) of the role to assume\nDefault: -\nrole_session_name (string, required) {#assume role-credentials-role_session_name} An identifier for the assumed role session\nDefault: -\npolicy (string, optional) {#assume role-credentials-policy} An IAM policy in JSON format\nDefault: -\nduration_seconds (string, optional) {#assume role-credentials-duration_seconds} The duration, in seconds, of the role session (900-3600)\nDefault: -\nexternal_id (string, optional) {#assume role-credentials-external_id} A unique identifier that is used by third parties when assuming roles in their customers’ accounts.\nDefault: -\nInstance Profile Credentials instance_profile_credentials\nip_address (string, optional) {#instance profile-credentials-ip_address} IP address\nDefault: 169.254.169.254\nport (string, optional) {#instance profile-credentials-port} Port number\nDefault: 80\nhttp_open_timeout (string, optional) {#instance profile-credentials-http_open_timeout} Number of seconds to wait for the connection to open\nDefault: -\nhttp_read_timeout (string, optional) {#instance profile-credentials-http_read_timeout} Number of seconds to wait for one block to be read\nDefault: -\nretries (string, optional) {#instance profile-credentials-retries} Number of times to retry when retrieving credentials\nDefault: -\nShared Credentials shared_credentials\nprofile_name (string, optional) Profile name. Default to ‘default’ or ENV[‘AWS_PROFILE’]\nDefault: -\npath (string, optional) Path to the shared file.\nDefault: $HOME/.aws/credentials\nParquet compressor parquet compressor\nparquet_compression_codec (string, optional) Parquet compression codec. (uncompressed, snappy, gzip, lzo, brotli, lz4, zstd)\nDefault: snappy\nparquet_page_size (string, optional) Parquet file page size.\nDefault: 8192 bytes\nparquet_row_group_size (string, optional) Parquet file row group size.\nDefault: 128 MB\nrecord_type (string, optional) Record data format type. (avro csv jsonl msgpack tsv msgpack json)\nDefault: msgpack\nschema_type (string, optional) Schema type. (avro, bigquery)\nDefault: avro\nschema_file (string, optional) Path to schema file.\nDefault: -\n","categories":"","description":"","excerpt":"Amazon S3 plugin for Fluentd Overview s3 output plugin buffers event …","ref":"/4.2/docs/configuration/plugins/outputs/s3/","tags":"","title":"Amazon S3"},{"body":"\nThis guide describes how to collect all the container logs in Kubernetes using the Logging operator, and how to send them to Amazon S3.\nThe following figure gives you an overview about how the system works. The Logging operator collects the logs from the application, selects which logs to forward to the output, and sends the selected log messages to the output. For more details about the Logging operator, see the Logging operator overview.\nDeploy the Logging operator Install the Logging operator.\nDeploy the Logging operator with Helm To install the Logging operator using Helm, complete these steps.\nNote: For the Helm-based installation you need Helm v3.2.1 or later.\nAdd the chart repository of the Logging operator using the following commands:\nhelm repo add kube-logging https://kube-logging.dev/helm-charts helm repo update Install the Logging operator into the logging namespace:\nhelm upgrade --install --wait --create-namespace --namespace logging logging-operator kube-logging/logging-operator Validate your deployment.\nConfigure the Logging operator Create AWS secret\nIf you have your $AWS_ACCESS_KEY_ID and $AWS_SECRET_ACCESS_KEY set you can use the following snippet.\nkubectl -n logging create secret generic logging-s3 --from-literal \"awsAccessKeyId=$AWS_ACCESS_KEY_ID\" --from-literal \"awsSecretAccessKey=$AWS_SECRET_ACCESS_KEY\" Or set up the secret manually.\nkubectl -n logging apply -f - \u003c\u003c\"EOF\" apiVersion: v1 kind: Secret metadata: name: logging-s3 type: Opaque data: awsAccessKeyId: \u003cbase64encoded\u003e awsSecretAccessKey: \u003cbase64encoded\u003e EOF Create the logging resource.\nkubectl -n logging apply -f - \u003c\u003c\"EOF\" apiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: default-logging-simple spec: fluentd: {} fluentbit: {} controlNamespace: logging EOF Note: You can use the ClusterOutput and ClusterFlow resources only in the controlNamespace.\nCreate an S3 output definition.\nkubectl -n logging apply -f - \u003c\u003c\"EOF\" apiVersion: logging.banzaicloud.io/v1beta1 kind: Output metadata: name: s3-output namespace: logging spec: s3: aws_key_id: valueFrom: secretKeyRef: name: logging-s3 key: awsAccessKeyId aws_sec_key: valueFrom: secretKeyRef: name: logging-s3 key: awsSecretAccessKey s3_bucket: logging-amazon-s3 s3_region: eu-central-1 path: logs/${tag}/%Y/%m/%d/ buffer: timekey: 10m timekey_wait: 30s timekey_use_utc: true EOF Note: In production environment, use a longer timekey interval to avoid generating too many objects.\nCreate a flow resource. (Mind the label selector in the match that selects a set of pods that we will install in the next step)\nkubectl -n logging apply -f - \u003c\u003c\"EOF\" apiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: s3-flow spec: filters: - tag_normaliser: {} match: - select: labels: app.kubernetes.io/name: log-generator localOutputRefs: - s3-output EOF Install log-generator to produce logs with the label app.kubernetes.io/name: log-generator\nhelm upgrade --install --wait --create-namespace --namespace logging log-generator kube-logging/log-generator Validate your deployment.\nValidate the deployment Check fluentd logs (errors with AWS credentials should be visible here):\nkubectl exec -ti -n logging default-logging-simple-fluentd-0 -- tail -f /fluentd/log/out Check the output. The logs will be available in the bucket on a path like:\n/logs/default.default-logging-simple-fluentbit-lsdp5.fluent-bit/2019/09/11/201909111432_0.gz If you don’t get the expected result you can find help in the troubleshooting section.\n","categories":"","description":"","excerpt":"\nThis guide describes how to collect all the container logs in …","ref":"/4.2/docs/quickstarts/example-s3/","tags":"","title":"Transport all logs into Amazon S3  with Logging operator"},{"body":"Azure Storage output plugin for Fluentd Overview Azure Storage output plugin buffers logs in local file and upload them to Azure Storage periodically. More info at https://github.com/microsoft/fluent-plugin-azure-storage-append-blob\nConfiguration Output Config path (string, optional) Path prefix of the files on Azure\nDefault: -\nazure_storage_account (*secret.Secret, required) Your azure storage account Secret\nDefault: -\nazure_storage_access_key (*secret.Secret, optional) Your azure storage access key Secret\nDefault: -\nazure_storage_sas_token (*secret.Secret, optional) Your azure storage sas token Secret\nDefault: -\nazure_container (string, required) Your azure storage container\nDefault: -\nazure_imds_api_version (string, optional) Azure Instance Metadata Service API Version\nDefault: -\nazure_object_key_format (string, optional) Object key format\nDefault: %{path}%{time_slice}_%{index}.%{file_extension}\nauto_create_container (bool, optional) Automatically create container if not exists\nDefault: true\nformat (string, optional) Compat format type: out_file, json, ltsv (default: out_file)\nDefault: json\nbuffer (*Buffer, optional) Buffer\nDefault: -\nslow_flush_log_threshold (string, optional) The threshold for chunk flush performance check. Parameter type is float, not time, default: 20.0 (seconds) If chunk flush takes longer time than this threshold, fluentd logs warning message and increases metric fluentd_output_status_slow_flush_count.\nDefault: -\n","categories":"","description":"","excerpt":"Azure Storage output plugin for Fluentd Overview Azure Storage output …","ref":"/4.2/docs/configuration/plugins/outputs/azurestore/","tags":"","title":"Azure Storage"},{"body":"Buffer disabled (bool, optional) Disable buffer section (default: false)\nDefault: false,hidden\ntype (string, optional) Fluentd core bundles memory and file plugins. 3rd party plugins are also available when installed.\nDefault: -\ntags (*string, optional) When tag is specified as buffer chunk key, output plugin writes events into chunks separately per tags.\nDefault: tag,time\npath (string, optional) The path where buffer chunks are stored. The ‘*’ is replaced with random characters. It’s highly recommended to leave this default.\nDefault: operator generated\nchunk_limit_size (string, optional) The max size of each chunks: events will be written into chunks until the size of chunks become this size (default: 8MB)\nDefault: 8MB\nchunk_limit_records (int, optional) The max number of events that each chunks can store in it\nDefault: -\ntotal_limit_size (string, optional) The size limitation of this buffer plugin instance. Once the total size of stored buffer reached this threshold, all append operations will fail with error (and data will be lost)\nDefault: -\nqueue_limit_length (int, optional) The queue length limitation of this buffer plugin instance\nDefault: -\nchunk_full_threshold (string, optional) The percentage of chunk size threshold for flushing. output plugin will flush the chunk when actual size reaches chunk_limit_size * chunk_full_threshold (== 8MB * 0.95 in default)\nDefault: -\nqueued_chunks_limit_size (int, optional) Limit the number of queued chunks. If you set smaller flush_interval, e.g. 1s, there are lots of small queued chunks in buffer. This is not good with file buffer because it consumes lots of fd resources when output destination has a problem. This parameter mitigates such situations.\nDefault: -\ncompress (string, optional) If you set this option to gzip, you can get Fluentd to compress data records before writing to buffer chunks.\nDefault: -\nflush_at_shutdown (bool, optional) The value to specify to flush/write all buffer chunks at shutdown, or not\nDefault: -\nflush_mode (string, optional) Default: default (equals to lazy if time is specified as chunk key, interval otherwise) lazy: flush/write chunks once per timekey interval: flush/write chunks per specified time via flush_interval immediate: flush/write chunks immediately after events are appended into chunks\nDefault: -\nflush_interval (string, optional) Default: 60s\nDefault: -\nflush_thread_count (int, optional) The number of threads of output plugins, which is used to write chunks in parallel\nDefault: -\nflush_thread_interval (string, optional) The sleep interval seconds of threads to wait next flush trial (when no chunks are waiting)\nDefault: -\nflush_thread_burst_interval (string, optional) The sleep interval seconds of threads between flushes when output plugin flushes waiting chunks next to next\nDefault: -\ndelayed_commit_timeout (string, optional) The timeout seconds until output plugin decides that async write operation fails\nDefault: -\noverflow_action (string, optional) How output plugin behaves when its buffer queue is full throw_exception: raise exception to show this error in log block: block processing of input plugin to emit events into that buffer drop_oldest_chunk: drop/purge oldest chunk to accept newly incoming chunk\nDefault: -\nretry_timeout (string, optional) The maximum seconds to retry to flush while failing, until plugin discards buffer chunks\nDefault: -\nretry_forever (*bool, optional) If true, plugin will ignore retry_timeout and retry_max_times options and retry flushing forever\nDefault: true\nretry_max_times (int, optional) The maximum number of times to retry to flush while failing\nDefault: -\nretry_secondary_threshold (string, optional) The ratio of retry_timeout to switch to use secondary while failing (Maximum valid value is 1.0)\nDefault: -\nretry_type (string, optional) exponential_backoff: wait seconds will become large exponentially per failures periodic: output plugin will retry periodically with fixed intervals (configured via retry_wait)\nDefault: -\nretry_wait (string, optional) Seconds to wait before next retry to flush, or constant factor of exponential backoff\nDefault: -\nretry_exponential_backoff_base (string, optional) The base number of exponential backoff for retries\nDefault: -\nretry_max_interval (string, optional) The maximum interval seconds for exponential backoff between retries while failing\nDefault: -\nretry_randomize (bool, optional) If true, output plugin will retry after randomized interval not to do burst retries\nDefault: -\ndisable_chunk_backup (bool, optional) Instead of storing unrecoverable chunks in the backup directory, just discard them. This option is new in Fluentd v1.2.6.\nDefault: -\ntimekey (string, required) Output plugin will flush chunks per specified time (enabled when time is specified in chunk keys)\nDefault: 10m\ntimekey_wait (string, optional) Output plugin writes chunks after timekey_wait seconds later after timekey expiration\nDefault: 1m\ntimekey_use_utc (bool, optional) Output plugin decides to use UTC or not to format placeholders using timekey\nDefault: -\ntimekey_zone (string, optional) The timezone (-0700 or Asia/Tokyo) string for formatting timekey placeholders\nDefault: -\n","categories":"","description":"","excerpt":"Buffer disabled (bool, optional) Disable buffer section (default: …","ref":"/4.2/docs/configuration/plugins/outputs/buffer/","tags":"","title":"Buffer"},{"body":"ClusterFlow ClusterFlow is the Schema for the clusterflows API\n(metav1.TypeMeta, required) Default: -\nmetadata (metav1.ObjectMeta, optional) Default: -\nspec (ClusterFlowSpec, optional) Name of the logging cluster to be attached\nDefault: -\nstatus (FlowStatus, optional) Default: -\nClusterMatch select (*ClusterSelect, optional) Default: -\nexclude (*ClusterExclude, optional) Default: -\nClusterSelect namespaces ([]string, optional) Default: -\nlabels (map[string]string, optional) Default: -\nhosts ([]string, optional) Default: -\ncontainer_names ([]string, optional) Default: -\nClusterExclude namespaces ([]string, optional) Default: -\nlabels (map[string]string, optional) Default: -\nhosts ([]string, optional) Default: -\ncontainer_names ([]string, optional) Default: -\nClusterFlowSpec ClusterFlowSpec is the Kubernetes spec for ClusterFlows\nselectors (map[string]string, optional) Deprecated\nDefault: -\nmatch ([]ClusterMatch, optional) Default: -\nfilters ([]Filter, optional) Default: -\nloggingRef (string, optional) Default: -\noutputRefs ([]string, optional) Deprecated\nDefault: -\nglobalOutputRefs ([]string, optional) Default: -\nflowLabel (string, optional) Default: -\nincludeLabelInRouter (*bool, optional) Default: -\nClusterFlowList ClusterFlowList contains a list of ClusterFlow\n(metav1.TypeMeta, required) Default: -\nmetadata (metav1.ListMeta, optional) Default: -\nitems ([]ClusterFlow, required) Default: -\n","categories":"","description":"","excerpt":"ClusterFlow ClusterFlow is the Schema for the clusterflows API …","ref":"/4.2/docs/configuration/crds/v1beta1/clusterflow_types/","tags":"","title":"ClusterFlow"},{"body":"ClusterOutput ClusterOutput is the Schema for the clusteroutputs API\n(metav1.TypeMeta, required) Default: -\nmetadata (metav1.ObjectMeta, optional) Default: -\nspec (ClusterOutputSpec, required) Default: -\nstatus (OutputStatus, optional) Default: -\nClusterOutputSpec ClusterOutputSpec contains Kubernetes spec for ClusterOutput\n(OutputSpec, required) Default: -\nenabledNamespaces ([]string, optional) Default: -\nClusterOutputList ClusterOutputList contains a list of ClusterOutput\n(metav1.TypeMeta, required) Default: -\nmetadata (metav1.ListMeta, optional) Default: -\nitems ([]ClusterOutput, required) Default: -\n","categories":"","description":"","excerpt":"ClusterOutput ClusterOutput is the Schema for the clusteroutputs API …","ref":"/4.2/docs/configuration/crds/v1beta1/clusteroutput_types/","tags":"","title":"ClusterOutput"},{"body":"ImageSpec ImageSpec struct hold information about image specification\nrepository (string, optional) Default: -\ntag (string, optional) Default: -\npullPolicy (string, optional) Default: -\nimagePullSecrets ([]corev1.LocalObjectReference, optional) Default: -\nMetrics Metrics defines the service monitor endpoints\ninterval (string, optional) Default: -\ntimeout (string, optional) Default: -\nport (int32, optional) Default: -\npath (string, optional) Default: -\nserviceMonitor (bool, optional) Default: -\nserviceMonitorConfig (ServiceMonitorConfig, optional) Default: -\nprometheusAnnotations (bool, optional) Default: -\nprometheusRules (bool, optional) Default: -\nBufferMetrics BufferMetrics defines the service monitor endpoints\n(Metrics, required) Default: -\nmount_name (string, optional) Default: -\nServiceMonitorConfig ServiceMonitorConfig defines the ServiceMonitor properties\nadditionalLabels (map[string]string, optional) Default: -\nhonorLabels (bool, optional) Default: -\nrelabelings ([]*v1.RelabelConfig, optional) Default: -\nmetricRelabelings ([]*v1.RelabelConfig, optional) Default: -\nscheme (string, optional) Default: -\ntlsConfig (*v1.TLSConfig, optional) Default: -\nSecurity Security defines Fluentd, FluentbitAgent deployment security properties\nserviceAccount (string, optional) Default: -\nroleBasedAccessControlCreate (*bool, optional) Default: -\npodSecurityPolicyCreate (bool, optional) Default: -\nsecurityContext (*corev1.SecurityContext, optional) Default: -\npodSecurityContext (*corev1.PodSecurityContext, optional) Default: -\nReadinessDefaultCheck ReadinessDefaultCheck Enable default readiness checks\nbufferFreeSpace (bool, optional) Enable default Readiness check it’ll fail if the buffer volume free space exceeds the readinessDefaultThreshold percentage (90%).\nDefault: -\nbufferFreeSpaceThreshold (int32, optional) Default: -\nbufferFileNumber (bool, optional) Default: -\nbufferFileNumberMax (int32, optional) Default: -\ninitialDelaySeconds (int32, optional) Default: -\ntimeoutSeconds (int32, optional) Default: -\nperiodSeconds (int32, optional) Default: -\nsuccessThreshold (int32, optional) Default: -\nfailureThreshold (int32, optional) Default: -\n","categories":"","description":"","excerpt":"ImageSpec ImageSpec struct hold information about image specification …","ref":"/4.2/docs/configuration/crds/v1beta1/common_types/","tags":"","title":"Common"},{"body":"Concat Filter Overview Fluentd Filter plugin to concatenate multiline log separated in multiple events.\nConfiguration Concat key (string, optional) Specify field name in the record to parse. If you leave empty the Container Runtime default will be used.\nDefault: -\nseparator (string, optional) The separator of lines.\nDefault: “\\n”\nn_lines (int, optional) The number of lines. This is exclusive with multiline_start_regex.\nDefault: -\nmultiline_start_regexp (string, optional) The regexp to match beginning of multiline. This is exclusive with n_lines.\nDefault: -\nmultiline_end_regexp (string, optional) The regexp to match ending of multiline. This is exclusive with n_lines.\nDefault: -\ncontinuous_line_regexp (string, optional) The regexp to match continuous lines. This is exclusive with n_lines.\nDefault: -\nstream_identity_key (string, optional) The key to determine which stream an event belongs to.\nDefault: -\nflush_interval (int, optional) The number of seconds after which the last received event log will be flushed. If specified 0, wait for next line forever.\nDefault: -\ntimeout_label (string, optional) The label name to handle events caused by timeout.\nDefault: -\nuse_first_timestamp (bool, optional) Use timestamp of first record when buffer is flushed.\nDefault: False\npartial_key (string, optional) The field name that is the reference to concatenate records\nDefault: -\npartial_value (string, optional) The value stored in the field specified by partial_key that represent partial log\nDefault: -\nkeep_partial_key (bool, optional) If true, keep partial_key in concatenated records\nDefault: False\nuse_partial_metadata (string, optional) Use partial metadata to concatenate multiple records\nDefault: -\nkeep_partial_metadata (string, optional) If true, keep partial metadata\nDefault: -\npartial_metadata_format (string, optional) Input format of the partial metadata (fluentd or journald docker log driver)( docker-fluentd, docker-journald, docker-journald-lowercase)\nDefault: -\nuse_partial_cri_logtag (bool, optional) Use cri log tag to concatenate multiple records\nDefault: -\npartial_cri_logtag_key (string, optional) The key name that is referred to concatenate records on cri log\nDefault: -\npartial_cri_stream_key (string, optional) The key name that is referred to detect stream name on cri log\nDefault: stream\nExample Concat filter configurations apiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: demo-flow spec: filters: - concat: partial_key: \"partial_message\" separator: \"\" n_lines: 10 selectors: {} localOutputRefs: - demo-output Fluentd Config Result \u003cfilter **\u003e @type concat @id test_concat key message n_lines 10 partial_key partial_message \u003c/filter\u003e ","categories":"","description":"","excerpt":"Concat Filter Overview Fluentd Filter plugin to concatenate multiline …","ref":"/4.2/docs/configuration/plugins/filters/concat/","tags":"","title":"Concat"},{"body":"Datadog output plugin for Fluentd Overview It mainly contains a proper JSON formatter and a socket handler that streams logs directly to Datadog - so no need to use a log shipper if you don’t wan’t to. More info at https://github.com/DataDog/fluent-plugin-datadog.\nExample spec: datadog: api_key '\u003cYOUR_API_KEY\u003e' dd_source: '\u003cINTEGRATION_NAME\u003e' dd_tags: '\u003cKEY1:VALUE1\u003e,\u003cKEY2:VALUE2\u003e' dd_sourcecategory: '\u003cYOUR_SOURCE_CATEGORY\u003e' Configuration Output Config api_key (*secret.Secret, required) This parameter is required in order to authenticate your fluent agent. +docLink:“Secret,../secret/”\nDefault: nil\nuse_json (bool, optional) Event format, if true, the event is sent in json format. Othwerwise, in plain text.\nDefault: true\ninclude_tag_key (bool, optional) Automatically include the Fluentd tag in the record.\nDefault: false\ntag_key (string, optional) Where to store the Fluentd tag.\nDefault: “tag”\ntimestamp_key (string, optional) Name of the attribute which will contain timestamp of the log event. If nil, timestamp attribute is not added.\nDefault: “@timestamp”\nuse_ssl (bool, optional) If true, the agent initializes a secure connection to Datadog. In clear TCP otherwise.\nDefault: true\nno_ssl_validation (bool, optional) Disable SSL validation (useful for proxy forwarding)\nDefault: false\nssl_port (string, optional) Port used to send logs over a SSL encrypted connection to Datadog. If use_http is disabled, use 10516 for the US region and 443 for the EU region.\nDefault: “443”\nmax_retries (string, optional) The number of retries before the output plugin stops. Set to -1 for unlimited retries\nDefault: “-1”\nmax_backoff (string, optional) The maximum time waited between each retry in seconds\nDefault: “30”\nuse_http (bool, optional) Enable HTTP forwarding. If you disable it, make sure to change the port to 10514 or ssl_port to 10516\nDefault: true\nuse_compression (bool, optional) Enable log compression for HTTP\nDefault: true\ncompression_level (string, optional) Set the log compression level for HTTP (1 to 9, 9 being the best ratio)\nDefault: “6”\ndd_source (string, optional) This tells Datadog what integration it is\nDefault: nil\ndd_sourcecategory (string, optional) Multiple value attribute. Can be used to refine the source attribute\nDefault: nil\ndd_tags (string, optional) Custom tags with the following format “key1:value1, key2:value2”\nDefault: nil\ndd_hostname (string, optional) Used by Datadog to identify the host submitting the logs.\nDefault: “hostname -f”\nservice (string, optional) Used by Datadog to correlate between logs, traces and metrics.\nDefault: nil\nport (string, optional) Proxy port when logs are not directly forwarded to Datadog and ssl is not used\nDefault: “80”\nhost (string, optional) Proxy endpoint when logs are not directly forwarded to Datadog\nDefault: “http-intake.logs.datadoghq.com”\nbuffer (*Buffer, optional) Buffer\nDefault: -\nslow_flush_log_threshold (string, optional) The threshold for chunk flush performance check. Parameter type is float, not time, default: 20.0 (seconds) If chunk flush takes longer time than this threshold, fluentd logs warning message and increases metric fluentd_output_status_slow_flush_count.\nDefault: -\n","categories":"","description":"","excerpt":"Datadog output plugin for Fluentd Overview It mainly contains a proper …","ref":"/4.2/docs/configuration/plugins/outputs/datadog/","tags":"","title":"Datadog"},{"body":"Dedot Filter Overview Fluentd Filter plugin to de-dot field name for elasticsearch.\nConfiguration DedotFilterConfig de_dot_nested (bool, optional) Will cause the plugin to recurse through nested structures (hashes and arrays), and remove dots in those key-names too.\nDefault: false\nde_dot_separator (string, optional) Separator\nDefault: _\nExample Dedot filter configurations apiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: demo-flow spec: filters: - dedot: de_dot_separator: \"-\" de_dot_nested: true selectors: {} localOutputRefs: - demo-output Fluentd Config Result \u003cfilter **\u003e @type dedot @id test_dedot de_dot_nested true de_dot_separator - \u003c/filter\u003e ","categories":"","description":"","excerpt":"Dedot Filter Overview Fluentd Filter plugin to de-dot field name for …","ref":"/4.2/docs/configuration/plugins/filters/dedot/","tags":"","title":"Dedot"},{"body":"DiskBuffer The parameters of the syslog-ng disk buffer. Using a disk buffer on the output helps avoid message loss in case of a system failure on the destination side. Documentation: https://www.syslog-ng.com/technical-documents/doc/syslog-ng-open-source-edition/3.37/administration-guide/56#TOPIC-1829124\ndisk_buf_size (int64, required) This is a required option. The maximum size of the disk-buffer in bytes. The minimum value is 1048576 bytes.\nDefault: -\nreliable (bool, required) If set to yes, syslog-ng OSE cannot lose logs in case of reload/restart, unreachable destination or syslog-ng OSE crash. This solution provides a slower, but reliable disk-buffer option.\nDefault: -\ncompaction (*bool, optional) Prunes the unused space in the LogMessage representation\nDefault: -\ndir (string, optional) Description: Defines the folder where the disk-buffer files are stored.\nDefault: -\nmem_buf_length (*int64, optional) Use this option if the option reliable() is set to no. This option contains the number of messages stored in overflow queue.\nDefault: -\nmem_buf_size (*int64, optional) Use this option if the option reliable() is set to yes. This option contains the size of the messages in bytes that is used in the memory part of the disk buffer.\nDefault: -\nq_out_size (*int64, optional) The number of messages stored in the output buffer of the destination.\nDefault: -\n","categories":"","description":"","excerpt":"DiskBuffer The parameters of the syslog-ng disk buffer. Using a disk …","ref":"/4.2/docs/configuration/plugins/syslog-ng-outputs/disk_buffer/","tags":"","title":"Disk buffer"},{"body":"Elasticsearch output plugin for Fluentd Overview More info at https://github.com/uken/fluent-plugin-elasticsearch\nExample Deployment: Save all logs to ElasticSearch\nExample output configurations spec: elasticsearch: host: elasticsearch-elasticsearch-cluster.default.svc.cluster.local port: 9200 scheme: https ssl_verify: false ssl_version: TLSv1_2 buffer: timekey: 1m timekey_wait: 30s timekey_use_utc: true Configuration Elasticsearch Send your logs to Elasticsearch\nhost (string, optional) You can specify Elasticsearch host by this parameter.\nDefault: localhost\nport (int, optional) You can specify Elasticsearch port by this parameter.\nDefault: 9200\nhosts (string, optional) You can specify multiple Elasticsearch hosts with separator “,”. If you specify hosts option, host and port options are ignored.\nDefault: -\nuser (string, optional) User for HTTP Basic authentication. This plugin will escape required URL encoded characters within %{} placeholders. e.g. %{demo+}\nDefault: -\npassword (*secret.Secret, optional) Password for HTTP Basic authentication. Secret\nDefault: -\npath (string, optional) Path for HTTP Basic authentication.\nDefault: -\nscheme (string, optional) Connection scheme\nDefault: http\nssl_verify (*bool, optional) Skip ssl verification (default: true)\nDefault: true\nssl_version (string, optional) If you want to configure SSL/TLS version, you can specify ssl_version parameter. [SSLv23, TLSv1, TLSv1_1, TLSv1_2]\nDefault: -\nssl_max_version (string, optional) Specify min/max SSL/TLS version\nDefault: -\nssl_min_version (string, optional) Default: -\nca_file (*secret.Secret, optional) CA certificate\nDefault: -\nclient_cert (*secret.Secret, optional) Client certificate\nDefault: -\nclient_key (*secret.Secret, optional) Client certificate key\nDefault: -\nclient_key_pass (*secret.Secret, optional) Client key password\nDefault: -\nlogstash_format (bool, optional) Enable Logstash log format.\nDefault: false\ninclude_timestamp (bool, optional) Adds a @timestamp field to the log, following all settings logstash_format does, except without the restrictions on index_name. This allows one to log to an alias in Elasticsearch and utilize the rollover API.\nDefault: false\nlogstash_prefix (string, optional) Set the Logstash prefix.\nDefault: logstash\nlogstash_prefix_separator (string, optional) Set the Logstash prefix separator.\nDefault: -\nlogstash_dateformat (string, optional) Set the Logstash date format.\nDefault: %Y.%m.%d\nindex_name (string, optional) The index name to write events to\nDefault: fluentd\ntype_name (string, optional) Set the index type for elasticsearch. This is the fallback if target_type_key is missing.\nDefault: fluentd\npipeline (string, optional) This param is to set a pipeline id of your elasticsearch to be added into the request, you can configure ingest node.\nDefault: -\ntime_key_format (string, optional) The format of the time stamp field (@timestamp or what you specify with time_key). This parameter only has an effect when logstash_format is true as it only affects the name of the index we write to.\nDefault: -\ntime_precision (string, optional) Should the record not include a time_key, define the degree of sub-second time precision to preserve from the time portion of the routed event.\nDefault: -\ntime_key (string, optional) By default, when inserting records in Logstash format, @timestamp is dynamically created with the time at log ingestion. If you’d like to use a custom time, include an @timestamp with your record.\nDefault: -\nutc_index (*bool, optional) By default, the records inserted into index logstash-YYMMDD with UTC (Coordinated Universal Time). This option allows to use local time if you describe utc_index to false.(default: true)\nDefault: true\nsuppress_type_name (*bool, optional) Suppress type name to avoid warnings in Elasticsearch 7.x\nDefault: -\ntarget_index_key (string, optional) Tell this plugin to find the index name to write to in the record under this key in preference to other mechanisms. Key can be specified as path to nested record using dot (’.’) as a separator. https://github.com/uken/fluent-plugin-elasticsearch#target_index_key\nDefault: -\ntarget_type_key (string, optional) Similar to target_index_key config, find the type name to write to in the record under this key (or nested record). If key not found in record - fallback to type_name.\nDefault: fluentd\ntemplate_name (string, optional) The name of the template to define. If a template by the name given is already present, it will be left unchanged, unless template_overwrite is set, in which case the template will be updated.\nDefault: -\ntemplate_file (*secret.Secret, optional) The path to the file containing the template to install. Secret\nDefault: -\ntemplates (string, optional) Specify index templates in form of hash. Can contain multiple templates.\nDefault: -\ncustomize_template (string, optional) Specify the string and its value to be replaced in form of hash. Can contain multiple key value pair that would be replaced in the specified template_file. This setting only creates template and to add rollover index please check the rollover_index configuration.\nDefault: -\nrollover_index (bool, optional) Specify this as true when an index with rollover capability needs to be created. https://github.com/uken/fluent-plugin-elasticsearch#rollover_index\nDefault: false\nindex_date_pattern (*string, optional) Specify this to override the index date pattern for creating a rollover index.\nDefault: now/d\ndeflector_alias (string, optional) Specify the deflector alias which would be assigned to the rollover index created. This is useful in case of using the Elasticsearch rollover API\nDefault: -\nindex_prefix (string, optional) Specify the index prefix for the rollover index to be created.\nDefault: logstash\napplication_name (*string, optional) Specify the application name for the rollover index to be created.\nDefault: default\ntemplate_overwrite (bool, optional) Always update the template, even if it already exists.\nDefault: false\nmax_retry_putting_template (string, optional) You can specify times of retry putting template.\nDefault: 10\nfail_on_putting_template_retry_exceed (*bool, optional) Indicates whether to fail when max_retry_putting_template is exceeded. If you have multiple output plugin, you could use this property to do not fail on fluentd statup.(default: true)\nDefault: true\nfail_on_detecting_es_version_retry_exceed (*bool, optional) fail_on_detecting_es_version_retry_exceed (default: true)\nDefault: true\nmax_retry_get_es_version (string, optional) You can specify times of retry obtaining Elasticsearch version.\nDefault: 15\nrequest_timeout (string, optional) You can specify HTTP request timeout.\nDefault: 5s\nreload_connections (*bool, optional) You can tune how the elasticsearch-transport host reloading feature works.(default: true)\nDefault: true\nreload_on_failure (bool, optional) Indicates that the elasticsearch-transport will try to reload the nodes addresses if there is a failure while making the request, this can be useful to quickly remove a dead node from the list of addresses.\nDefault: false\nreload_after (string, optional) When reload_connections true, this is the integer number of operations after which the plugin will reload the connections. The default value is 10000.\nDefault: -\nresurrect_after (string, optional) You can set in the elasticsearch-transport how often dead connections from the elasticsearch-transport’s pool will be resurrected.\nDefault: 60s\ninclude_tag_key (bool, optional) This will add the Fluentd tag in the JSON record.\nDefault: false\ntag_key (string, optional) This will add the Fluentd tag in the JSON record.\nDefault: tag\nid_key (string, optional) https://github.com/uken/fluent-plugin-elasticsearch#id_key\nDefault: -\nrouting_key (string, optional) Similar to parent_key config, will add _routing into elasticsearch command if routing_key is set and the field does exist in input event.\nDefault: -\nremove_keys (string, optional) https://github.com/uken/fluent-plugin-elasticsearch#remove_keys\nDefault: -\nremove_keys_on_update (string, optional) Remove keys on update will not update the configured keys in elasticsearch when a record is being updated. This setting only has any effect if the write operation is update or upsert.\nDefault: -\nremove_keys_on_update_key (string, optional) This setting allows remove_keys_on_update to be configured with a key in each record, in much the same way as target_index_key works.\nDefault: -\nretry_tag (string, optional) This setting allows custom routing of messages in response to bulk request failures. The default behavior is to emit failed records using the same tag that was provided.\nDefault: -\nwrite_operation (string, optional) The write_operation can be any of: (index,create,update,upsert)\nDefault: index\nreconnect_on_error (bool, optional) Indicates that the plugin should reset connection on any error (reconnect on next send). By default it will reconnect only on “host unreachable exceptions”. We recommended to set this true in the presence of elasticsearch shield.\nDefault: false\nwith_transporter_log (bool, optional) This is debugging purpose option to enable to obtain transporter layer log.\nDefault: false\ncontent_type (string, optional) With content_type application/x-ndjson, elasticsearch plugin adds application/x-ndjson as Content-Profile in payload.\nDefault: application/json\ninclude_index_in_url (bool, optional) With this option set to true, Fluentd manifests the index name in the request URL (rather than in the request body). You can use this option to enforce an URL-based access control.\nDefault: -\ntime_parse_error_tag (string, optional) With logstash_format true, elasticsearch plugin parses timestamp field for generating index name. If the record has invalid timestamp value, this plugin emits an error event to @ERROR label with time_parse_error_tag configured tag.\nDefault: -\nhttp_backend (string, optional) With http_backend typhoeus, elasticsearch plugin uses typhoeus faraday http backend. Typhoeus can handle HTTP keepalive.\nDefault: excon\nprefer_oj_serializer (bool, optional) With default behavior, Elasticsearch client uses Yajl as JSON encoder/decoder. Oj is the alternative high performance JSON encoder/decoder. When this parameter sets as true, Elasticsearch client uses Oj as JSON encoder/decoder.\nDefault: false\nflatten_hashes (bool, optional) Elasticsearch will complain if you send object and concrete values to the same field. For example, you might have logs that look this, from different places: {“people” =\u003e 100} {“people” =\u003e {“some” =\u003e “thing”}} The second log line will be rejected by the Elasticsearch parser because objects and concrete values can’t live in the same field. To combat this, you can enable hash flattening.\nDefault: -\nflatten_hashes_separator (string, optional) Flatten separator\nDefault: -\nvalidate_client_version (bool, optional) When you use mismatched Elasticsearch server and client libraries, fluent-plugin-elasticsearch cannot send data into Elasticsearch.\nDefault: false\nunrecoverable_error_types (string, optional) Default unrecoverable_error_types parameter is set up strictly. Because es_rejected_execution_exception is caused by exceeding Elasticsearch’s thread pool capacity. Advanced users can increase its capacity, but normal users should follow default behavior. If you want to increase it and forcibly retrying bulk request, please consider to change unrecoverable_error_types parameter from default value. Change default value of thread_pool.bulk.queue_size in elasticsearch.yml)\nDefault: -\nverify_es_version_at_startup (*bool, optional) Because Elasticsearch plugin should change behavior each of Elasticsearch major versions. For example, Elasticsearch 6 starts to prohibit multiple type_names in one index, and Elasticsearch 7 will handle only _doc type_name in index. If you want to disable to verify Elasticsearch version at start up, set it as false. When using the following configuration, ES plugin intends to communicate into Elasticsearch 6. (default: true)\nDefault: true\ndefault_elasticsearch_version (string, optional) This parameter changes that ES plugin assumes default Elasticsearch version.\nDefault: 5\ncustom_headers (string, optional) This parameter adds additional headers to request. Example: {“token”:“secret”}\nDefault: {}\napi_key (*secret.Secret, optional) api_key parameter adds authentication header.\nDefault: -\nlog_es_400_reason (bool, optional) By default, the error logger won’t record the reason for a 400 error from the Elasticsearch API unless you set log_level to debug. However, this results in a lot of log spam, which isn’t desirable if all you want is the 400 error reasons. You can set this true to capture the 400 error reasons without all the other debug logs.\nDefault: false\nsuppress_doc_wrap (bool, optional) By default, record body is wrapped by ‘doc’. This behavior can not handle update script requests. You can set this to suppress doc wrapping and allow record body to be untouched.\nDefault: false\nignore_exceptions (string, optional) A list of exception that will be ignored - when the exception occurs the chunk will be discarded and the buffer retry mechanism won’t be called. It is possible also to specify classes at higher level in the hierarchy. For example ignore_exceptions [\"Elasticsearch::Transport::Transport::ServerError\"] will match all subclasses of ServerError - Elasticsearch::Transport::Transport::Errors::BadRequest, Elasticsearch::Transport::Transport::Errors::ServiceUnavailable, etc.\nDefault: -\nexception_backup (*bool, optional) Indicates whether to backup chunk when ignore exception occurs. (default: true)\nDefault: true\nbulk_message_request_threshold (string, optional) Configure bulk_message request splitting threshold size. Default value is 20MB. (20 * 1024 * 1024) If you specify this size as negative number, bulk_message request splitting feature will be disabled.\nDefault: 20MB\nsniffer_class_name (string, optional) The default Sniffer used by the Elasticsearch::Transport class works well when Fluentd has a direct connection to all of the Elasticsearch servers and can make effective use of the _nodes API. This doesn’t work well when Fluentd must connect through a load balancer or proxy. The parameter sniffer_class_name gives you the ability to provide your own Sniffer class to implement whatever connection reload logic you require. In addition, there is a new Fluent::Plugin::ElasticsearchSimpleSniffer class which reuses the hosts given in the configuration, which is typically the hostname of the load balancer or proxy. https://github.com/uken/fluent-plugin-elasticsearch#sniffer-class-name\nDefault: -\nbuffer (*Buffer, optional) Buffer\nDefault: -\nslow_flush_log_threshold (string, optional) The threshold for chunk flush performance check. Parameter type is float, not time, default: 20.0 (seconds) If chunk flush takes longer time than this threshold, fluentd logs warning message and increases metric fluentd_output_status_slow_flush_count.\nDefault: -\nenable_ilm (bool, optional) Enable Index Lifecycle Management (ILM).\nDefault: -\nilm_policy_id (string, optional) Specify ILM policy id.\nDefault: -\nilm_policy (string, optional) Specify ILM policy contents as Hash.\nDefault: -\nilm_policy_overwrite (bool, optional) Specify whether overwriting ilm policy or not.\nDefault: -\ndata_stream_enable (*bool, optional) Use @type elasticsearch_data_stream\nDefault: -\ndata_stream_name (string, optional) You can specify Elasticsearch data stream name by this parameter. This parameter is mandatory for elasticsearch_data_stream. There are some limitations about naming rule. For more details https://www.elastic.co/guide/en/elasticsearch/reference/master/indices-create-data-stream.html#indices-create-data-stream-api-path-params\nDefault: -\ndata_stream_template_name (string, optional) Specify an existing index template for the data stream. If not present, a new template is created and named after the data stream. Further details here https://github.com/uken/fluent-plugin-elasticsearch#configuration---elasticsearch-output-data-stream\nDefault: data_stream_name\ndata_stream_ilm_name (string, optional) Specify an existing ILM policy to be applied to the data stream. If not present, either the specified template’s or a new ILM default policy is applied. Further details here https://github.com/uken/fluent-plugin-elasticsearch#configuration---elasticsearch-output-data-stream\nDefault: data_stream_name\ndata_stream_ilm_policy (string, optional) Specify data stream ILM policy contents as Hash.\nDefault: -\ndata_stream_ilm_policy_overwrite (bool, optional) Specify whether overwriting data stream ilm policy or not.\nDefault: -\n","categories":"","description":"","excerpt":"Elasticsearch output plugin for Fluentd Overview More info at …","ref":"/4.2/docs/configuration/plugins/outputs/elasticsearch/","tags":"","title":"Elasticsearch"},{"body":"ElasticsearchGenId hash_id_key (string, optional) You can specify generated hash storing key.\nDefault: -\ninclude_tag_in_seed (bool, optional) You can specify to use tag for hash generation seed.\nDefault: -\ninclude_time_in_seed (bool, optional) You can specify to use time for hash generation seed.\nDefault: -\nuse_record_as_seed (bool, optional) You can specify to use record in events for hash generation seed. This parameter should be used with record_keys parameter in practice.\nDefault: -\nrecord_keys (string, optional) You can specify keys which are record in events for hash generation seed. This parameter should be used with use_record_as_seed parameter in practice.\nDefault: -\nuse_entire_record (bool, optional) You can specify to use entire record in events for hash generation seed.\nDefault: -\nseparator (string, optional) You can specify separator charactor to creating seed for hash generation.\nDefault: -\nhash_type (string, optional) You can specify hash algorithm. Support algorithms md5, sha1, sha256, sha512. Default: sha1\nDefault: -\n","categories":"","description":"","excerpt":"ElasticsearchGenId hash_id_key (string, optional) You can specify …","ref":"/4.2/docs/configuration/plugins/filters/elasticsearch_genid/","tags":"","title":"ElasticsearchGenId"},{"body":"Enhance K8s Metadata Overview Fluentd Filter plugin to fetch several metadata for a Pod\nConfiguration EnhanceK8s in_namespace_path ([]string, optional) parameters for read/write record\nDefault: [’$.namespace']\nin_pod_path ([]string, optional) Default: [’$.pod’,’$.pod_name']\ndata_type (string, optional) Sumologic data type\nDefault: metrics\nkubernetes_url (string, optional) Kubernetes API URL\nDefault: nil\nclient_cert (secret.Secret, optional) Kubernetes API Client certificate\nDefault: nil\nclient_key (secret.Secret, optional) // Kubernetes API Client certificate key\nDefault: nil\nca_file (secret.Secret, optional) Kubernetes API CA file\nDefault: nil\nsecret_dir (string, optional) Service account directory\nDefault: /var/run/secrets/kubernetes.io/serviceaccount\nbearer_token_file (string, optional) Bearer token path\nDefault: nil\nverify_ssl (*bool, optional) Verify SSL\nDefault: true\ncore_api_versions ([]string, optional) Kubernetes core API version (for different Kubernetes versions)\nDefault: [‘v1’]\napi_groups ([]string, optional) Kubernetes resources api groups\nDefault: [“apps/v1”, “extensions/v1beta1”]\nssl_partial_chain (*bool, optional) if ca_file is for an intermediate CA, or otherwise we do not have the root CA and want to trust the intermediate CA certs we do have, set this to true - this corresponds to the openssl s_client -partial_chain flag and X509_V_FLAG_PARTIAL_CHAIN\nDefault: false\ncache_size (int, optional) Cache size\nDefault: 1000\ncache_ttl (int, optional) Cache TTL\nDefault: 60602\ncache_refresh (int, optional) Cache refresh\nDefault: 60*60\ncache_refresh_variation (int, optional) Cache refresh variation\nDefault: 60*15\nExample EnhanceK8s filter configurations apiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: demo-flow spec: globalFilters: - enhanceK8s: {} Fluentd Config Result \u003cfilter **\u003e @type enhance_k8s_metadata @id test_enhanceK8s \u003c/filter\u003e ","categories":"","description":"","excerpt":"Enhance K8s Metadata Overview Fluentd Filter plugin to fetch several …","ref":"/4.2/docs/configuration/plugins/filters/enhance_k8s/","tags":"","title":"Enhance K8s Metadata"},{"body":"EventTailerSpec EventTailerSpec defines the desired state of EventTailer\ncontrolNamespace (string, required) The resources of EventTailer will be placed into this namespace\nDefault: -\npositionVolume (volume.KubernetesVolume, optional) Volume definition for tracking fluentbit file positions (optional)\nDefault: -\nworkloadMetaOverrides (*types.MetaBase, optional) Override metadata of the created resources\nDefault: -\nworkloadOverrides (*types.PodSpecBase, optional) Override podSpec fields for the given statefulset\nDefault: -\ncontainerOverrides (*types.ContainerBase, optional) Override container fields for the given statefulset\nDefault: -\nEventTailerStatus EventTailerStatus defines the observed state of EventTailer\nEventTailer EventTailer is the Schema for the eventtailers API\n(metav1.TypeMeta, required) Default: -\nmetadata (metav1.ObjectMeta, optional) Default: -\nspec (EventTailerSpec, optional) Default: -\nstatus (EventTailerStatus, optional) Default: -\nEventTailerList EventTailerList contains a list of EventTailer\n(metav1.TypeMeta, required) Default: -\nmetadata (metav1.ListMeta, optional) Default: -\nitems ([]EventTailer, required) Default: -\n","categories":"","description":"","excerpt":"EventTailerSpec EventTailerSpec defines the desired state of …","ref":"/4.2/docs/configuration/crds/extensions/eventtailer_types/","tags":"","title":"EventTailer"},{"body":"Exception Detector Overview This filter plugin consumes a log stream of JSON objects which contain single-line log messages. If a consecutive sequence of log messages form an exception stack trace, they forwarded as a single, combined JSON object. Otherwise, the input log data is forwarded as is. More info at https://github.com/GoogleCloudPlatform/fluent-plugin-detect-exceptions\nNote: As Tag management is not supported yet, this Plugin is mutually exclusive with Tag normaliser\nExample output configurations filters: - detectExceptions: languages: java, python multiline_flush_interval: 0.1 Configuration DetectExceptions message (string, optional) The field which contains the raw message text in the input JSON data.\nDefault: \"\"\nremove_tag_prefix (string, optional) The prefix to be removed from the input tag when outputting a record.\nDefault: kubernetes\nmultiline_flush_interval (string, optional) The interval of flushing the buffer for multiline format.\nDefault: nil\nlanguages ([]string, optional) Programming languages for which to detect exceptions.\nDefault: []\nmax_lines (int, optional) Maximum number of lines to flush (0 means no limit)\nDefault: 1000\nmax_bytes (int, optional) Maximum number of bytes to flush (0 means no limit)\nDefault: 0\nstream (string, optional) Separate log streams by this field in the input JSON data.\nDefault: \"\"\nforce_line_breaks (bool, optional) Force line breaks between each lines when comibining exception stacks.\nDefault: false\nmatch_tag (string, optional) Tag used in match directive.\nDefault: kubernetes.**\nExample Exception Detector filter configurations apiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: demo-flow spec: filters: - detectExceptions: multiline_flush_interval: 0.1 languages: - java - python selectors: {} localOutputRefs: - demo-output Fluentd Config Result \u003cmatch kubernetes.**\u003e @type detect_exceptions @id test_detect_exceptions languages [\"java\",\"python\"] multiline_flush_interval 0.1 remove_tag_prefix kubernetes \u003c/match\u003e ","categories":"","description":"","excerpt":"Exception Detector Overview This filter plugin consumes a log stream …","ref":"/4.2/docs/configuration/plugins/filters/detect_exceptions/","tags":"","title":"Exception Detector"},{"body":"File Output Overview This plugin has been designed to output logs or metrics to File.\nConfiguration FileOutputConfig path (string, required) The Path of the file. The actual path is path + time + “.log” by default.\nDefault: -\nappend (bool, optional) The flushed chunk is appended to existence file or not. The default is not appended.\nDefault: -\nadd_path_suffix (*bool, optional) Add path suffix(default: true)\nDefault: true\npath_suffix (string, optional) The suffix of output result.\nDefault: “.log”\nsymlink_path (bool, optional) Create symlink to temporary buffered file when buffer_type is file. This is useful for tailing file content to check logs.\nDefault: false\ncompress (string, optional) Compresses flushed files using gzip. No compression is performed by default.\nDefault: -\nrecompress (bool, optional) Performs compression again even if the buffer chunk is already compressed.\nDefault: false\nformat (*Format, optional) Format\nDefault: -\nbuffer (*Buffer, optional) Buffer\nDefault: -\nslow_flush_log_threshold (string, optional) The threshold for chunk flush performance check. Parameter type is float, not time, default: 20.0 (seconds) If chunk flush takes longer time than this threshold, fluentd logs warning message and increases metric fluentd_output_status_slow_flush_count.\nDefault: -\nExample File output configurations apiVersion: logging.banzaicloud.io/v1beta1 kind: Output metadata: name: demo-output spec: file: path: /tmp/logs/${tag}/%Y/%m/%d.%H.%M append: true buffer: timekey: 1m timekey_wait: 10s timekey_use_utc: true Fluentd Config Result \u003cmatch **\u003e @type file @id test_file add_path_suffix true append true path /tmp/logs/${tag}/%Y/%m/%d.%H.%M \u003cbuffer tag,time\u003e @type file path /buffers/test_file.*.buffer retry_forever true timekey 1m timekey_use_utc true timekey_wait 30s \u003c/buffer\u003e \u003c/match\u003e ","categories":"","description":"","excerpt":"File Output Overview This plugin has been designed to output logs or …","ref":"/4.2/docs/configuration/plugins/outputs/file/","tags":"","title":"File"},{"body":"The file output stores log records in a plain text file.\nspec: file: path: /mnt/archive/logs/${YEAR}/${MONTH}/${DAY}/app.log create_dirs: true For details on the available options of the output, see the syslog-ng documentation.\npath (string, required) Store file path\nDefault: -\ncreate_dirs (bool, optional) Enable creating non-existing directories.\nDefault: false\ndir_group (string, optional) The group of the directories created by syslog-ng. To preserve the original properties of an existing directory, use the option without specifying an attribute: dir-group().\nDefault: Use the global settings\ndir_owner (string, optional) The owner of the directories created by syslog-ng. To preserve the original properties of an existing directory, use the option without specifying an attribute: dir-owner().\nDefault: Use the global settings\ndir_perm (int, optional) The permission mask of directories created by syslog-ng. Log directories are only created if a file after macro expansion refers to a non-existing directory, and directory creation is enabled (see also the create-dirs() option). For octal numbers prefix the number with 0, for example use 0755 for rwxr-xr-x.\nDefault: Use the global settings\ndisk_buffer (*DiskBuffer, optional) This option enables putting outgoing messages into the disk buffer of the destination to avoid message loss in case of a system failure on the destination side. For details, see the Syslog-ng DiskBuffer options.\nDefault: false\ntemplate (string, optional) Default: -\npersist_name (string, optional) Default: -\n","categories":"","description":"","excerpt":"The file output stores log records in a plain text file.\nspec: file: …","ref":"/4.2/docs/configuration/plugins/syslog-ng-outputs/file/","tags":"","title":"File"},{"body":"FlowSpec FlowSpec is the Kubernetes spec for Flows\nselectors (map[string]string, optional) Deprecated\nDefault: -\nmatch ([]Match, optional) Default: -\nfilters ([]Filter, optional) Default: -\nloggingRef (string, optional) Default: -\noutputRefs ([]string, optional) Deprecated\nDefault: -\nglobalOutputRefs ([]string, optional) Default: -\nlocalOutputRefs ([]string, optional) Default: -\nflowLabel (string, optional) Default: -\nincludeLabelInRouter (*bool, optional) Default: -\nMatch select (*Select, optional) Default: -\nexclude (*Exclude, optional) Default: -\nSelect labels (map[string]string, optional) Default: -\nhosts ([]string, optional) Default: -\ncontainer_names ([]string, optional) Default: -\nExclude labels (map[string]string, optional) Default: -\nhosts ([]string, optional) Default: -\ncontainer_names ([]string, optional) Default: -\nFilter Filter definition for FlowSpec\nstdout (*filter.StdOutFilterConfig, optional) Default: -\nparser (*filter.ParserConfig, optional) Default: -\ntag_normaliser (*filter.TagNormaliser, optional) Default: -\ndedot (*filter.DedotFilterConfig, optional) Default: -\nelasticsearch_genid (*filter.ElasticsearchGenId, optional) Default: -\nrecord_transformer (*filter.RecordTransformer, optional) Default: -\nrecord_modifier (*filter.RecordModifier, optional) Default: -\ngeoip (*filter.GeoIP, optional) Default: -\nconcat (*filter.Concat, optional) Default: -\ndetectExceptions (*filter.DetectExceptions, optional) Default: -\ngrep (*filter.GrepConfig, optional) Default: -\nprometheus (*filter.PrometheusConfig, optional) Default: -\nthrottle (*filter.Throttle, optional) Default: -\nsumologic (*filter.SumoLogic, optional) Default: -\nenhanceK8s (*filter.EnhanceK8s, optional) Default: -\nkube_events_timestamp (*filter.KubeEventsTimestampConfig, optional) Default: -\nFlowStatus FlowStatus defines the observed state of Flow\nactive (*bool, optional) Default: -\nproblems ([]string, optional) Default: -\nproblemsCount (int, optional) Default: -\nFlow Flow Kubernetes object\n(metav1.TypeMeta, required) Default: -\nmetadata (metav1.ObjectMeta, optional) Default: -\nspec (FlowSpec, optional) Default: -\nstatus (FlowStatus, optional) Default: -\nFlowList FlowList contains a list of Flow\n(metav1.TypeMeta, required) Default: -\nmetadata (metav1.ListMeta, optional) Default: -\nitems ([]Flow, required) Default: -\n","categories":"","description":"","excerpt":"FlowSpec FlowSpec is the Kubernetes spec for Flows\nselectors …","ref":"/4.2/docs/configuration/crds/v1beta1/flow_types/","tags":"","title":"FlowSpec"},{"body":"\nThe following sections help you troubleshoot the Fluent Bit component of the Logging operator.\nCheck the Fluent Bit daemonset Verify that the Fluent Bit daemonset is available. Issue the following command: kubectl get daemonsets The output should include a Fluent Bit daemonset, for example:\nNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE logging-demo-fluentbit 1 1 1 1 1 \u003cnone\u003e 110s Check the Fluent Bit configuration You can display the current configuration of the Fluent Bit daemonset using the following command: kubectl get secret logging-demo-fluentbit -o jsonpath=\"{.data['fluent-bit\\.conf']}\" | base64 --decode\nThe output looks like the following:\n[SERVICE] Flush 1 Daemon Off Log_Level info Parsers_File parsers.conf storage.path /buffers [INPUT] Name tail DB /tail-db/tail-containers-state.db Mem_Buf_Limit 5MB Parser docker Path /var/log/containers/*.log Refresh_Interval 5 Skip_Long_Lines On Tag kubernetes.* [FILTER] Name kubernetes Kube_CA_File /var/run/secrets/kubernetes.io/serviceaccount/ca.crt Kube_Tag_Prefix kubernetes.var.log.containers Kube_Token_File /var/run/secrets/kubernetes.io/serviceaccount/token Kube_URL https://kubernetes.default.svc:443 Match kubernetes.* Merge_Log On [OUTPUT] Name forward Match * Host logging-demo-fluentd.logging.svc Port 24240 tls On tls.verify Off tls.ca_file /fluent-bit/tls/ca.crt tls.crt_file /fluent-bit/tls/tls.crt tls.key_file /fluent-bit/tls/tls.key Shared_Key Kamk2_SukuWenk Retry_Limit False Debug version of the fluentbit container All Fluent Bit image tags have a debug version marked with the -debug suffix. You can install this debug version using the following command: kubectl edit loggings.logging.banzaicloud.io logging-demo\nfluentbit: image: pullPolicy: Always repository: fluent/fluent-bit tag: 1.3.2-debug After deploying the debug version, you can kubectl exec into the pod using sh and look around. For example: kubectl exec -it logging-demo-fluentbit-778zg sh\nCheck the queued log messages You can check the buffer directory if Fluent Bit is configured to buffer queued log messages to disk instead of in memory. (You can configure it through the InputTail fluentbit config, by setting the storage.type field to filesystem.)\nkubectl exec -it logging-demo-fluentbit-9dpzg ls /buffers\nGetting Support If you encounter any problems that the documentation does not address, file an issue or talk to us on Discord or Slack.\nCommercial support is also available for the Logging operator.\nBefore asking for help, prepare the following information to make troubleshooting faster:\nLogging operator version kubernetes version helm/chart version (if you installed the Logging operator with helm) Logging operator logs fluentd configuration fluentd logs fluentbit configuration fluentbit logs Do not forget to remove any sensitive information (for example, passwords and private keys) before sharing.\n","categories":"","description":"","excerpt":"\nThe following sections help you troubleshoot the Fluent Bit component …","ref":"/4.2/docs/operation/troubleshooting/fluentbit/","tags":"","title":"Troubleshooting Fluent Bit"},{"body":"FluentbitAgent FluentbitAgent is the Schema for the loggings API\n(metav1.TypeMeta, required) Default: -\nmetadata (metav1.ObjectMeta, optional) Default: -\nspec (FluentbitSpec, optional) Default: -\nstatus (FluentbitStatus, optional) Default: -\nFluentbitAgentList FluentbitAgentList contains a list of FluentbitAgent\n(metav1.TypeMeta, required) Default: -\nmetadata (metav1.ListMeta, optional) Default: -\nitems ([]FluentbitAgent, required) Default: -\nFluentbitSpec FluentbitSpec defines the desired state of FluentbitAgent\nLoggingRef (string, optional) Default: -\ndaemonsetAnnotations (map[string]string, optional) Default: -\nannotations (map[string]string, optional) Default: -\nlabels (map[string]string, optional) Default: -\nenvVars ([]corev1.EnvVar, optional) Default: -\nimage (ImageSpec, optional) Default: -\ntls (*FluentbitTLS, optional) Default: -\ntargetHost (string, optional) Default: -\ntargetPort (int32, optional) Default: -\nflush (int32, optional) Set the flush time in seconds.nanoseconds. The engine loop uses a Flush timeout to define when is required to flush the records ingested by input plugins through the defined output plugins. (default: 1)\nDefault: 1\ngrace (int32, optional) Set the grace time in seconds as Integer value. The engine loop uses a Grace timeout to define wait time on exit (default: 5)\nDefault: 5\nlogLevel (string, optional) Set the logging verbosity level. Allowed values are: error, warn, info, debug and trace. Values are accumulative, e.g: if ‘debug’ is set, it will include error, warning, info and debug. Note that trace mode is only available if Fluent Bit was built with the WITH_TRACE option enabled. (default: info)\nDefault: info\ncoroStackSize (int32, optional) Set the coroutines stack size in bytes. The value must be greater than the page size of the running system. Don’t set too small value (say 4096), or coroutine threads can overrun the stack buffer. Do not change the default value of this parameter unless you know what you are doing. (default: 24576)\nDefault: 24576\nresources (corev1.ResourceRequirements, optional) Default: -\ntolerations ([]corev1.Toleration, optional) Default: -\nnodeSelector (map[string]string, optional) Default: -\naffinity (*corev1.Affinity, optional) Default: -\nmetrics (*Metrics, optional) Default: -\nsecurity (*Security, optional) Default: -\npositiondb (volume.KubernetesVolume, optional) volume.KubernetesVolume\nDefault: -\nposition_db (*volume.KubernetesVolume, optional) Deprecated, use positiondb\nDefault: -\nmountPath (string, optional) Default: -\nextraVolumeMounts ([]*VolumeMount, optional) Default: -\ninputTail (InputTail, optional) Default: -\nfilterAws (*FilterAws, optional) Default: -\nfilterModify ([]FilterModify, optional) Default: -\nparser (string, optional) Deprecated, use inputTail.parser\nDefault: -\nfilterKubernetes (FilterKubernetes, optional) Parameters for Kubernetes metadata filter\nDefault: -\ndisableKubernetesFilter (*bool, optional) Disable Kubernetes metadata filter\nDefault: -\nbufferStorage (BufferStorage, optional) Default: -\nbufferStorageVolume (volume.KubernetesVolume, optional) volume.KubernetesVolume\nDefault: -\nbufferVolumeMetrics (*Metrics, optional) Default: -\nbufferVolumeImage (ImageSpec, optional) Default: -\nbufferVolumeArgs ([]string, optional) Default: -\ncustomConfigSecret (string, optional) Default: -\npodPriorityClassName (string, optional) Default: -\nlivenessProbe (*corev1.Probe, optional) Default: -\nlivenessDefaultCheck (bool, optional) Default: -\nreadinessProbe (*corev1.Probe, optional) Default: -\nnetwork (*FluentbitNetwork, optional) Default: -\nforwardOptions (*ForwardOptions, optional) Default: -\nenableUpstream (bool, optional) Default: -\nserviceAccount (*typeoverride.ServiceAccount, optional) Default: -\ndnsPolicy (corev1.DNSPolicy, optional) Default: -\ndnsConfig (*corev1.PodDNSConfig, optional) Default: -\nHostNetwork (bool, optional) Default: -\nsyslogng_output (*FluentbitTCPOutput, optional) Default: -\nupdateStrategy (appsv1.DaemonSetUpdateStrategy, optional) Default: -\ncustomParsers (string, optional) Available in Logging operator version 4.2 and later.\nSpecify a custom parser file to load in addition to the default parsers file. It must be a valid key in the configmap specified by customConfig.\nDefault: -\nThe following example defines a Fluentd parser that places the parsed containerd log messages into the log field instead of the message field.\napiVersion: logging.banzaicloud.io/v1beta1 kind: FluentbitAgent metadata: name: containerd spec: inputTail: Parser: cri-log-key # Parser that populates `log` instead of `message` to enable the Kubernetes filter's Merge_Log feature to work # Mind the indentation, otherwise Fluent Bit will parse the whole message into the `log` key customParsers: | [PARSER] Name cri-log-key Format regex Regex ^(?\u003ctime\u003e[^ ]+) (?\u003cstream\u003estdout|stderr) (?\u003clogtag\u003e[^ ]*) (?\u003clog\u003e.*)$ Time_Key time Time_Format %Y-%m-%dT%H:%M:%S.%L%z # Required key remap if one wants to rely on the existing auto-detected log key in the fluentd parser and concat filter otherwise should be omitted filterModify: - rules: - Rename: key: log value: message FluentbitStatus FluentbitStatus defines the resource status for FluentbitAgent\nFluentbitTLS FluentbitTLS defines the TLS configs\nenabled (*bool, required) Default: -\nsecretName (string, optional) Default: -\nsharedKey (string, optional) Default: -\nFluentbitTCPOutput FluentbitTCPOutput defines the TLS configs\njson_date_key (string, optional) Default: ts\njson_date_format (string, optional) Default: iso8601\nFluentbitNetwork FluentbitNetwork defines network configuration for fluentbit\nconnectTimeout (*uint32, optional) Sets the timeout for connecting to an upstream\nDefault: 10\nconnectTimeoutLogError (*bool, optional) On connection timeout, specify if it should log an error. When disabled, the timeout is logged as a debug message\nDefault: true\ndnsMode (string, optional) Sets the primary transport layer protocol used by the asynchronous DNS resolver for connections established\nDefault: UDP, UDP or TCP\ndnsPreferIpv4 (*bool, optional) Prioritize IPv4 DNS results when trying to establish a connection\nDefault: false\ndnsResolver (string, optional) Select the primary DNS resolver type\nDefault: ASYNC, LEGACY or ASYNC\nkeepalive (*bool, optional) Whether or not TCP keepalive is used for the upstream connection\nDefault: true\nkeepaliveIdleTimeout (*uint32, optional) How long in seconds a TCP keepalive connection can be idle before being recycled\nDefault: 30\nkeepaliveMaxRecycle (*uint32, optional) How many times a TCP keepalive connection can be used before being recycled\nDefault: 0, disabled\nsourceAddress (string, optional) Specify network address (interface) to use for connection and data traffic.\nDefault: disabled\nBufferStorage BufferStorage is the Service Section Configuration of fluent-bit\nstorage.path (string, optional) Set an optional location in the file system to store streams and chunks of data. If this parameter is not set, Input plugins can only use in-memory buffering.\nDefault: -\nstorage.sync (string, optional) Configure the synchronization mode used to store the data into the file system. It can take the values normal or full.\nDefault: normal\nstorage.checksum (string, optional) Enable the data integrity check when writing and reading data from the filesystem. The storage layer uses the CRC32 algorithm.\nDefault: Off\nstorage.backlog.mem_limit (string, optional) If storage.path is set, Fluent Bit will look for data chunks that were not delivered and are still in the storage layer, these are called backlog data. This option configure a hint of maximum value of memory to use when processing these records.\nDefault: 5M\nInputTail InputTail defines FluentbitAgent tail input configuration The tail input plugin allows to monitor one or several text files. It has a similar behavior like tail -f shell command.\nstorage.type (string, optional) Specify the buffering mechanism to use. It can be memory or filesystem.\nDefault: memory\nBuffer_Chunk_Size (string, optional) Set the buffer size for HTTP client when reading responses from Kubernetes API server. The value must be according to the Unit Size specification.\nDefault: 32k\nBuffer_Max_Size (string, optional) Set the limit of the buffer size per monitored file. When a buffer needs to be increased (e.g: very long lines), this value is used to restrict how much the memory buffer can grow. If reading a file exceed this limit, the file is removed from the monitored file list. The value must be according to the Unit Size specification.\nDefault: Buffer_Chunk_Size\nPath (string, optional) Pattern specifying a specific log files or multiple ones through the use of common wildcards.\nDefault: -\nPath_Key (string, optional) If enabled, it appends the name of the monitored file as part of the record. The value assigned becomes the key in the map.\nDefault: -\nExclude_Path (string, optional) Set one or multiple shell patterns separated by commas to exclude files matching a certain criteria, e.g: exclude_path=.gz,.zip\nDefault: -\nRead_From_Head (bool, optional) For new discovered files on start (without a database offset/position), read the content from the head of the file, not tail.\nDefault: -\nRefresh_Interval (string, optional) The interval of refreshing the list of watched files in seconds.\nDefault: 60\nRotate_Wait (string, optional) Specify the number of extra time in seconds to monitor a file once is rotated in case some pending data is flushed.\nDefault: 5\nIgnore_Older (string, optional) Ignores files that have been last modified before this time in seconds. Supports m,h,d (minutes, hours,days) syntax. Default behavior is to read all specified files.\nDefault: -\nSkip_Long_Lines (string, optional) When a monitored file reach it buffer capacity due to a very long line (Buffer_Max_Size), the default behavior is to stop monitoring that file. Skip_Long_Lines alter that behavior and instruct Fluent Bit to skip long lines and continue processing other lines that fits into the buffer size.\nDefault: Off\nDB (*string, optional) Specify the database file to keep track of monitored files and offsets.\nDefault: -\nDB_Sync (string, optional) Set a default synchronization (I/O) method. Values: Extra, Full, Normal, Off. This flag affects how the internal SQLite engine do synchronization to disk, for more details about each option please refer to this section.\nDefault: Full\nDB.locking (*bool, optional) Specify that the database will be accessed only by Fluent Bit. Enabling this feature helps to increase performance when accessing the database but it restrict any external tool to query the content.\nDefault: true\nDB.journal_mode (string, optional) sets the journal mode for databases (WAL). Enabling WAL provides higher performance. Note that WAL is not compatible with shared network file systems.\nDefault: WAL\nMem_Buf_Limit (string, optional) Set a limit of memory that Tail plugin can use when appending data to the Engine. If the limit is reach, it will be paused; when the data is flushed it resumes.\nDefault: -\nParser (string, optional) Specify the name of a parser to interpret the entry as a structured message.\nDefault: -\nKey (string, optional) When a message is unstructured (no parser applied), it’s appended as a string under the key name log. This option allows to define an alternative name for that key.\nDefault: log\nTag (string, optional) Set a tag (with regex-extract fields) that will be placed on lines read.\nDefault: -\nTag_Regex (string, optional) Set a regex to extract fields from the file.\nDefault: -\nMultiline (string, optional) If enabled, the plugin will try to discover multiline messages and use the proper parsers to compose the outgoing messages. Note that when this option is enabled the Parser option is not used.\nDefault: Off\nMultiline_Flush (string, optional) Wait period time in seconds to process queued multiline messages\nDefault: 4\nParser_Firstline (string, optional) Name of the parser that machs the beginning of a multiline message. Note that the regular expression defined in the parser must include a group name (named capture)\nDefault: -\nParser_N ([]string, optional) Optional-extra parser to interpret and structure multiline entries. This option can be used to define multiple parsers, e.g: Parser_1 ab1, Parser_2 ab2, Parser_N abN.\nDefault: -\nDocker_Mode (string, optional) If enabled, the plugin will recombine split Docker log lines before passing them to any parser as configured above. This mode cannot be used at the same time as Multiline.\nDefault: Off\nDocker_Mode_Parser (string, optional) Specify an optional parser for the first line of the docker multiline mode.\nDefault: -\nDocker_Mode_Flush (string, optional) Wait period time in seconds to flush queued unfinished split lines.\nDefault: 4\nmultiline.parser ([]string, optional) Specify one or multiple parser definitions to apply to the content. Part of the new Multiline Core support in 1.8\nDefault: \"\"\nFilterKubernetes FilterKubernetes Fluent Bit Kubernetes Filter allows to enrich your log files with Kubernetes metadata.\nMatch (string, optional) Match filtered records (default:kube.*)\nDefault: kubernetes.*\nBuffer_Size (string, optional) Set the buffer size for HTTP client when reading responses from Kubernetes API server. The value must be according to the Unit Size specification. A value of 0 results in no limit, and the buffer will expand as-needed. Note that if pod specifications exceed the buffer limit, the API response will be discarded when retrieving metadata, and some kubernetes metadata will fail to be injected to the logs. If this value is empty we will set it “0”.\nDefault: “0”\nKube_URL (string, optional) API Server end-point (default: https://kubernetes.default.svc:443)\nDefault: https://kubernetes.default.svc:443\nKube_CA_File (string, optional) CA certificate file (default:/var/run/secrets/kubernetes.io/serviceaccount/ca.crt)\nDefault: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\nKube_CA_Path (string, optional) Absolute path to scan for certificate files\nDefault: -\nKube_Token_File (string, optional) Token file (default:/var/run/secrets/kubernetes.io/serviceaccount/token)\nDefault: /var/run/secrets/kubernetes.io/serviceaccount/token\nKube_Token_TTL (string, optional) Token TTL configurable ’time to live’ for the K8s token. By default, it is set to 600 seconds. After this time, the token is reloaded from Kube_Token_File or the Kube_Token_Command. (default:“600”)\nDefault: 600\nKube_Tag_Prefix (string, optional) When the source records comes from Tail input plugin, this option allows to specify what’s the prefix used in Tail configuration. (default:kube.var.log.containers.)\nDefault: kubernetes.var.log.containers\nMerge_Log (string, optional) When enabled, it checks if the log field content is a JSON string map, if so, it append the map fields as part of the log structure. (default:Off)\nDefault: On\nMerge_Log_Key (string, optional) When Merge_Log is enabled, the filter tries to assume the log field from the incoming message is a JSON string message and make a structured representation of it at the same level of the log field in the map. Now if Merge_Log_Key is set (a string name), all the new structured fields taken from the original log content are inserted under the new key.\nDefault: -\nMerge_Log_Trim (string, optional) When Merge_Log is enabled, trim (remove possible \\n or \\r) field values.\nDefault: On\nMerge_Parser (string, optional) Optional parser name to specify how to parse the data contained in the log key. Recommended use is for developers or testing only.\nDefault: -\nKeep_Log (string, optional) When Keep_Log is disabled, the log field is removed from the incoming message once it has been successfully merged (Merge_Log must be enabled as well).\nDefault: On\ntls.debug (string, optional) Debug level between 0 (nothing) and 4 (every detail).\nDefault: -1\ntls.verify (string, optional) When enabled, turns on certificate validation when connecting to the Kubernetes API server.\nDefault: On\nUse_Journal (string, optional) When enabled, the filter reads logs coming in Journald format.\nDefault: Off\nCache_Use_Docker_Id (string, optional) When enabled, metadata will be fetched from K8s when docker_id is changed.\nDefault: Off\nRegex_Parser (string, optional) Set an alternative Parser to process record Tag and extract pod_name, namespace_name, container_name and docker_id. The parser must be registered in a parsers file (refer to parser filter-kube-test as an example).\nDefault: -\nK8S-Logging.Parser (string, optional) Allow Kubernetes Pods to suggest a pre-defined Parser (read more about it in Kubernetes Annotations section)\nDefault: Off\nK8S-Logging.Exclude (string, optional) Allow Kubernetes Pods to exclude their logs from the log processor (read more about it in Kubernetes Annotations section).\nDefault: Off\nLabels (string, optional) Include Kubernetes resource labels in the extra metadata.\nDefault: On\nAnnotations (string, optional) Include Kubernetes resource annotations in the extra metadata.\nDefault: On\nKube_meta_preload_cache_dir (string, optional) If set, Kubernetes meta-data can be cached/pre-loaded from files in JSON format in this directory, named as namespace-pod.meta\nDefault: -\nDummy_Meta (string, optional) If set, use dummy-meta data (for test/dev purposes)\nDefault: Off\nDNS_Retries (string, optional) DNS lookup retries N times until the network start working\nDefault: 6\nDNS_Wait_Time (string, optional) DNS lookup interval between network status checks\nDefault: 30\nUse_Kubelet (string, optional) This is an optional feature flag to get metadata information from kubelet instead of calling Kube Server API to enhance the log.\nDefault: Off\nKubelet_Port (string, optional) kubelet port using for HTTP request, this only works when Use_Kubelet set to On\nDefault: 10250\nKube_Meta_Cache_TTL (string, optional) Configurable TTL for K8s cached metadata. By default, it is set to 0 which means TTL for cache entries is disabled and cache entries are evicted at random when capacity is reached. In order to enable this option, you should set the number to a time interval. For example, set this value to 60 or 60s and cache entries which have been created more than 60s will be evicted.\nDefault: 0\nFilterAws FilterAws The AWS Filter Enriches logs with AWS Metadata.\nimds_version (string, optional) Specify which version of the instance metadata service to use. Valid values are ‘v1’ or ‘v2’ (default).\nDefault: v2\naz (*bool, optional) The availability zone (default:true).\nDefault: true\nec2_instance_id (*bool, optional) The EC2 instance ID. (default:true)\nDefault: true\nec2_instance_type (*bool, optional) The EC2 instance type. (default:false)\nDefault: false\nprivate_ip (*bool, optional) The EC2 instance private ip. (default:false)\nDefault: false\nami_id (*bool, optional) The EC2 instance image id. (default:false)\nDefault: false\naccount_id (*bool, optional) The account ID for current EC2 instance. (default:false)\nDefault: false\nhostname (*bool, optional) The hostname for current EC2 instance. (default:false)\nDefault: false\nvpc_id (*bool, optional) The VPC ID for current EC2 instance. (default:false)\nDefault: false\nMatch (string, optional) Match filtered records (default:*)\nDefault: *\nFilterModify FilterModify The Modify Filter plugin allows you to change records using rules and conditions.\nrules ([]FilterModifyRule, optional) FluentbitAgent Filter Modification Rule\nDefault: -\nconditions ([]FilterModifyCondition, optional) FluentbitAgent Filter Modification Condition\nDefault: -\nFilterModifyRule FilterModifyRule The Modify Filter plugin allows you to change records using rules and conditions.\nSet (*FilterKeyValue, optional) Add a key/value pair with key KEY and value VALUE. If KEY already exists, this field is overwritten\nDefault: -\nAdd (*FilterKeyValue, optional) Add a key/value pair with key KEY and value VALUE if KEY does not exist\nDefault: -\nRemove (*FilterKey, optional) Remove a key/value pair with key KEY if it exists\nDefault: -\nRemove_wildcard (*FilterKey, optional) Remove all key/value pairs with key matching wildcard KEY\nDefault: -\nRemove_regex (*FilterKey, optional) Remove all key/value pairs with key matching regexp KEY\nDefault: -\nRename (*FilterKeyValue, optional) Rename a key/value pair with key KEY to RENAMED_KEY if KEY exists AND RENAMED_KEY does not exist\nDefault: -\nHard_rename (*FilterKeyValue, optional) Rename a key/value pair with key KEY to RENAMED_KEY if KEY exists. If RENAMED_KEY already exists, this field is overwritten\nDefault: -\nCopy (*FilterKeyValue, optional) Copy a key/value pair with key KEY to COPIED_KEY if KEY exists AND COPIED_KEY does not exist\nDefault: -\nHard_copy (*FilterKeyValue, optional) Copy a key/value pair with key KEY to COPIED_KEY if KEY exists. If COPIED_KEY already exists, this field is overwritten\nDefault: -\nFilterModifyCondition FilterModifyCondition The Modify Filter plugin allows you to change records using rules and conditions.\nKey_exists (*FilterKey, optional) Is true if KEY exists\nDefault: -\nKey_does_not_exist (*FilterKeyValue, optional) Is true if KEY does not exist\nDefault: -\nA_key_matches (*FilterKey, optional) Is true if a key matches regex KEY\nDefault: -\nNo_key_matches (*FilterKey, optional) Is true if no key matches regex KEY\nDefault: -\nKey_value_equals (*FilterKeyValue, optional) Is true if KEY exists and its value is VALUE\nDefault: -\nKey_value_does_not_equal (*FilterKeyValue, optional) Is true if KEY exists and its value is not VALUE\nDefault: -\nKey_value_matches (*FilterKeyValue, optional) Is true if key KEY exists and its value matches VALUE\nDefault: -\nKey_value_does_not_match (*FilterKeyValue, optional) Is true if key KEY exists and its value does not match VALUE\nDefault: -\nMatching_keys_have_matching_values (*FilterKeyValue, optional) Is true if all keys matching KEY have values that match VALUE\nDefault: -\nMatching_keys_do_not_have_matching_values (*FilterKeyValue, optional) Is true if all keys matching KEY have values that do not match VALUE\nDefault: -\nOperation Operation Doc stub\nOp (string, optional) Default: -\nKey (string, optional) Default: -\nValue (string, optional) Default: -\nFilterKey key (string, optional) Default: -\nFilterKeyValue key (string, optional) Default: -\nvalue (string, optional) Default: -\nVolumeMount VolumeMount defines source and destination folders of a hostPath type pod mount\nsource (string, required) Source folder\nDefault: -\ndestination (string, required) Destination Folder\nDefault: -\nreadOnly (*bool, optional) Mount Mode\nDefault: -\nForwardOptions ForwardOptions defines custom forward output plugin options, see https://docs.fluentbit.io/manual/pipeline/outputs/forward\nTime_as_Integer (bool, optional) Default: -\nSend_options (bool, optional) Default: -\nRequire_ack_response (bool, optional) Default: -\nTag (string, optional) Default: -\nRetry_Limit (string, optional) Default: -\nstorage.total_limit_size (string, optional) storage.total_limit_size Limit the maximum number of Chunks in the filesystem for the current output logical destination.\nDefault: -\nFluentbitNameProvider Defines a FluentbitNameProvider\nLoggingRef (*Logging, optional) Default: -\nfluentbit (*FluentbitAgent, optional) Default: -\n","categories":"","description":"","excerpt":"FluentbitAgent FluentbitAgent is the Schema for the loggings API …","ref":"/4.2/docs/configuration/crds/v1beta1/fluentbit_types/","tags":"","title":"FluentbitSpec"},{"body":"\nThe following sections help you troubleshoot the Fluentd statefulset component of the Logging operator.\nCheck Fluentd pod status (statefulset) Verify that the Fluentd statefulset is available using the following command: kubectl get statefulsets\nExpected output:\nNAME READY AGE logging-demo-fluentd 1/1 1m ConfigCheck The Logging operator has a builtin mechanism that validates the generated fluentd configuration before applying it to fluentd. You should be able to see the configcheck pod and its log output. The result of the check is written into the status field of the corresponding Logging resource.\nIn case the operator is stuck in an error state caused by a failed configcheck, restore the previous configuration by modifying or removing the invalid resources to the point where the configcheck pod is finally able to complete successfully.\nCheck Fluentd configuration Use the following command to display the configuration of Fluentd: kubectl get secret logging-demo-fluentd-app -o jsonpath=\"{.data['fluentd\\.conf']}\" | base64 --decode\nThe output should be similar to the following:\n\u003csource\u003e @type forward @id main_forward bind 0.0.0.0 port 24240 \u003ctransport tls\u003e ca_path /fluentd/tls/ca.crt cert_path /fluentd/tls/tls.crt client_cert_auth true private_key_path /fluentd/tls/tls.key version TLSv1_2 \u003c/transport\u003e \u003csecurity\u003e self_hostname fluentd shared_key Kamk2_SukuWenk \u003c/security\u003e \u003c/source\u003e \u003cmatch **\u003e @type label_router @id main_label_router \u003croute\u003e @label @427b3e18f3a3bc3f37643c54e9fc960b labels app.kubernetes.io/instance:logging-demo,app.kubernetes.io/name:log-generator namespace logging \u003c/route\u003e \u003c/match\u003e \u003clabel @427b3e18f3a3bc3f37643c54e9fc960b\u003e \u003cmatch kubernetes.**\u003e @type tag_normaliser @id logging-demo-flow_0_tag_normaliser format ${namespace_name}.${pod_name}.${container_name} \u003c/match\u003e \u003cfilter **\u003e @type parser @id logging-demo-flow_1_parser key_name log remove_key_name_field true reserve_data true \u003cparse\u003e @type nginx \u003c/parse\u003e \u003c/filter\u003e \u003cmatch **\u003e @type s3 @id logging_logging-demo-flow_logging-demo-output-minio_s3 aws_key_id WVKblQelkDTSKTn4aaef aws_sec_key LAmjIah4MTKTM3XGrDxuD2dTLLmysVHvZrtxpzK6 force_path_style true path logs/${tag}/%Y/%m/%d/ s3_bucket demo s3_endpoint http://logging-demo-minio.logging.svc.cluster.local:9000 s3_region test_region \u003cbuffer tag,time\u003e @type file path /buffers/logging_logging-demo-flow_logging-demo-output-minio_s3.*.buffer retry_forever true timekey 10s timekey_use_utc true timekey_wait 0s \u003c/buffer\u003e \u003c/match\u003e \u003c/label\u003e Set Fluentd log Level Use the following command to change the log level of Fluentd. kubectl edit loggings.logging.banzaicloud.io logging-demo\nfluentd: logLevel: debug Get Fluentd logs The following command displays the logs of the Fluentd container. kubectl exec -it logging-demo-fluentd-0 cat /fluentd/log/out\nTip: If the logs include the error=\"can't create buffer file ... error message, Fluentd can’t create the buffer file at the specified location. This can mean for example that the disk is full, the filesystem is read-only, or some other permission error. Check the buffer-related settings of your Fluentd configuration.\nSet stdout as an output You can use an stdout filter at any point in the flow to dump the log messages to the stdout of the Fluentd container. For example: kubectl edit loggings.logging.banzaicloud.io logging-demo\napiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: exchange namespace: logging spec: filters: - stdout: {} localOutputRefs: - exchange selectors: application: exchange Check the buffer path in the fluentd container kubectl exec -it logging-demo-fluentd-0 ls /buffers\nDefaulting container name to fluentd. Use 'kubectl describe pod/logging-demo-fluentd-0 -n logging' to see all of the containers in this pod. logging_logging-demo-flow_logging-demo-output-minio_s3.b598f7eb0b2b34076b6da13a996ff2671.buffer logging_logging-demo-flow_logging-demo-output-minio_s3.b598f7eb0b2b34076b6da13a996ff2671.buffer.meta Getting Support If you encounter any problems that the documentation does not address, file an issue or talk to us on Discord or Slack.\nCommercial support is also available for the Logging operator.\nBefore asking for help, prepare the following information to make troubleshooting faster:\nLogging operator version kubernetes version helm/chart version (if you installed the Logging operator with helm) Logging operator logs fluentd configuration fluentd logs fluentbit configuration fluentbit logs Do not forget to remove any sensitive information (for example, passwords and private keys) before sharing.\n","categories":"","description":"","excerpt":"\nThe following sections help you troubleshoot the Fluentd statefulset …","ref":"/4.2/docs/operation/troubleshooting/fluentd/","tags":"","title":"Troubleshooting Fluentd"},{"body":"You can configure the deployment of the Fluentd log forwarder via the fluentd section of the The Logging custom resource. This page shows some examples on configuring Fluentd. For the detailed list of available parameters, see FluentdSpec.\nCustom pvc volume for Fluentd buffers apiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: default-logging-simple spec: fluentd: bufferStorageVolume: pvc: spec: accessModes: - ReadWriteOnce resources: requests: storage: 40Gi storageClassName: fast volumeMode: Filesystem fluentbit: {} controlNamespace: logging Custom Fluentd hostPath volume for buffers apiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: default-logging-simple spec: fluentd: disablePvc: true bufferStorageVolume: hostPath: path: \"\" # leave it empty to automatically generate: /opt/logging-operator/default-logging-simple/default-logging-simple-fluentd-buffer fluentbit: {} controlNamespace: logging FluentOutLogrotate The following snippet redirects Fluentd’s stdout to a file and configures rotation settings.\nThis is important to avoid Fluentd getting into a ripple effect when there is an error and the error message gets back to the system as a log message, which generates another error, and so on.\nDefault settings configured by the operator:\nspec: fluentd: fluentOutLogrotate: enabled: true path: /fluentd/log/out age: 10 size: 10485760 Disabling it and write to stdout (not recommended):\nspec: fluentd: fluentOutLogrotate: enabled: false Scaling You can scale the Fluentd deployment manually by changing the number of replicas in the fluentd section of the The Logging custom resource. For example:\napiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: default-logging-simple spec: fluentd: scaling: replicas: 3 fluentbit: {} controlNamespace: logging For automatic scaling, see Autoscaling with HPA.\nGraceful draining While you can scale down the Fluentd deployment by decreasing the number of replicas in the fluentd section of the The Logging custom resource, it won’t automatically be graceful, as the controller will stop the extra replica pods without waiting for any remaining buffers to be flushed. You can enable graceful draining in the scaling subsection:\napiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: default-logging-simple spec: fluentd: scaling: drain: enabled: true fluentbit: {} controlNamespace: logging When graceful draining is enabled, the operator starts drainer jobs for any undrained volumes. The drainer job flushes any remaining buffers before terminating, and the operator marks the associated volume (the PVC, actually) as drained until it gets used again. The drainer job has a template very similar to that of the Fluentd deployment with the addition of a sidecar container that oversees the buffers and signals Fluentd to terminate when all buffers are gone. Pods created by the job are labeled as not to receive any further logs, thus buffers will clear out eventually.\nIf you want, you can specify a custom drainer job sidecar image in the drain subsection:\napiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: default-logging-simple spec: fluentd: scaling: drain: enabled: true image: repository: ghcr.io/banzaicloud/fluentd-drain-watch tag: latest fluentbit: {} controlNamespace: logging In addition to the drainer job, the operator also creates a placeholder pod with the same name as the terminated pod of the Fluentd deployment to keep the deployment from recreating that pod which would result in concurrent access of the volume. The placeholder pod just runs a pause container, and goes away as soon as the job has finished successfully or the deployment is scaled back up and explicitly flushing the buffers is no longer necessary because the newly created replica will take care of processing them.\nYou can mark volumes that should be ignored by the drain logic by adding the label logging.banzaicloud.io/drain: no to the PVC.\nAutoscaling with HPA To configure autoscaling of the Fluentd deployment using Horizontal Pod Autoscaler (HPA), complete the following steps.\nConfigure the aggregation layer. Many providers already have this configured, including kind.\nInstall Prometheus and the Prometheus Adapter if you don’t already have them installed on the cluster. Adjust the default Prometheus address values as needed for your environment (set prometheus.url, prometheus.port, and prometheus.path to the appropriate values).\n(Optional) Install metrics-server to access basic metrics. If the readiness of the metrics-server pod fails with HTTP 500, try adding the --kubelet-insecure-tls flag to the container.\nIf you want to use a custom metric for autoscaling Fluentd and the necessary metric is not available in Prometheus, define a Prometheus recording rule:\ngroups: - name: my-logging-hpa.rules rules: - expr: (node_filesystem_size_bytes{container=\"buffer-metrics-sidecar\",mountpoint=\"/buffers\"}-node_filesystem_free_bytes{container=\"buffer-metrics-sidecar\",mountpoint=\"/buffers\"})/node_filesystem_size_bytes{container=\"buffer-metrics-sidecar\",mountpoint=\"/buffers\"} record: buffer_space_usage_ratio Alternatively, you can define the derived metric as a configuration rule in the Prometheus Adapter’s config map.\nIf it’s not already installed, install the logging-operator and configure a logging resource with at least one flow. Make sure that the logging resource has buffer volume metrics monitoring enabled under spec.fluentd:\n#spec: # fluentd: bufferVolumeMetrics: serviceMonitor: true Verify that the custom metric is available by running:\nkubectl get --raw '/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/buffer_space_usage_ratio' The logging-operator enforces the replica count of the stateful set based on the logging resource’s replica count, even if it’s not set explicitly. To allow for HPA to control the replica count of the stateful set, this coupling has to be severed. Currently, the only way to do that is by deleting the logging-operator deployment.\nCreate a HPA resource. The following example tries to keep the average buffer volume usage of Fluentd instances at 80%.\napiVersion: autoscaling/v2beta2 kind: HorizontalPodAutoscaler metadata: name: logging-fluentd spec: scaleTargetRef: apiVersion: apps/v1 kind: StatefulSet name: logging-fluentd minReplicas: 1 maxReplicas: 10 metrics: - type: Pods pods: metric: name: buffer_space_usage_ratio target: type: AverageValue averageValue: 800m Probe A Probe is a diagnostic performed periodically by the kubelet on a Container. To perform a diagnostic, the kubelet calls a Handler implemented by the Container. You can configure a probe for Fluentd in the livenessProbe section of the The Logging custom resource. For example:\napiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: default-logging-simple spec: fluentd: livenessProbe: periodSeconds: 60 initialDelaySeconds: 600 exec: command: - \"/bin/sh\" - \"-c\" - \u003e LIVENESS_THRESHOLD_SECONDS=${LIVENESS_THRESHOLD_SECONDS:-300}; if [ ! -e /buffers ]; then exit 1; fi; touch -d \"${LIVENESS_THRESHOLD_SECONDS} seconds ago\" /tmp/marker-liveness; if [ -z \"$(find /buffers -type d -newer /tmp/marker-liveness -print -quit)\" ]; then exit 1; fi; fluentbit: {} controlNamespace: logging You can use the following parameters:\nName Type Default Description initialDelaySeconds int 600 Number of seconds after the container has started before liveness probes are initiated. timeoutSeconds int 0 Number of seconds after which the probe times out. periodSeconds int 60 How often (in seconds) to perform the probe. successThreshold int 0 Minimum consecutive successes for the probe to be considered successful after having failed. failureThreshold int 0 Minimum consecutive failures for the probe to be considered failed after having succeeded. exec array {} Exec specifies the action to take. More info httpGet array {} HTTPGet specifies the http request to perform. More info tcpSocket array {} TCPSocket specifies an action involving a TCP port. More info Note: To configure readiness probes, see Readiness probe.\nCustom Fluentd image You can deploy custom images by overriding the default images using the following parameters in the fluentd or fluentbit sections of the logging resource.\nName Type Default Description repository string \"\" Image repository tag string \"\" Image tag pullPolicy string \"\" Always, IfNotPresent, Never The following example deploys a custom fluentd image:\napiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: default-logging-simple spec: fluentd: image: repository: banzaicloud/fluentd tag: v1.10.4-alpine-1 pullPolicy: IfNotPresent configReloaderImage: repository: jimmidyson/configmap-reload tag: v0.4.0 pullPolicy: IfNotPresent scaling: drain: image: repository: ghcr.io/banzaicloud/fluentd-drain-watch tag: v0.0.1 pullPolicy: IfNotPresent bufferVolumeImage: repository: quay.io/prometheus/node-exporter tag: v1.1.2 pullPolicy: IfNotPresent fluentbit: {} controlNamespace: logging KubernetesStorage Define Kubernetes storage.\nName Type Default Description hostPath HostPathVolumeSource - Represents a host path mapped into a pod. If path is empty, it will automatically be set to /opt/logging-operator/\u003cname of the logging CR\u003e/\u003cname of the volume\u003e emptyDir EmptyDirVolumeSource - Represents an empty directory for a pod. pvc PersistentVolumeClaim - A PersistentVolumeClaim (PVC) is a request for storage by a user. Persistent Volume Claim Name Type Default Description spec PersistentVolumeClaimSpec - Spec defines the desired characteristics of a volume requested by a pod author. source PersistentVolumeClaimVolumeSource - PersistentVolumeClaimVolumeSource references the user’s PVC in the same namespace. The Persistent Volume Claim should be created with the given spec and with the name defined in the source’s claimName.\nCPU and memory requirements To adjust the CPU and memory limits and requests of the pods managed by Logging operator, see CPU and memory requirements.\n","categories":"","description":"","excerpt":"You can configure the deployment of the Fluentd log forwarder via the …","ref":"/4.2/docs/logging-infrastructure/fluentd/","tags":"","title":"Configure Fluentd"},{"body":"FluentdSpec FluentdSpec defines the desired state of Fluentd\nstatefulsetAnnotations (map[string]string, optional) Default: -\nannotations (map[string]string, optional) Default: -\nconfigCheckAnnotations (map[string]string, optional) Default: -\nlabels (map[string]string, optional) Default: -\nenvVars ([]corev1.EnvVar, optional) Default: -\ntls (FluentdTLS, optional) Default: -\nimage (ImageSpec, optional) Default: -\ndisablePvc (bool, optional) Default: -\nbufferStorageVolume (volume.KubernetesVolume, optional) BufferStorageVolume is by default configured as PVC using FluentdPvcSpec volume.KubernetesVolume\nDefault: -\nextraVolumes ([]ExtraVolume, optional) Default: -\nfluentdPvcSpec (*volume.KubernetesVolume, optional) Deprecated, use bufferStorageVolume\nDefault: -\nvolumeMountChmod (bool, optional) Default: -\nvolumeModImage (ImageSpec, optional) Default: -\nconfigReloaderImage (ImageSpec, optional) Default: -\nresources (corev1.ResourceRequirements, optional) Default: -\nconfigCheckResources (corev1.ResourceRequirements, optional) Default: -\nconfigReloaderResources (corev1.ResourceRequirements, optional) Default: -\nlivenessProbe (*corev1.Probe, optional) Default: -\nlivenessDefaultCheck (bool, optional) Default: -\nreadinessProbe (*corev1.Probe, optional) Default: -\nreadinessDefaultCheck (ReadinessDefaultCheck, optional) Default: -\nport (int32, optional) Default: -\ntolerations ([]corev1.Toleration, optional) Default: -\nnodeSelector (map[string]string, optional) Default: -\naffinity (*corev1.Affinity, optional) Default: -\ntopologySpreadConstraints ([]corev1.TopologySpreadConstraint, optional) Default: -\nmetrics (*Metrics, optional) Default: -\nbufferVolumeMetrics (*Metrics, optional) Default: -\nbufferVolumeImage (ImageSpec, optional) Default: -\nbufferVolumeArgs ([]string, optional) Default: -\nsecurity (*Security, optional) Default: -\nscaling (*FluentdScaling, optional) Default: -\nworkers (int32, optional) Default: -\nrootDir (string, optional) Default: -\nlogLevel (string, optional) Default: -\nignoreSameLogInterval (string, optional) Ignore same log lines more info\nDefault: -\nignoreRepeatedLogInterval (string, optional) Ignore repeated log lines more info\nDefault: -\nenableMsgpackTimeSupport (bool, optional) Allows Time object in buffer’s MessagePack serde more info\nDefault: -\npodPriorityClassName (string, optional) Default: -\nfluentLogDestination (string, optional) Default: -\nfluentOutLogrotate (*FluentOutLogrotate, optional) FluentOutLogrotate sends fluent’s stdout to file and rotates it\nDefault: -\nforwardInputConfig (*input.ForwardInputConfig, optional) Default: -\nserviceAccount (*typeoverride.ServiceAccount, optional) Default: -\ndnsPolicy (corev1.DNSPolicy, optional) Default: -\ndnsConfig (*corev1.PodDNSConfig, optional) Default: -\nextraArgs ([]string, optional) Default: -\ncompressConfigFile (bool, optional) Default: -\nFluentOutLogrotate enabled (bool, required) Default: -\npath (string, optional) Default: -\nage (string, optional) Default: -\nsize (string, optional) Default: -\nExtraVolume ExtraVolume defines the fluentd extra volumes\nvolumeName (string, optional) Default: -\npath (string, optional) Default: -\ncontainerName (string, optional) Default: -\nvolume (*volume.KubernetesVolume, optional) Default: -\nFluentdScaling FluentdScaling enables configuring the scaling behaviour of the fluentd statefulset\nreplicas (int, optional) Default: -\npodManagementPolicy (string, optional) Default: -\ndrain (FluentdDrainConfig, optional) Default: -\nFluentdTLS FluentdTLS defines the TLS configs\nenabled (bool, required) Default: -\nsecretName (string, optional) Default: -\nsharedKey (string, optional) Default: -\nFluentdDrainConfig FluentdDrainConfig enables configuring the drain behavior when scaling down the fluentd statefulset\nenabled (bool, optional) Should buffers on persistent volumes left after scaling down the statefulset be drained\nDefault: -\nannotations (map[string]string, optional) Container image to use for the drain watch sidecar\nDefault: -\ndeleteVolume (bool, optional) Should persistent volume claims be deleted after draining is done\nDefault: -\nimage (ImageSpec, optional) Default: -\npauseImage (ImageSpec, optional) Container image to use for the fluentd placeholder pod\nDefault: -\n","categories":"","description":"","excerpt":"FluentdSpec FluentdSpec defines the desired state of Fluentd …","ref":"/4.2/docs/configuration/crds/v1beta1/fluentd_types/","tags":"","title":"FluentdSpec"},{"body":"Format output records Overview Specify how to format output records. For details, see https://docs.fluentd.org/configuration/format-section.\nExample spec: format: path: /tmp/logs/${tag}/%Y/%m/%d.%H.%M format: type: single_value add_newline: true message_key: msg Configuration Format type (string, optional) Output line formatting: out_file,json,ltsv,csv,msgpack,hash,single_value\nDefault: json\nadd_newline (*bool, optional) When type is single_value add ‘\\n’ to the end of the message\nDefault: true\nmessage_key (string, optional) When type is single_value specify the key holding information\nDefault: -\n","categories":"","description":"","excerpt":"Format output records Overview Specify how to format output records. …","ref":"/4.2/docs/configuration/plugins/outputs/format/","tags":"","title":"Format"},{"body":"FormatRfc5424 type (string, optional) Output line formatting: out_file,json,ltsv,csv,msgpack,hash,single_value\nDefault: json\nrfc6587_message_size (*bool, optional) Prepends message length for syslog transmission\nDefault: true\nhostname_field (string, optional) Sets host name in syslog from field in fluentd, delimited by ‘.’\nDefault: hostname\napp_name_field (string, optional) Sets app name in syslog from field in fluentd, delimited by ‘.’\nDefault: app_name\nproc_id_field (string, optional) Sets proc id in syslog from field in fluentd, delimited by ‘.’\nDefault: proc_id\nmessage_id_field (string, optional) Sets msg id in syslog from field in fluentd, delimited by ‘.’\nDefault: message_id\nstructured_data_field (string, optional) Sets structured data in syslog from field in fluentd, delimited by ‘.’ (default structured_data)\nDefault: -\nlog_field (string, optional) Sets log in syslog from field in fluentd, delimited by ‘.’\nDefault: log\n","categories":"","description":"","excerpt":"FormatRfc5424 type (string, optional) Output line formatting: …","ref":"/4.2/docs/configuration/plugins/outputs/format_rfc5424/","tags":"","title":"Format rfc5424"},{"body":"ForwardOutput servers ([]FluentdServer, required) Server definitions at least one is required Server\nDefault: -\ntransport (string, optional) The transport protocol to use [ tcp, tls ]\nDefault: -\nrequire_ack_response (bool, optional) Change the protocol to at-least-once. The plugin waits the ack from destination’s in_forward plugin.\nDefault: -\nack_response_timeout (int, optional) This option is used when require_ack_response is true. This default value is based on popular tcp_syn_retries.\nDefault: 190\nsend_timeout (int, optional) The timeout time when sending event logs.\nDefault: 60\nconnect_timeout (int, optional) The timeout time for socket connect. When the connection timed out during establishment, Errno::ETIMEDOUT is raised.\nDefault: -\nrecover_wait (int, optional) The wait time before accepting a server fault recovery.\nDefault: 10\nheartbeat_type (string, optional) The transport protocol to use for heartbeats. Set “none” to disable heartbeat. [transport, tcp, udp, none]\nDefault: -\nheartbeat_interval (int, optional) The interval of the heartbeat packer.\nDefault: 1\nphi_failure_detector (bool, optional) Use the “Phi accrual failure detector” to detect server failure.\nDefault: true\nphi_threshold (int, optional) The threshold parameter used to detect server faults. phi_threshold is deeply related to heartbeat_interval. If you are using longer heartbeat_interval, please use the larger phi_threshold. Otherwise you will see frequent detachments of destination servers. The default value 16 is tuned for heartbeat_interval 1s.\nDefault: 16\nhard_timeout (int, optional) The hard timeout used to detect server failure. The default value is equal to the send_timeout parameter.\nDefault: 60\nexpire_dns_cache (int, optional) Set TTL to expire DNS cache in seconds. Set 0 not to use DNS Cache.\nDefault: 0\ndns_round_robin (bool, optional) Enable client-side DNS round robin. Uniform randomly pick an IP address to send data when a hostname has several IP addresses. heartbeat_type udp is not available with dns_round_robin true. Use heartbeat_type tcp or heartbeat_type none.\nDefault: -\nignore_network_errors_at_startup (bool, optional) Ignore DNS resolution and errors at startup time.\nDefault: -\ntls_version (string, optional) The default version of TLS transport. [TLSv1_1, TLSv1_2]\nDefault: TLSv1_2\ntls_ciphers (string, optional) The cipher configuration of TLS transport.\nDefault: ALL:!aNULL:!eNULL:!SSLv2\ntls_insecure_mode (bool, optional) Skip all verification of certificates or not.\nDefault: false\ntls_allow_self_signed_cert (bool, optional) Allow self signed certificates or not.\nDefault: false\ntls_verify_hostname (bool, optional) Verify hostname of servers and certificates or not in TLS transport.\nDefault: true\ntls_cert_path (*secret.Secret, optional) The additional CA certificate path for TLS.\nDefault: -\ntls_client_cert_path (*secret.Secret, optional) The client certificate path for TLS\nDefault: -\ntls_client_private_key_path (*secret.Secret, optional) The client private key path for TLS.\nDefault: -\ntls_client_private_key_passphrase (*secret.Secret, optional) The client private key passphrase for TLS.\nDefault: -\ntls_cert_thumbprint (string, optional) The certificate thumbprint for searching from Windows system certstore This parameter is for Windows only.\nDefault: -\ntls_cert_logical_store_name (string, optional) The certificate logical store name on Windows system certstore. This parameter is for Windows only.\nDefault: -\ntls_cert_use_enterprise_store (bool, optional) Enable to use certificate enterprise store on Windows system certstore. This parameter is for Windows only.\nDefault: -\nkeepalive (bool, optional) Enable keepalive connection.\nDefault: false\nkeepalive_timeout (int, optional) Expired time of keepalive. Default value is nil, which means to keep connection as long as possible.\nDefault: 0\nsecurity (*common.Security, optional) Security\nDefault: -\nverify_connection_at_startup (bool, optional) Verify that a connection can be made with one of out_forward nodes at the time of startup.\nDefault: false\nbuffer (*Buffer, optional) Buffer\nDefault: -\nslow_flush_log_threshold (string, optional) The threshold for chunk flush performance check. Parameter type is float, not time, default: 20.0 (seconds) If chunk flush takes longer time than this threshold, fluentd logs warning message and increases metric fluentd_output_status_slow_flush_count.\nDefault: -\nFluentd Server server\nhost (string, required) The IP address or host name of the server.\nDefault: -\nname (string, optional) The name of the server. Used for logging and certificate verification in TLS transport (when host is address).\nDefault: -\nport (int, optional) The port number of the host. Note that both TCP packets (event stream) and UDP packets (heartbeat message) are sent to this port.\nDefault: 24224\nshared_key (*secret.Secret, optional) The shared key per server.\nDefault: -\nusername (*secret.Secret, optional) The username for authentication.\nDefault: -\npassword (*secret.Secret, optional) The password for authentication.\nDefault: -\nstandby (bool, optional) Marks a node as the standby node for an Active-Standby model between Fluentd nodes. When an active node goes down, the standby node is promoted to an active node. The standby node is not used by the out_forward plugin until then.\nDefault: -\nweight (int, optional) The load balancing weight. If the weight of one server is 20 and the weight of the other server is 30, events are sent in a 2:3 ratio. .\nDefault: 60\n","categories":"","description":"","excerpt":"ForwardOutput servers ([]FluentdServer, required) Server definitions …","ref":"/4.2/docs/configuration/plugins/outputs/forward/","tags":"","title":"Forward"},{"body":"GELF Output Overview Fluentd output plugin for GELF.\nConfiguration Output Config host (string, required) Destination host\nDefault: -\nport (int, required) Destination host port\nDefault: -\nprotocol (string, optional) Transport Protocol\nDefault: “udp”\ntls (*bool, optional) Enable TlS\nDefault: false\ntls_options (map[string]string, optional) TLS Options - for options see https://github.com/graylog-labs/gelf-rb/blob/72916932b789f7a6768c3cdd6ab69a3c942dbcef/lib/gelf/transport/tcp_tls.rb#L7-L12\nDefault: {}\nExample GELF output configurations apiVersion: logging.banzaicloud.io/v1beta1 kind: Output metadata: name: gelf-output-sample spec: gelf: host: gelf-host port: 12201 Fluentd Config Result \u003cmatch **\u003e @type gelf @id test_gelf host gelf-host port 12201 \u003c/match\u003e ","categories":"","description":"","excerpt":"GELF Output Overview Fluentd output plugin for GELF.\nConfiguration …","ref":"/4.2/docs/configuration/plugins/outputs/gelf/","tags":"","title":"GELF"},{"body":"Fluentd GeoIP filter Overview Fluentd Filter plugin to add information about geographical location of IP addresses with Maxmind GeoIP databases. More information at https://github.com/y-ken/fluent-plugin-geoip\nConfiguration GeoIP geoip_lookup_keys (string, optional) Specify one or more geoip lookup field which has ip address\nDefault: host\ngeoip_database (string, optional) Specify optional geoip database (using bundled GeoLiteCity databse by default)\nDefault: -\ngeoip2_database (string, optional) Specify optional geoip2 database (using bundled GeoLite2-City.mmdb by default)\nDefault: -\nbackend_library (string, optional) Specify backend library (geoip2_c, geoip, geoip2_compat)\nDefault: -\nskip_adding_null_record (*bool, optional) To avoid get stacktrace error with [null, null] array for elasticsearch.\nDefault: true\nrecords ([]Record, optional) Records are represented as maps: key: value\nDefault: -\nExample GeoIP filter configurations apiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: demo-flow spec: filters: - geoip: geoip_lookup_keys: remote_addr records: - city: ${city.names.en[\"remote_addr\"]} location_array: '''[${location.longitude[\"remote\"]},${location.latitude[\"remote\"]}]''' country: ${country.iso_code[\"remote_addr\"]} country_name: ${country.names.en[\"remote_addr\"]} postal_code: ${postal.code[\"remote_addr\"]} selectors: {} localOutputRefs: - demo-output Fluentd Config Result \u003cfilter **\u003e @type geoip @id test_geoip geoip_lookup_keys remote_addr skip_adding_null_record true \u003crecord\u003e city ${city.names.en[\"remote_addr\"]} country ${country.iso_code[\"remote_addr\"]} country_name ${country.names.en[\"remote_addr\"]} location_array '[${location.longitude[\"remote\"]},${location.latitude[\"remote\"]}]' postal_code ${postal.code[\"remote_addr\"]} \u003c/record\u003e \u003c/filter\u003e ","categories":"","description":"","excerpt":"Fluentd GeoIP filter Overview Fluentd Filter plugin to add information …","ref":"/4.2/docs/configuration/plugins/filters/geoip/","tags":"","title":"Geo IP"},{"body":"Google Cloud Storage Overview Store logs in Google Cloud Storage. For details, see https://github.com/kube-logging/fluent-plugin-gcs.\nExample spec: gcs: project: logging-example bucket: banzai-log-test path: logs/${tag}/%Y/%m/%d/ Configuration GCSOutput project (string, required) Project identifier for GCS\nDefault: -\nkeyfile (string, optional) Path of GCS service account credentials JSON file\nDefault: -\ncredentials_json (*secret.Secret, optional) GCS service account credentials in JSON format Secret\nDefault: -\nclient_retries (int, optional) Number of times to retry requests on server error\nDefault: -\nclient_timeout (int, optional) Default timeout to use in requests\nDefault: -\nbucket (string, required) Name of a GCS bucket\nDefault: -\nobject_key_format (string, optional) Format of GCS object keys\nDefault: %{path}%{time_slice}_%{index}.%{file_extension}\npath (string, optional) Path prefix of the files on GCS\nDefault: -\nstore_as (string, optional) Archive format on GCS: gzip json text\nDefault: gzip\ntranscoding (bool, optional) Enable the decompressive form of transcoding\nDefault: -\nauto_create_bucket (bool, optional) Create GCS bucket if it does not exists\nDefault: true\nhex_random_length (int, optional) Max length of %{hex_random} placeholder(4-16)\nDefault: 4\noverwrite (bool, optional) Overwrite already existing path\nDefault: false\nacl (string, optional) Permission for the object in GCS: auth_read owner_full owner_read private project_private public_read\nDefault: -\nstorage_class (string, optional) Storage class of the file: dra nearline coldline multi_regional regional standard\nDefault: -\nencryption_key (string, optional) Customer-supplied, AES-256 encryption key\nDefault: -\nobject_metadata ([]ObjectMetadata, optional) User provided web-safe keys and arbitrary string values that will returned with requests for the file as “x-goog-meta-” response headers. Object Metadata\nDefault: -\nformat (*Format, optional) Format\nDefault: -\nbuffer (*Buffer, optional) Buffer\nDefault: -\nslow_flush_log_threshold (string, optional) The threshold for chunk flush performance check. Parameter type is float, not time, default: 20.0 (seconds) If chunk flush takes longer time than this threshold, fluentd logs warning message and increases metric fluentd_output_status_slow_flush_count.\nDefault: -\nObjectMetadata key (string, required) Key\nDefault: -\nvalue (string, required) Value\nDefault: -\n","categories":"","description":"","excerpt":"Google Cloud Storage Overview Store logs in Google Cloud Storage. For …","ref":"/4.2/docs/configuration/plugins/outputs/gcs/","tags":"","title":"Google Cloud Storage"},{"body":"Loki output plugin Overview Fluentd output plugin to ship logs to a Loki server. More info at https://grafana.com/docs/loki/latest/clients/fluentd/\nExample: Store Nginx Access Logs in Grafana Loki with Logging Operator\nExample output configurations spec: loki: url: http://loki:3100 buffer: timekey: 1m timekey_wait: 30s timekey_use_utc: true Configuration Output Config url (string, optional) The url of the Loki server to send logs to.\nDefault: https://logs-us-west1.grafana.net\nusername (*secret.Secret, optional) Specify a username if the Loki server requires authentication. Secret\nDefault: -\npassword (*secret.Secret, optional) Specify password if the Loki server requires authentication. Secret\nDefault: -\ncert (*secret.Secret, optional) TLS: parameters for presenting a client certificate Secret\nDefault: -\nkey (*secret.Secret, optional) TLS: parameters for presenting a client certificate Secret\nDefault: -\nca_cert (*secret.Secret, optional) TLS: CA certificate file for server certificate verification Secret\nDefault: -\ninsecure_tls (*bool, optional) TLS: disable server certificate verification\nDefault: false\ntenant (string, optional) Loki is a multi-tenant log storage platform and all requests sent must include a tenant.\nDefault: -\nlabels (Label, optional) Set of labels to include with every Loki stream.\nDefault: -\nextra_labels (map[string]string, optional) Set of extra labels to include with every Loki stream.\nDefault: -\nline_format (string, optional) Format to use when flattening the record to a log line: json, key_value (default: key_value)\nDefault: json\nextract_kubernetes_labels (*bool, optional) Extract kubernetes labels as loki labels\nDefault: false\nremove_keys ([]string, optional) Comma separated list of needless record keys to remove\nDefault: []\ndrop_single_key (*bool, optional) If a record only has 1 key, then just set the log line to the value and discard the key.\nDefault: false\nconfigure_kubernetes_labels (*bool, optional) Configure Kubernetes metadata in a Prometheus like format\nDefault: false\nbuffer (*Buffer, optional) Buffer\nDefault: -\nslow_flush_log_threshold (string, optional) The threshold for chunk flush performance check. Parameter type is float, not time, default: 20.0 (seconds) If chunk flush takes longer time than this threshold, fluentd logs warning message and increases metric fluentd_output_status_slow_flush_count.\nDefault: -\n","categories":"","description":"","excerpt":"Loki output plugin Overview Fluentd output plugin to ship logs to a …","ref":"/4.2/docs/configuration/plugins/outputs/loki/","tags":"","title":"Grafana Loki"},{"body":"Grep Filter Overview The grep filter plugin “greps” events by the values of specified fields.\nConfiguration GrepConfig regexp ([]RegexpSection, optional) Regexp Directive\nDefault: -\nexclude ([]ExcludeSection, optional) Exclude Directive\nDefault: -\nor ([]OrSection, optional) Or Directive\nDefault: -\nand ([]AndSection, optional) And Directive\nDefault: -\nRegexp Directive Specify filtering rule. This directive contains two parameters.\nkey (string, required) Specify field name in the record to parse.\nDefault: -\npattern (string, required) Pattern expression to evaluate\nDefault: -\nExample Regexp filter configurations apiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: demo-flow spec: filters: - grep: regexp: - key: first pattern: /^5\\d\\d$/ selectors: {} localOutputRefs: - demo-output Fluentd Config Result \u003cfilter **\u003e @type grep @id demo-flow_1_grep \u003cregexp\u003e key first pattern /^5\\d\\d$/ \u003c/regexp\u003e \u003c/filter\u003e Exclude Directive Specify filtering rule to reject events. This directive contains two parameters.\nkey (string, required) Specify field name in the record to parse.\nDefault: -\npattern (string, required) Pattern expression to evaluate\nDefault: -\nExample Exclude filter configurations apiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: demo-flow spec: filters: - grep: exclude: - key: first pattern: /^5\\d\\d$/ selectors: {} localOutputRefs: - demo-output Fluentd Config Result \u003cfilter **\u003e @type grep @id demo-flow_0_grep \u003cexclude\u003e key first pattern /^5\\d\\d$/ \u003c/exclude\u003e \u003c/filter\u003e Or Directive Specify filtering rule. This directive contains either regexp or exclude directive.\nregexp ([]RegexpSection, optional) Regexp Directive\nDefault: -\nexclude ([]ExcludeSection, optional) Exclude Directive\nDefault: -\nExample Or filter configurations apiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: demo-flow spec: filters: - grep: or: - exclude: - key: first pattern: /^5\\d\\d$/ - key: second pattern: /\\.css$/ selectors: {} localOutputRefs: - demo-output Fluentd Config Result \u003cor\u003e \u003cexclude\u003e key first pattern /^5\\d\\d$/ \u003c/exclude\u003e \u003cexclude\u003e key second pattern /\\.css$/ \u003c/exclude\u003e \u003c/or\u003e And Directive Specify filtering rule. This directive contains either regexp or exclude directive.\nregexp ([]RegexpSection, optional) Regexp Directive\nDefault: -\nexclude ([]ExcludeSection, optional) Exclude Directive\nDefault: -\nExample And filter configurations apiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: demo-flow spec: filters: - grep: and: - regexp: - key: first pattern: /^5\\d\\d$/ - key: second pattern: /\\.css$/ selectors: {} localOutputRefs: - demo-output Fluentd Config Result \u003cand\u003e \u003cregexp\u003e key first pattern /^5\\d\\d$/ \u003c/regexp\u003e \u003cregexp\u003e key second pattern /\\.css$/ \u003c/regexp\u003e \u003c/and\u003e ","categories":"","description":"","excerpt":"Grep Filter Overview The grep filter plugin “greps” events by the …","ref":"/4.2/docs/configuration/plugins/filters/grep/","tags":"","title":"Grep"},{"body":"HostTailerSpec HostTailerSpec defines the desired state of HostTailer\nfileTailers ([]FileTailer, optional) List of file tailers.\nDefault: -\nsystemdTailers ([]SystemdTailer, optional) List of systemd tailers.\nDefault: -\nenableRecreateWorkloadOnImmutableFieldChange (bool, optional) EnableRecreateWorkloadOnImmutableFieldChange enables the operator to recreate the daemonset (and possibly other resource in the future) in case there is a change in an immutable field that otherwise couldn’t be managed with a simple update.\nDefault: -\nworkloadMetaOverrides (*types.MetaBase, optional) Override metadata of the created resources\nDefault: -\nworkloadOverrides (*types.PodSpecBase, optional) Override podSpec fields for the given daemonset\nDefault: -\nHostTailerStatus HostTailerStatus defines the observed state of HostTailer.\nHostTailer HostTailer is the Schema for the hosttailers API\n(metav1.TypeMeta, required) Default: -\nmetadata (metav1.ObjectMeta, optional) Default: -\nspec (HostTailerSpec, optional) Default: -\nstatus (HostTailerStatus, optional) Default: -\nHostTailerList HostTailerList contains a list of HostTailers.\n(metav1.TypeMeta, required) Default: -\nmetadata (metav1.ListMeta, optional) Default: -\nitems ([]HostTailer, required) Default: -\nFileTailer FileTailer configuration options\nname (string, required) Name for the tailer\nDefault: -\npath (string, optional) Path to the loggable file\nDefault: -\ndisabled (bool, optional) Disable tailing the file\nDefault: -\nbuffer_max_size (string, optional) Set the limit of the buffer size per active filetailer\nDefault: -\nbuffer_chunk_size (string, optional) Set the buffer chunk size per active filetailer\nDefault: -\nskip_long_lines (string, optional) Skip long line when exceeding Buffer_Max_Size\nDefault: -\nread_from_head (bool, optional) Start reading from the head of new log files\nDefault: -\nbuffer_max_size (string, optional) Set the limit of the buffer size per active filetailer\nDefault: -\nbuffer_chunk_size (string, optional) Set the buffer chunk size per active filetailer\nDefault: -\nskip_long_lines (string, optional) Skip long line when exceeding Buffer_Max_Size\nDefault: -\nread_from_head (bool, optional) Start reading from the head of new log files\nDefault: -\ncontainerOverrides (*types.ContainerBase, optional) Override container fields for the given tailer\nDefault: -\nSystemdTailer SystemdTailer configuration options\nname (string, required) Name for the tailer\nDefault: -\npath (string, optional) Override systemd log path\nDefault: -\ndisabled (bool, optional) Disable component\nDefault: -\nsystemdFilter (string, optional) Filter to select systemd unit example: kubelet.service\nDefault: -\nmaxEntries (int, optional) Maximum entries to read when starting to tail logs to avoid high pressure\nDefault: -\ncontainerOverrides (*types.ContainerBase, optional) Override container fields for the given tailer\nDefault: -\n","categories":"","description":"","excerpt":"HostTailerSpec HostTailerSpec defines the desired state of HostTailer …","ref":"/4.2/docs/configuration/crds/extensions/hosttailer_types/","tags":"","title":"HostTailer"},{"body":"Http plugin for Fluentd Overview Sends logs to HTTP/HTTPS endpoints. More info at https://docs.fluentd.org/output/http.\nExample output configurations spec: http: endpoint: http://logserver.com:9000/api buffer: tags: \"[]\" flush_interval: 10s Configuration Output Config endpoint (string, required) Endpoint for HTTP request.\nDefault: -\nhttp_method (string, optional) Method for HTTP request. [post, put]\nDefault: post\nproxy (string, optional) Proxy for HTTP request.\nDefault: -\ncontent_type (string, optional) Content-Profile for HTTP request.\nDefault: -\njson_array (bool, optional) Using array format of JSON. This parameter is used and valid only for json format. When json_array as true, Content-Profile should be application/json and be able to use JSON data for the HTTP request body.\nDefault: false\nformat (*Format, optional) Format\nDefault: -\nheaders (map[string]string, optional) Additional headers for HTTP request.\nDefault: -\nopen_timeout (int, optional) Connection open timeout in seconds.\nDefault: -\nread_timeout (int, optional) Read timeout in seconds.\nDefault: -\nssl_timeout (int, optional) TLS timeout in seconds.\nDefault: -\ntls_version (string, optional) The default version of TLS transport. [TLSv1_1, TLSv1_2]\nDefault: TLSv1_2\ntls_ciphers (string, optional) The cipher configuration of TLS transport.\nDefault: ALL:!aNULL:!eNULL:!SSLv2\ntls_ca_cert_path (*secret.Secret, optional) The CA certificate path for TLS.\nDefault: -\ntls_client_cert_path (*secret.Secret, optional) The client certificate path for TLS.\nDefault: -\ntls_private_key_path (*secret.Secret, optional) The client private key path for TLS.\nDefault: -\ntls_private_key_passphrase (*secret.Secret, optional) The client private key passphrase for TLS.\nDefault: -\ntls_verify_mode (string, optional) The verify mode of TLS. [peer, none]\nDefault: peer\nerror_response_as_unrecoverable (*bool, optional) Raise UnrecoverableError when the response code is non success, 1xx/3xx/4xx/5xx. If false, the plugin logs error message instead of raising UnrecoverableError.\nDefault: true\nretryable_response_codes ([]int, optional) List of retryable response codes. If the response code is included in this list, the plugin retries the buffer flush. Since Fluentd v2 the Status code 503 is going to be removed from default.\nDefault: [503]\nauth (*HTTPAuth, optional) HTTP auth\nDefault: -\nbuffer (*Buffer, optional) Buffer\nDefault: -\nslow_flush_log_threshold (string, optional) The threshold for chunk flush performance check. Parameter type is float, not time, default: 20.0 (seconds) If chunk flush takes longer time than this threshold, fluentd logs warning message and increases metric fluentd_output_status_slow_flush_count.\nDefault: -\nHTTP auth config http_auth\nusername (*secret.Secret, required) {#http auth-config-username} Username for basic authentication. Secret\nDefault: -\npassword (*secret.Secret, required) {#http auth-config-password} Password for basic authentication. Secret\nDefault: -\n","categories":"","description":"","excerpt":"Http plugin for Fluentd Overview Sends logs to HTTP/HTTPS endpoints. …","ref":"/4.2/docs/configuration/plugins/outputs/http/","tags":"","title":"Http"},{"body":"Sending messages over HTTP Overview More info at https://www.syslog-ng.com/technical-documents/doc/syslog-ng-open-source-edition/3.37/administration-guide/40#TOPIC-1829058\nConfiguration HTTPOutput url (string, optional) Specifies the hostname or IP address and optionally the port number of the web service that can receive log data via HTTP. Use a colon (:) after the address to specify the port number of the server. For example: http://127.0.0.1:8000\nDefault: -\nheaders ([]string, optional) Custom HTTP headers to include in the request, for example, headers(“HEADER1: header1”, “HEADER2: header2”).\nDefault: empty\ntime_reopen (int, optional) The time to wait in seconds before a dead connection is reestablished.\nDefault: 60\ntls (*TLS, optional) This option sets various options related to TLS encryption, for example, key/certificate files and trusted CA locations. TLS can be used only with tcp-based transport protocols. For details, see TLS for syslog-ng outputs and the syslog-ng documentation.\nDefault: -\ndisk_buffer (*DiskBuffer, optional) This option enables putting outgoing messages into the disk buffer of the destination to avoid message loss in case of a system failure on the destination side. For details, see the Syslog-ng DiskBuffer options.\nDefault: false\n(Batch, required) Batching parameters\nDefault: -\nbody (string, optional) The body of the HTTP request, for example, body(\"${ISODATE} ${MESSAGE}\"). You can use strings, macros, and template functions in the body. If not set, it will contain the message received from the source by default.\nDefault: -\nbody-prefix (string, optional) The string syslog-ng OSE puts at the beginning of the body of the HTTP request, before the log message.\nDefault: -\nbody-suffix (string, optional) The string syslog-ng OSE puts to the end of the body of the HTTP request, after the log message.\nDefault: -\ndelimiter (string, optional) By default, syslog-ng OSE separates the log messages of the batch with a newline character.\nDefault: -\nmethod (string, optional) Specifies the HTTP method to use when sending the message to the server. POST | PUT\nDefault: -\nretries (int, optional) The number of times syslog-ng OSE attempts to send a message to this destination. If syslog-ng OSE could not send a message, it will try again until the number of attempts reaches retries, then drops the message.\nDefault: -\nuser (string, optional) The username that syslog-ng OSE uses to authenticate on the server where it sends the messages.\nDefault: -\npassword (secret.Secret, optional) The password that syslog-ng OSE uses to authenticate on the server where it sends the messages.\nDefault: -\nuser-agent (string, optional) The value of the USER-AGENT header in the messages sent to the server.\nDefault: -\nworkers (int, optional) Description: Specifies the number of worker threads (at least 1) that syslog-ng OSE uses to send messages to the server. Increasing the number of worker threads can drastically improve the performance of the destination.\nDefault: -\npersist_name (string, optional) Default: -\nBatch batch-lines (int, optional) Description: Specifies how many lines are flushed to a destination in one batch. The syslog-ng OSE application waits for this number of lines to accumulate and sends them off in a single batch. Increasing this number increases throughput as more messages are sent in a single batch, but also increases message latency. For example, if you set batch-lines() to 100, syslog-ng OSE waits for 100 messages.\nDefault: -\nbatch-bytes (int, optional) Description: Sets the maximum size of payload in a batch. If the size of the messages reaches this value, syslog-ng OSE sends the batch to the destination even if the number of messages is less than the value of the batch-lines() option. Note that if the batch-timeout() option is enabled and the queue becomes empty, syslog-ng OSE flushes the messages only if batch-timeout() expires, or the batch reaches the limit set in batch-bytes().\nDefault: -\nbatch-timeout (int, optional) Description: Specifies the time syslog-ng OSE waits for lines to accumulate in the output buffer. The syslog-ng OSE application sends batches to the destinations evenly. The timer starts when the first message arrives to the buffer, so if only few messages arrive, syslog-ng OSE sends messages to the destination at most once every batch-timeout() milliseconds.\nDefault: -\n","categories":"","description":"","excerpt":"Sending messages over HTTP Overview More info at …","ref":"/4.2/docs/configuration/plugins/syslog-ng-outputs/http/","tags":"","title":"HTTP"},{"body":"Kafka output plugin for Fluentd Overview More info at https://github.com/fluent/fluent-plugin-kafka\nExample Deployment: Transport Nginx Access Logs into Kafka with Logging Operator\nExample output configurations spec: kafka: brokers: kafka-headless.kafka.svc.cluster.local:29092 default_topic: topic sasl_over_ssl: false format: type: json buffer: tags: topic timekey: 1m timekey_wait: 30s timekey_use_utc: true Configuration Kafka Send your logs to Kafka\nbrokers (string, required) The list of all seed brokers, with their host and port information.\nDefault: -\ntopic_key (string, optional) Topic Key\nDefault: “topic”\npartition_key (string, optional) Partition\nDefault: “partition”\npartition_key_key (string, optional) Partition Key\nDefault: “partition_key”\nmessage_key_key (string, optional) Message Key\nDefault: “message_key”\nclient_id (string, optional) Client ID\nDefault: “kafka”\ndefault_topic (string, optional) The name of default topic .\nDefault: nil\ndefault_partition_key (string, optional) The name of default partition key .\nDefault: nil\ndefault_message_key (string, optional) The name of default message key .\nDefault: nil\nexclude_topic_key (bool, optional) Exclude Topic key\nDefault: false\nexclude_partion_key (bool, optional) Exclude Partition key\nDefault: false\nget_kafka_client_log (bool, optional) Get Kafka Client log\nDefault: false\nheaders (map[string]string, optional) Headers\nDefault: {}\nheaders_from_record (map[string]string, optional) Headers from Record\nDefault: {}\nuse_default_for_unknown_topic (bool, optional) Use default for unknown topics\nDefault: false\nidempotent (bool, optional) Idempotent\nDefault: false\nsasl_over_ssl (bool, required) SASL over SSL\nDefault: true\nprincipal (string, optional) Default: -\nkeytab (*secret.Secret, optional) Default: -\nusername (*secret.Secret, optional) Username when using PLAIN/SCRAM SASL authentication\nDefault: -\npassword (*secret.Secret, optional) Password when using PLAIN/SCRAM SASL authentication\nDefault: -\nscram_mechanism (string, optional) If set, use SCRAM authentication with specified mechanism. When unset, default to PLAIN authentication\nDefault: -\nmax_send_retries (int, optional) Number of times to retry sending of messages to a leader\nDefault: 1\nrequired_acks (int, optional) The number of acks required per request .\nDefault: -1\nack_timeout (int, optional) How long the producer waits for acks. The unit is seconds\nDefault: nil =\u003e Uses default of ruby-kafka library\ncompression_codec (string, optional) The codec the producer uses to compress messages . The available options are gzip and snappy.\nDefault: nil\nkafka_agg_max_bytes (int, optional) Maximum value of total message size to be included in one batch transmission. .\nDefault: 4096\nkafka_agg_max_messages (int, optional) Maximum number of messages to include in one batch transmission. .\nDefault: nil\ndiscard_kafka_delivery_failed (bool, optional) Discard the record where Kafka DeliveryFailed occurred\nDefault: false\nssl_ca_certs_from_system (*bool, optional) System’s CA cert store\nDefault: false\nssl_ca_cert (*secret.Secret, optional) CA certificate\nDefault: -\nssl_client_cert (*secret.Secret, optional) Client certificate\nDefault: -\nssl_client_cert_chain (*secret.Secret, optional) Client certificate chain\nDefault: -\nssl_client_cert_key (*secret.Secret, optional) Client certificate key\nDefault: -\nssl_verify_hostname (*bool, optional) Verify certificate hostname\nDefault: -\nformat (*Format, required) Format\nDefault: -\nbuffer (*Buffer, optional) Buffer\nDefault: -\nslow_flush_log_threshold (string, optional) The threshold for chunk flush performance check. Parameter type is float, not time, default: 20.0 (seconds) If chunk flush takes longer time than this threshold, fluentd logs warning message and increases metric fluentd_output_status_slow_flush_count.\nDefault: -\n","categories":"","description":"","excerpt":"Kafka output plugin for Fluentd Overview More info at …","ref":"/4.2/docs/configuration/plugins/outputs/kafka/","tags":"","title":"Kafka"},{"body":"Kubernetes Events Timestamp Filter Overview Fluentd Filter plugin to select particular timestamp into an additional field\nConfiguration KubeEventsTimestampConfig timestamp_fields ([]string, optional) Time field names in order of relevance\nDefault: event.eventTime, event.lastTimestamp, event.firstTimestamp\nmapped_time_key (string, optional) Added time field name\nDefault: triggerts\nExample Kubernetes Events Timestamp filter configurations apiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: es-flow spec: filters: - kube_events_timestamp: timestamp_fields: - \"event.eventTime\" - \"event.lastTimestamp\" - \"event.firstTimestamp\" mapped_time_key: mytimefield selectors: {} localOutputRefs: - es-output Fluentd Config Result \u003cfilter **\u003e @type kube_events_timestamp @id test-kube-events-timestamp timestamp_fields [\"event.eventTime\",\"event.lastTimestamp\",\"event.firstTimestamp\"] mapped_time_key mytimefield \u003c/filter\u003e ","categories":"","description":"","excerpt":"Kubernetes Events Timestamp Filter Overview Fluentd Filter plugin to …","ref":"/4.2/docs/configuration/plugins/filters/kube_events_timestamp/","tags":"","title":"Kubernetes Events Timestamp"},{"body":"Kubernetes host tailer allows you to tail logs like kubelet, audit logs, or the systemd journal from the nodes.\nCreate host tailer To tail logs from the node’s host filesystem, define one or more file tailers in the host-tailer configuration.\nkubectl apply -f - \u003c\u003cEOF apiVersion: logging-extensions.banzaicloud.io/v1alpha1 kind: HostTailer metadata: name: multi-sample spec: # list of File tailers fileTailers: - name: nginx-access path: /var/log/nginx/access.log - name: nginx-error path: /var/log/nginx/error.log # list of Systemd tailers systemdTailers: - name: my-systemd-tailer maxEntries: 100 systemdFilter: kubelet.service EOF Create file tailer When an application (mostly legacy programs) is not logging in a Kubernetes-native way, Logging operator cannot process its logs. (For example, an old application does not send its logs to stdout, but uses some log files instead.) File-tailer helps to solve this problem: It configures Fluent Bit to tail the given file(s), and sends the logs to the stdout, to implement Kubernetes-native logging.\nHowever, file-tailer cannot access the pod’s local dir, so the logfiles need to be written on a mounted volume.\nLet’s assume the following code represents a legacy application that generates logs into the /legacy-logs/date.log file. While the legacy-logs directory is mounted, it’s accessible from other pods by mounting the same volume.\nkubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: Pod metadata: name: test-pod spec: containers: - image: busybox name: test volumeMounts: - mountPath: /legacy-logs name: test-volume command: [\"/bin/sh\", \"-c\"] args: - while true; do date \u003e\u003e /legacy-logs/date.log; sleep 1; done volumes: - name: test-volume hostPath: path: /legacy-logs EOF To tail the logs of the previous example application, you can use the following host-tailer custom resource:\nkubectl apply -f - \u003c\u003cEOF apiVersion: logging-extensions.banzaicloud.io/v1alpha1 kind: HostTailer metadata: name: file-hosttailer-sample spec: fileTailers: - name: sample-logfile path: /legacy-logs/date.log disabled: false EOF Logging operator configure the environment and start a file-tailer pod. It’s also able to deal with multi-node clusters, since is starts the host-tailer pod through a daemonset.\nCheck the created file tailer pod:\nkubectl get pod The output should be similar to:\nNAME READY STATUS RESTARTS AGE file-hosttailer-sample-host-tailer-5tqhv 1/1 Running 0 117s test-pod 1/1 Running 0 5m40s Checking the logs of the file-tailer's pod. You will see the logfile’s content on stdout. This way Logging operator can process those logs as well.\nkubectl logs file-hosttailer-sample-host-tailer-5tqhv The logs of the sample application should be similar to:\nFluent Bit v1.9.5 * Copyright (C) 2015-2022 The Fluent Bit Authors * Fluent Bit is a CNCF sub-project under the umbrella of Fluentd * https://fluentbit.io [2022/09/13 12:26:02] [ info] [fluent bit] version=1.9.5, commit=9ec43447b6, pid=1 [2022/09/13 12:26:02] [ info] [storage] version=1.2.0, type=memory-only, sync=normal, checksum=disabled, max_chunks_up=128 [2022/09/13 12:26:02] [ info] [cmetrics] version=0.3.4 [2022/09/13 12:26:02] [ info] [sp] stream processor started [2022/09/13 12:26:02] [ info] [output:file:file.0] worker #0 started [2022/09/13 12:26:02] [ info] [input:tail:tail.0] inotify_fs_add(): inode=418051 watch_fd=1 name=/legacy-logs/date.log Tue Sep 13 12:22:51 UTC 2022 Tue Sep 13 12:22:52 UTC 2022 Tue Sep 13 12:22:53 UTC 2022 Tue Sep 13 12:22:54 UTC 2022 Tue Sep 13 12:22:55 UTC 2022 Tue Sep 13 12:22:56 UTC 2022 File Tailer configuration options Variable Name Type Required Default Description name string Yes - Name for the tailer\npath string No - Path to the loggable file\ndisabled bool No - Disable tailing the file\ncontainerOverrides *types.ContainerBase No - Override container fields for the given tailer\nTail systemd journal This is a special case of file-tailer, since it tails the systemd journal file specifically.\nkubectl apply -f - \u003c\u003cEOF apiVersion: logging-extensions.banzaicloud.io/v1alpha1 kind: HostTailer metadata: name: systemd-tailer-sample spec: # list of Systemd tailers systemdTailers: - name: my-systemd-tailer maxEntries: 100 systemdFilter: kubelet.service EOF Systemd tailer configuration options Variable Name Type Required Default Description name string Yes - Name for the tailer\npath string No - Override systemd log path\ndisabled bool No - Disable component\nsystemdFilter string No - Filter to select systemd unit example: kubelet.service\nmaxEntries int No - Maximum entries to read when starting to tail logs to avoid high pressure\ncontainerOverrides *types.ContainerBase No - Override container fields for the given tailer\nExample: Configure logging Flow to route logs from a host tailer The following example uses the flow’s match term to listen the previously created file-hosttailer-sample Hosttailer’s log.\nkubectl apply -f - \u003c\u003cEOF apiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: hosttailer-flow namespace: default spec: filters: - tag_normaliser: {} # keeps data matching to label, the rest of the data will be discarded by this flow implicitly match: - select: labels: app.kubernetes.io/name: file-hosttailer-sample # there might be a need to match on container name too (in case of multiple containers) container_names: - nginx-access outputRefs: - sample-output EOF Example: Kubernetes host tailer with multiple tailers kubectl apply -f - \u003c\u003cEOF apiVersion: logging-extensions.banzaicloud.io/v1alpha1 kind: HostTailer metadata: name: multi-sample spec: # list of File tailers fileTailers: - name: nginx-access path: /var/log/nginx/access.log - name: nginx-error path: /var/log/nginx/error.log # list of Systemd tailers systemdTailers: - name: my-systemd-tailer maxEntries: 100 systemdFilter: kubelet.service EOF Set custom priority Create your own custom priority class in Kubernetes. Set its value between 0 and 2000000000. Note that:\n0 is the default priority To change the default priority, set the globalDefault key. 2000000000 and above are reserved for the Kubernetes system PriorityClass is a non-namespaced object. kubectl apply -f - \u003c\u003cEOF apiVersion: scheduling.k8s.io/v1 kind: PriorityClass metadata: name: hosttailer-priority value: 1000000 globalDefault: false description: \"This priority class should be used for hosttailer pods only.\" EOF Now you can use your private priority class name to start hosttailer/eventtailer, for example:\nkubectl apply -f -\u003c\u003cEOF apiVersion: logging-extensions.banzaicloud.io/v1alpha1 kind: HostTailer metadata: name: priority-sample spec: controlNamespace: default # Override podSpecBase variables here workloadOverrides: priorityClassName: hosttailer-priority fileTailers: - name: nginx-access path: /var/log/nginx/access.log - name: nginx-error path: /var/log/nginx/error.log EOF Configuration options Variable Name Type Required Default Description fileTailers []FileTailer No - List of file tailers\nc []SystemdTailer No - List of systemd tailers\nenableRecreateWorkloadOnImmutableFieldChange bool No - EnableRecreateWorkloadOnImmutableFieldChange enables the operator to recreate the\nfluentbit daemonset and the fluentd statefulset (and possibly other resource in the future)\nin case there is a change in an immutable field\nthat otherwise couldn’t be managed with a simple update.\nworkloadMetaOverrides *types.MetaBase No - Override metadata of the created resources\nworkloadOverrides *types.PodSpecBase No - Override podSpec fields for the given daemonset\nAdvanced configuration overrides MetaBase Variable Name Type Required Default Description annotations map[string]string No - labels map[string]string No - PodSpecBase Variable Name Type Required Default Description tolerations []corev1.Toleration No - nodeSelector map[string]string No - serviceAccountName string No - affinity *corev1.Affinity No - securityContext *corev1.PodSecurityContext No - volumes []corev1.Volume No - priorityClassName string No - ContainerBase Variable Name Type Required Default Description resources *corev1.ResourceRequirements No - image string No - pullPolicy corev1.PullPolicy No - command []string No - volumeMounts []corev1.VolumeMount No - securityContext *corev1.SecurityContext No - ","categories":"","description":"","excerpt":"Kubernetes host tailer allows you to tail logs like kubelet, audit …","ref":"/4.2/docs/configuration/extensions/kubernetes-host-tailer/","tags":"","title":"Kubernetes host logs, journals, and logfiles"},{"body":" Note: This page describes routing logs with Fluentd. If you are using syslog-ng to route your log messages, see Routing your logs with syslog-ng.\nThe first step to process your logs is to select which logs go where. The Logging operator uses Kubernetes labels, namespaces and other metadata to separate different log flows.\nAvailable routing metadata keys:\nName Type Description Empty namespaces []string List of matching namespaces All namespaces labels map[string]string Key - Value pairs of labels All labels hosts []string List of matching hosts All hosts container_names []string List of matching containers (not Pods) All containers Match statement To select or exclude logs you can use the match statement. Match is a collection of select and exclude expressions. In both expression you can use the labels attribute to filter for pod’s labels. Moreover, in Cluster flow you can use namespaces as a selecting or excluding criteria.\nIf you specify more than one label in a select or exclude expression, the labels have a logical AND connection between them. For example, an exclude expression with two labels excludes messages that have both labels. If you want an OR connection between labels, list them in separate expressions. For example, to exclude messages that have one of two specified labels, create a separate exclude expression for each label.\nThe select and exclude statements are evaluated in order!\nWithout at least one select criteria, no messages will be selected!\nFlow:\nkind: Flow metadata: name: flow-sample spec: match: - exclude: labels: exclude-this: label - select: labels: app: nginx label/xxx: example ClusterFlow:\nkind: ClusterFlow metadata: name: flow-sample spec: match: - exclude: labels: exclude-this: label namespaces: - developer - select: labels: app: nginx label/xxx: example namespaces: - production - beta Examples Example 0. Select all logs To select all logs, or if you only want to exclude some logs but retain others you need an empty select statement.\napiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: flow-all namespace: default spec: localOutputRefs: - forward-output-sample match: - select: {} Example 1. Select logs by label Select logs with app: nginx labels from the namespace:\napiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: flow-sample namespace: default spec: localOutputRefs: - forward-output-sample match: - select: labels: app: nginx Example 2. Exclude logs by label Exclude logs with app: nginx labels from the namespace\napiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: flow-sample namespace: default spec: localOutputRefs: - forward-output-sample match: - exclude: labels: app: nginx - select: {} Example 3. Exclude and select logs by label Select logs with app: nginx labels from the default namespace but exclude logs with env: dev labels\napiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: flow-sample namespace: default spec: localOutputRefs: - forward-output-sample match: - exclude: labels: env: dev - select: labels: app: nginx Example 4. Exclude cluster logs by namespace Select app: nginx from all namespaces except from dev and sandbox\napiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterFlow metadata: name: clusterflow-sample spec: globalOutputRefs: - forward-output-sample match: - exclude: namespaces: - dev - sandbox - select: labels: app: nginx Example 5. Exclude and select cluster logs by namespace Select app: nginx from all prod and infra namespaces but exclude cluster logs from dev, sandbox namespaces\napiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterFlow metadata: name: clusterflow-sample spec: globalOutputRefs: - forward-output-sample match: - exclude: namespaces: - dev - sandbox - select: labels: app: nginx namespaces: - prod - infra Example 6. Multiple labels - AND Exclude logs that have both the app: nginx and app.kubernetes.io/instance: nginx-demo labels\napiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: flow-sample namespace: default spec: localOutputRefs: - forward-output-sample match: - exclude: labels: app: nginx app.kubernetes.io/instance: nginx-demo - select: {} Example 6. Multiple labels - OR Exclude logs that have either the app: nginx or the app.kubernetes.io/instance: nginx-demo labels\napiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: flow-sample namespace: default spec: localOutputRefs: - forward-output-sample match: - exclude: labels: app: nginx - exclude: labels: app.kubernetes.io/instance: nginx-demo - select: {} ","categories":"","description":"","excerpt":" Note: This page describes routing logs with Fluentd. If you are using …","ref":"/4.2/docs/configuration/log-routing/","tags":"","title":"Routing your logs with Fluentd match directives"},{"body":" Note: This page describes routing logs with syslog-ng. If you are using Fluentd to route your log messages, see Routing your logs with Fluentd match directives.\nsyslog-ng is supported only in Logging operator 4.0 or newer.\nThe first step to process your logs is to select which logs go where.\nThe match field of the SyslogNGFlow and SyslogNGClusterFlow resources define the routing rules of the logs.\nNote: Fluentd can use only metadata to route the logs. When using syslog-ng filter expressions, you can filter both on metadata and log content as well.\nThe syntax of syslog-ng match statement is slightly different from the Fluentd match statements.\nAvailable routing metadata keys:\nName Type Description Empty namespaces []string List of matching namespaces All namespaces labels map[string]string Key - Value pairs of labels All labels hosts []string List of matching hosts All hosts container_names []string List of matching containers (not Pods) All containers Match statement Match expressions select messages by applying patterns on the content or metadata of the messages. You can use simple string matching, and also complex regular expressions. You can combine matches using the and, or, and not boolean operators to create complex expressions to select or exclude messages as needed for your use case.\nCurrently, only a pattern matching function is supported (called match in syslog-ng parlance, but renamed to regexp in the CRD to avoid confusion).\nThe match field can have one of the following options:\nregexp: A pattern that matches the value of a field or a templated value. For example:\nmatch: regexp: \u003cparameters\u003e and: Combines the nested match expressions with the logical AND operator.\nmatch: and: \u003clist of nested match expressions\u003e or: Combines the nested match expressions with the logical OR operator.\nmatch: or: \u003clist of nested match expressions\u003e not: Matches the logical NOT of the nested match expressions with the logical AND operator.\nmatch: not: \u003clist of nested match expressions\u003e regexp patterns The regexp field (called match in syslog-ng parlance, but renamed to regexp in the CRD to avoid confusion) defines the pattern that selects the matching messages. You can do two different kinds of matching:\nFind a pattern in the value of a field of the messages, for example, to select the messages of a specific application. To do that, set the pattern and value fields (and optionally the type and flags fields). Find a pattern in a template expression created from multiple fields of the message. To do that, set the pattern and template fields (and optionally the type and flags fields). CAUTION:\nYou need to use the json. prefix in field names. You can reference fields using the dot notation. For example, if the log contains {\"kubernetes\": {\"namespace_name\": \"default\"}}, then you can reference the namespace_name field using json.kubernetes.namespace_name.\nThe following example filters for specific Pod labels:\nmatch: and: - regexp: value: json.kubernetes.labels.app.kubernetes.io/instance pattern: log-generator type: string - regexp: value: json.kubernetes.labels.app.kubernetes.io/name pattern: log-generator type: string regexp parameters The regexp field can have the following parameters:\npattern (string) Defines the pattern to match against the messages. The type field determines how the pattern is interpreted (for example, string or regular expression).\nvalue (string) References a field of the message. The pattern is applied to the value of this field. If the value field is set, you cannot use the template field.\nCAUTION:\nYou need to use the json. prefix in field names. You can reference fields using the dot notation. For example, if the log contains {\"kubernetes\": {\"namespace_name\": \"default\"}}, then you can reference the namespace_name field using json.kubernetes.namespace_name.\nFor example:\nmatch: regexp: value: json.kubernetes.labels.app.kubernetes.io/name pattern: nginx template (string) Specifies a template expression that combines fields. The pattern is matched against the value of these combined fields. If the template field is set, you cannot use the value field. For details on template expressions, see the syslog-ng documentation.\ntype (string) Specifies how the pattern is interpreted. For details, see Types of regexp.\nflags (list) Specifies flags for the type field.\nregexp types By default, syslog-ng uses PCRE-style regular expressions. Since evaluating complex regular expressions can greatly increase CPU usage and are not always needed, you can following expression types:\nPerl Compatible Regular Expressions (PCRE) Literal string searches Glob patterns (without regular expression support) pcre Description: Use Perl Compatible Regular Expressions (PCRE). If the type() parameter is not specified, syslog-ng uses PCRE regular expressions by default.\npcre flags PCRE regular expressions have the following flag options:\ndisable-jit: Disable the just-in-time compilation function for PCRE regular expressions.\ndupnames: Allow using duplicate names for named subpatterns.\nglobal: Usable only in rewrite rules: match for every occurrence of the expression, not only the first one.\nignore-case: Disable case-sensitivity.\nnewline: When configured, it changes the newline definition used in PCRE regular expressions to accept either of the following:\na single carriage-return linefeed the sequence carriage-return and linefeed (\\r, \\n and \\r\\n, respectively) This newline definition is used when the circumflex and dollar patterns (^ and $) are matched against an input. By default, PCRE interprets the linefeed character as indicating the end of a line. It does not affect the \\r, \\n or \\R characters used in patterns.\nstore-matches: Store the matches of the regular expression into the $0, … $255 variables. The $0 stores the entire match, $1 is the first group of the match (parentheses), and so on. Named matches (also called named subpatterns), for example (?\u003cname\u003e...), are stored as well. Matches from the last filter expression can be referenced in regular expressions.\nunicode: Use Unicode support for UTF-8 matches. UTF-8 character sequences are handled as single characters.\nutf8: An alias for the unicode flag.\nFor example:\nmatch: and: - regexp: value: json.kubernetes.labels.app.kubernetes.io/instance pattern: log-generator flag: ignore-case string Description: Match the strings literally, without regular expression support. By default, only identical strings are matched. For partial matches, use the flags: prefix or flags: substring flags. For example, if the consider the following patterns.\nmatch: and: - regexp: value: json.kubernetes.labels.app.kubernetes.io/name pattern: log-generator type: string - regexp: value: json.kubernetes.labels.app.kubernetes.io/name pattern: log-generator type: string flag: prefix - regexp: value: json.kubernetes.labels.app.kubernetes.io/name pattern: log-generator type: string flag: substring The first matches only the log-generator label. The second matches labels beginning with log-generator, for example, log-generator-1. The third one matches labels that contain the log-generator string, for example, my-log-generator. string flags Literal string searches have the following flags() options:\nglobal: Usable only in rewrite rules, match for every occurrence of the expression, not only the first one.\nignore-case: Disables case-sensitivity.\nprefix: During the matching process, patterns (also called search expressions) are matched against the input string starting from the beginning of the input string, and the input string is matched only for the maximum character length of the pattern. The initial characters of the pattern and the input string must be identical in the exact same order, and the pattern’s length is definitive for the matching process (that is, if the pattern is longer than the input string, the match will fail).\nFor example, for the input string exam:\nthe following patterns will match: ex (the pattern contains the initial characters of the input string in the exact same order) exam (the pattern is an exact match for the input string) the following patterns will not match: example (the pattern is longer than the input string) hexameter (the pattern’s initial characters do not match the input string’s characters in the exact same order, and the pattern is longer than the input string) store-matches: Stores the matches of the regular expression into the $0, … $255 variables. The $0 stores the entire match, $1 is the first group of the match (parentheses), and so on. Named matches (also called named subpatterns), for example, (?\u003cname\u003e...), are stored as well. Matches from the last filter expression can be referenced in regular expressions.\nNOTE: To convert match variables into a syslog-ng list, use the $* macro, which can be further manipulated using List manipulation, or turned into a list in type-aware destinations.\nsubstring: The given literal string will match when the pattern is found within the input. Unlike flags: prefix, the pattern does not have to be identical with the given literal string.\nglob Description: Match the strings against a pattern containing ‘*’ and ‘?’ wildcards, without regular expression and character range support. The advantage of glob patterns to regular expressions is that globs can be processed much faster.\n*: matches an arbitrary string, including an empty string ?: matches an arbitrary character NOTE:\nThe wildcards can match the / character. You cannot use the * and ? characters literally in the pattern. Glob patterns cannot have any flags.\nExamples Select all logs To select all logs, or if you only want to exclude some logs but retain others you need an empty select statement.\napiVersion: logging.banzaicloud.io/v1beta1 kind: SyslogNGFlow metadata: name: flow-all namespace: default spec: match: regexp: value: json.kubernetes.labels.app.kubernetes.io/instance pattern: \"*\" type: glob localOutputRefs: - syslog-output Select logs by label Select logs with app: nginx labels from the namespace:\napiVersion: logging.banzaicloud.io/v1beta1 kind: SyslogNGFlow metadata: name: flow-app-nginx namespace: default spec: match: regexp: value: json.kubernetes.labels.app.kubernetes.io/name pattern: nginx type: glob localOutputRefs: - syslog-output Exclude logs by label Exclude logs with app: nginx labels from the namespace.\napiVersion: logging.banzaicloud.io/v1beta1 kind: SyslogNGFlow metadata: name: flow-not-nginx namespace: default spec: match: not: regexp: value: json.kubernetes.labels.app.kubernetes.io/name pattern: nginx type: glob localOutputRefs: - syslog-output Exclude and select logs by label Exclude logs with env: dev labels but select app: nginx labels from the namespace.\napiVersion: logging.banzaicloud.io/v1beta1 kind: SyslogNGFlow metadata: name: flow-not-nginx namespace: default spec: match: and: - regexp: value: json.kubernetes.labels.app.kubernetes.io/name pattern: nginx type: glob - not: regexp: value: json.kubernetes.labels.app.kubernetes.io/env pattern: dev type: glob localOutputRefs: - syslog-output Multiple labels - AND Exclude logs that have both the app: nginx and app.kubernetes.io/instance: nginx-demo labels.\napiVersion: logging.banzaicloud.io/v1beta1 kind: SyslogNGFlow metadata: name: flow-sample namespace: default spec: localOutputRefs: - forward-output-sample match: not: and: - regexp: value: json.kubernetes.labels.app.kubernetes.io/name pattern: nginx type: glob - regexp: value: json.kubernetes.labels.app.kubernetes.io/instance pattern: nginx-demo type: glob Multiple labels - OR Exclude logs that have either the app: nginx or the app.kubernetes.io/instance: nginx-demo labels\napiVersion: logging.banzaicloud.io/v1beta1 kind: SyslogNGFlow metadata: name: flow-sample namespace: default spec: localOutputRefs: - forward-output-sample match: not: or: - regexp: value: json.kubernetes.labels.app.kubernetes.io/name pattern: nginx type: glob - regexp: value: json.kubernetes.labels.app.kubernetes.io/instance pattern: nginx-demo type: glob ","categories":"","description":"","excerpt":" Note: This page describes routing logs with syslog-ng. If you are …","ref":"/4.2/docs/configuration/log-routing-syslog-ng/","tags":"","title":"Routing your logs with syslog-ng"},{"body":"LogDNA Output Overview This plugin has been designed to output logs to LogDNA. Example Deployment: Transport Nginx Access Logs into LogDNA with Logging Operator\nConfiguration LogDNA Send your logs to LogDNA\napi_key (string, required) LogDNA Api key\nDefault: -\nhostname (string, required) Hostname\nDefault: -\napp (string, optional) Application name\nDefault: -\ntags (string, optional) Comma-Separated List of Tags, Optional\nDefault: -\nrequest_timeout (string, optional) HTTPS POST Request Timeout, Optional. Supports s and ms Suffices\nDefault: 30 s\ningester_domain (string, optional) Custom Ingester URL, Optional\nDefault: https://logs.logdna.com\ningester_endpoint (string, optional) Custom Ingester Endpoint, Optional\nDefault: /logs/ingest\nbuffer (*Buffer, optional) Buffer\nDefault: -\nslow_flush_log_threshold (string, optional) The threshold for chunk flush performance check. Parameter type is float, not time, default: 20.0 (seconds) If chunk flush takes longer time than this threshold, fluentd logs warning message and increases metric fluentd_output_status_slow_flush_count.\nDefault: -\nExample LogDNA filter configurations apiVersion: logging.banzaicloud.io/v1beta1 kind: Output metadata: name: logdna-output-sample spec: logdna: api_key: xxxxxxxxxxxxxxxxxxxxxxxxxxx hostname: logging-operator app: my-app tags: web,dev ingester_domain https://logs.logdna.com ingester_endpoint /logs/ingest Fluentd Config Result \u003cmatch **\u003e @type logdna @id test_logdna api_key xxxxxxxxxxxxxxxxxxxxxxxxxxy app my-app hostname logging-operator \u003c/match\u003e ","categories":"","description":"","excerpt":"LogDNA Output Overview This plugin has been designed to output logs to …","ref":"/4.2/docs/configuration/plugins/outputs/logdna/","tags":"","title":"LogDNA"},{"body":"LoggingSpec LoggingSpec defines the desired state of Logging\nloggingRef (string, optional) Reference to the logging system. Each of the loggingRefs can manage a fluentbit daemonset and a fluentd statefulset.\nDefault: -\nflowConfigCheckDisabled (bool, optional) Disable configuration check before applying new fluentd configuration.\nDefault: -\nskipInvalidResources (bool, optional) Whether to skip invalid Flow and ClusterFlow resources\nDefault: -\nflowConfigOverride (string, optional) Override generated config. This is a raw configuration string for troubleshooting purposes.\nDefault: -\nfluentbit (*FluentbitSpec, optional) FluentbitAgent daemonset configuration. Deprecated, will be removed with next major version Migrate to the standalone NodeAgent resource\nDefault: -\nfluentd (*FluentdSpec, optional) Fluentd statefulset configuration\nDefault: -\nsyslogNG (*SyslogNGSpec, optional) Syslog-NG statefulset configuration\nDefault: -\ndefaultFlow (*DefaultFlowSpec, optional) Default flow for unmatched logs. This Flow configuration collects all logs that didn’t matched any other Flow.\nDefault: -\nerrorOutputRef (string, optional) GlobalOutput name to flush ERROR events to\nDefault: -\nglobalFilters ([]Filter, optional) Global filters to apply on logs before any match or filter mechanism.\nDefault: -\nwatchNamespaces ([]string, optional) Limit namespaces to watch Flow and Output custom resources.\nDefault: -\nclusterDomain (*string, optional) Cluster domain name to be used when templating URLs to services .\nDefault: “cluster.local”\ncontrolNamespace (string, required) Namespace for cluster wide configuration resources like ClusterFlow and ClusterOutput. This should be a protected namespace from regular users. Resources like fluentbit and fluentd will run in this namespace as well.\nDefault: -\nallowClusterResourcesFromAllNamespaces (bool, optional) Allow configuration of cluster resources from any namespace. Mutually exclusive with ControlNamespace restriction of Cluster resources\nDefault: -\nnodeAgents ([]*InlineNodeAgent, optional) InlineNodeAgent Configuration Deprecated, will be removed with next major version\nDefault: -\nenableRecreateWorkloadOnImmutableFieldChange (bool, optional) EnableRecreateWorkloadOnImmutableFieldChange enables the operator to recreate the fluentbit daemonset and the fluentd statefulset (and possibly other resource in the future) in case there is a change in an immutable field that otherwise couldn’t be managed with a simple update.\nDefault: -\nLoggingStatus LoggingStatus defines the observed state of Logging\nconfigCheckResults (map[string]bool, optional) Default: -\nproblems ([]string, optional) Default: -\nLogging Logging is the Schema for the loggings API\n(metav1.TypeMeta, required) Default: -\nmetadata (metav1.ObjectMeta, optional) Default: -\nspec (LoggingSpec, optional) Default: -\nstatus (LoggingStatus, optional) Default: -\nLoggingList LoggingList contains a list of Logging\n(metav1.TypeMeta, required) Default: -\nmetadata (metav1.ListMeta, optional) Default: -\nitems ([]Logging, required) Default: -\nDefaultFlowSpec DefaultFlowSpec is a Flow for logs that did not match any other Flow\nfilters ([]Filter, optional) Default: -\noutputRefs ([]string, optional) Deprecated\nDefault: -\nglobalOutputRefs ([]string, optional) Default: -\nflowLabel (string, optional) Default: -\nincludeLabelInRouter (*bool, optional) Default: -\n","categories":"","description":"","excerpt":"LoggingSpec LoggingSpec defines the desired state of Logging …","ref":"/4.2/docs/configuration/crds/v1beta1/logging_types/","tags":"","title":"LoggingSpec"},{"body":"Overview The loggly() destination sends log messages to the Loggly Logging-as-a-Service provider. You can send log messages over TCP, or encrypted with TLS. For details, see the syslog-ng documentation.\nPrerequisites You need a Loggly account and your user token to use this output.\nConfiguration host (string, optional) Address of the destination host\nDefault: -\ntag (string, optional) Event tag more information\nDefault: -\ntoken (*secret.Secret, required) Your Customer Token that you received from Loggly more information\nDefault: -\n(SyslogOutput, required) syslog output configuration\nDefault: -\n","categories":"","description":"","excerpt":"Overview The loggly() destination sends log messages to the Loggly …","ref":"/4.2/docs/configuration/plugins/syslog-ng-outputs/loggly/","tags":"","title":"Loggly output"},{"body":"logscale Overview Configuration LogScaleOutput url (*secret.Secret, optional) Ingester URL is the URL of the Humio cluster you want to send data to.\nDefault: https://cloud.humio.com\ntoken (*secret.Secret, optional) An Ingest Token is a unique string that identifies a repository and allows you to send data to that repository.\nDefault: empty\nrawstring (string, optional) The raw string representing the Event. The default display for an Event in LogScale is the rawstring. If you do not provide the rawstring field, then the response defaults to a JSON representation of the attributes field.\nDefault: empty\nattributes (string, optional) A JSON object representing key-value pairs for the Event. These key-value pairs adds structure to Events, making it easier to search. Attributes can be nested JSON objects, however, we recommend limiting the amount of nesting.\nDefault: “–scope rfc5424 –exclude MESSAGE –exclude DATE –leave-initial-dot”\ntimezone (string, optional) The timezone is only required if you specify the timestamp in milliseconds. The timezone specifies the local timezone for the event. Note that you must still specify the timestamp in UTC time.\nDefault: -\nextra_headers (string, optional) This field represents additional headers that can be included in the HTTP request when sending log records to Falcon’s LogScale.\nDefault: empty\ncontent_type (string, optional) This field specifies the content type of the log records being sent to Falcon’s LogScale.\nDefault: “application/json”\ndisk_buffer (*DiskBuffer, optional) This option enables putting outgoing messages into the disk buffer of the destination to avoid message loss in case of a system failure on the destination side. For details, see the Syslog-ng DiskBuffer options.\nDefault: false\nbody (string, optional) Default: -\nbatch_lines (int, optional) Default: -\nbatch_bytes (int, optional) Default: -\nbatch_timeout (int, optional) Default: -\npersist_name (string, optional) Default: -\n","categories":"","description":"","excerpt":"logscale Overview Configuration LogScaleOutput url (*secret.Secret, …","ref":"/4.2/docs/configuration/plugins/syslog-ng-outputs/logscale/","tags":"","title":"LogScale"},{"body":"LogZ output plugin for Fluentd Overview More info at https://github.com/tarokkk/fluent-plugin-logzio\nExample output configurations spec: logz: endpoint: url: https://listener.logz.io port: 8071 token: valueFrom: secretKeyRef: name: logz-token key: token output_include_tags: true output_include_time: true buffer: type: file flush_mode: interval flush_thread_count: 4 flush_interval: 5s chunk_limit_size: 16m queue_limit_length: 4096 Configuration Logzio LogZ Send your logs to LogZ.io\nendpoint (*Endpoint, required) Define LogZ endpoint URL\nDefault: -\noutput_include_time (bool, optional) Should the appender add a timestamp to your logs on their process time (recommended).\nDefault: -\noutput_include_tags (bool, optional) Should the appender add the fluentd tag to the document, called “fluentd_tag”\nDefault: -\nhttp_idle_timeout (int, optional) Timeout in seconds that the http persistent connection will stay open without traffic.\nDefault: -\nretry_count (int, optional) How many times to resend failed bulks.\nDefault: -\nretry_sleep (int, optional) How long to sleep initially between retries, exponential step-off.\nDefault: -\nbulk_limit (int, optional) Limit to the size of the Logz.io upload bulk. Defaults to 1000000 bytes leaving about 24kB for overhead.\nDefault: -\nbulk_limit_warning_limit (int, optional) Limit to the size of the Logz.io warning message when a record exceeds bulk_limit to prevent a recursion when Fluent warnings are sent to the Logz.io output.\nDefault: -\ngzip (bool, optional) Should the plugin ship the logs in gzip compression. Default is false.\nDefault: -\nbuffer (*Buffer, optional) Buffer\nDefault: -\nslow_flush_log_threshold (string, optional) The threshold for chunk flush performance check. Parameter type is float, not time, default: 20.0 (seconds) If chunk flush takes longer time than this threshold, fluentd logs warning message and increases metric fluentd_output_status_slow_flush_count.\nDefault: -\nEndpoint Endpoint defines connection details for LogZ.io.\nurl (string, optional) LogZ URL.\nDefault: https://listener.logz.io\nport (int, optional) Port over which to connect to LogZ URL.\nDefault: 8071\ntoken (*secret.Secret, optional) LogZ API Token. Secret\nDefault: -\n","categories":"","description":"","excerpt":"LogZ output plugin for Fluentd Overview More info at …","ref":"/4.2/docs/configuration/plugins/outputs/logz/","tags":"","title":"LogZ"},{"body":"Match filters can be used to select the log records to process. These filters have the same options and syntax as the syslog-ng flow match expressions.\nfilters: - match: or: - regexp: value: json.kubernetes.labels.app.kubernetes.io/name pattern: apache type: string - regexp: value: json.kubernetes.labels.app.kubernetes.io/name pattern: nginx type: string Configuration MatchExpr and ([]MatchExpr, optional) Default: -\nnot (*MatchExpr, optional) Default: -\nregexp (*RegexpMatchExpr, optional) Regexp Directive\nDefault: -\nor ([]MatchExpr, optional) Default: -\nRegexp Directive Specify filtering rule.\npattern (string, required) Pattern expression to evaluate\nDefault: -\ntemplate (string, optional) Specify a template of the record fields to match against.\nDefault: -\nvalue (string, optional) Specify a field name of the record to match against the value of.\nDefault: -\nflags ([]string, optional) Pattern flags\nDefault: -\ntype (string, optional) Pattern type\nDefault: -\nExample Regexp filter configurations apiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: demo-flow spec: filters: - match: regexp: - value: first pattern: ^5\\d\\d$ match: {} localOutputRefs: - demo-output Syslog-NG Config Result log { source(main_input); filter { match(\"^5\\d\\d$\" value(\"first\")); }; destination(output_default_demo-output); }; ","categories":"","description":"","excerpt":"Match filters can be used to select the log records to process. These …","ref":"/4.2/docs/configuration/plugins/syslog-ng-filters/match/","tags":"","title":"Match"},{"body":"Mattermost plugin for Fluentd Overview Sends logs to Mattermost via webhooks. More info at https://github.com/levigo-systems/fluent-plugin-mattermost\nExample output configurations spec: mattermost: webhook_url: https://xxx.xx/hooks/xxxxxxxxxxxxxxx channel_id: xxxxxxxxxxxxxxx message_color: \"#FFA500\" enable_tls: false Configuration Output Config webhook_url (*secret.Secret, required) webhook_url Incoming Webhook URI (Required for Incoming Webhook mode).\nDefault: -\nchannel_id (string, optional) channel_id the id of the channel where you want to receive the information.\nDefault: -\nmessage_color (string, optional) message_color color of the message you are sending, the format is hex code.\nDefault: #A9A9A9\nmessage_title (string, optional) message_title title you want to add to the message.\nDefault: fluent_title_default\nmessage (string, optional) message The message you want to send, can be a static message, which you add at this point, or you can receive the fluent infos with the %s\nDefault: -\nenable_tls (*bool, optional) enable_tls you can set the communication channel if it uses tls.\nDefault: true\nca_path (*secret.Secret, optional) ca_path you can set the path of the certificates.\nDefault: -\n","categories":"","description":"","excerpt":"Mattermost plugin for Fluentd Overview Sends logs to Mattermost via …","ref":"/4.2/docs/configuration/plugins/outputs/mattermost/","tags":"","title":"Mattermost"},{"body":"Sending messages from a local network to an MQTT broker Overview Prerequisites Example apiVersion: logging.banzaicloud.io/v1beta1 kind: SyslogNGOutput metadata: name: mqtt namespace: default spec: mqtt: address: tcp://mosquitto:1883 template: | $(format-json --subkeys json~ --key-delimiter ~) topic: test/demo Configuration MQTT address (string, optional) Address of the destination host\nDefault: -\ntopic (string, optional) Topic defines in which topic syslog-ng stores the log message. You can also use templates here, and use, for example, the $HOST macro in the topic name hierarchy.\nDefault: -\nfallback-topic (string, optional) fallback-topic is used when syslog-ng cannot post a message to the originally defined topic (which can include invalid characters coming from templates).\nDefault: -\ntemplate (string, optional) Template where you can configure the message template sent to the MQTT broker. By default, the template is: “$ISODATE $HOST $MSGHDR$MSG”\nDefault: -\nqos (int, optional) qos stands for quality of service and can take three values in the MQTT world. Its default value is 0, where there is no guarantee that the message is ever delivered.\nDefault: -\n","categories":"","description":"","excerpt":"Sending messages from a local network to an MQTT broker Overview …","ref":"/4.2/docs/configuration/plugins/syslog-ng-outputs/mqtt/","tags":"","title":"MQTT"},{"body":"New Relic Logs plugin for Fluentd Overview newrelic output plugin send log data to New Relic Logs\nExample output configurations spec: newrelic: license_key: valueFrom: secretKeyRef: name: logging-newrelic key: licenseKey Configuration Output Config api_key (*secret.Secret, optional) New Relic API Insert key Secret\nDefault: -\nlicense_key (*secret.Secret, optional) New Relic License Key (recommended) [Secret](../secret/\" LicenseKey *secret.Secret json:\"license_key)\nDefault: -\nbase_uri (string, optional) New Relic ingestion endpoint Secret\nDefault: https://log-api.newrelic.com/log/v1\nformat (*Format, optional) Format\nDefault: -\nbuffer (*Buffer, optional) Buffer\nDefault: -\n","categories":"","description":"","excerpt":"New Relic Logs plugin for Fluentd Overview newrelic output plugin send …","ref":"/4.2/docs/configuration/plugins/outputs/newrelic/","tags":"","title":"NewRelic"},{"body":"NodeAgent NodeAgent\n(metav1.TypeMeta, required) Default: -\nmetadata (metav1.ObjectMeta, optional) Default: -\nspec (NodeAgentSpec, optional) Default: -\nstatus (NodeAgentStatus, optional) Default: -\nNodeAgentSpec NodeAgentSpec\nloggingRef (string, optional) Default: -\n(NodeAgentConfig, required) InlineNodeAgent\nDefault: -\nNodeAgentConfig profile (string, optional) Default: -\nmetadata (types.MetaBase, optional) Default: -\nnodeAgentFluentbit (*NodeAgentFluentbit, optional) Default: -\nNodeAgentStatus NodeAgentStatus\nNodeAgentList NodeAgentList\n(metav1.TypeMeta, required) Default: -\nmetadata (metav1.ListMeta, optional) Default: -\nitems ([]NodeAgent, required) Default: -\nInlineNodeAgent InlineNodeAgent @deprecated, replaced by NodeAgent\nname (string, optional) InlineNodeAgent unique name.\nDefault: -\n(NodeAgentConfig, required) Default: -\nNodeAgentFluentbit enabled (*bool, optional) Default: -\ndaemonSet (*typeoverride.DaemonSet, optional) Default: -\nserviceAccount (*typeoverride.ServiceAccount, optional) Default: -\ntls (*FluentbitTLS, optional) Default: -\ntargetHost (string, optional) Default: -\ntargetPort (int32, optional) Default: -\nflush (int32, optional) Set the flush time in seconds.nanoseconds. The engine loop uses a Flush timeout to define when is required to flush the records ingested by input plugins through the defined output plugins. (default: 1)\nDefault: 1\ngrace (int32, optional) Set the grace time in seconds as Integer value. The engine loop uses a Grace timeout to define wait time on exit (default: 5)\nDefault: 5\nlogLevel (string, optional) Set the logging verbosity level. Allowed values are: error, warn, info, debug and trace. Values are accumulative, e.g: if ‘debug’ is set, it will include error, warning, info and debug. Note that trace mode is only available if Fluent Bit was built with the WITH_TRACE option enabled. (default: info)\nDefault: info\ncoroStackSize (int32, optional) Set the coroutines stack size in bytes. The value must be greater than the page size of the running system. Don’t set too small value (say 4096), or coroutine threads can overrun the stack buffer. Do not change the default value of this parameter unless you know what you are doing. (default: 24576)\nDefault: 24576\nmetrics (*Metrics, optional) Default: -\nmetricsService (*typeoverride.Service, optional) Default: -\nsecurity (*Security, optional) Default: -\npositiondb (volume.KubernetesVolume, optional) volume.KubernetesVolume\nDefault: -\ncontainersPath (string, optional) Default: -\nvarLogsPath (string, optional) Default: -\nextraVolumeMounts ([]*VolumeMount, optional) Default: -\ninputTail (InputTail, optional) Default: -\nfilterAws (*FilterAws, optional) Default: -\nfilterKubernetes (FilterKubernetes, optional) Default: -\ndisableKubernetesFilter (*bool, optional) Default: -\nbufferStorage (BufferStorage, optional) Default: -\nbufferStorageVolume (volume.KubernetesVolume, optional) volume.KubernetesVolume\nDefault: -\ncustomConfigSecret (string, optional) Default: -\npodPriorityClassName (string, optional) Default: -\nlivenessDefaultCheck (*bool, optional) Default: true\nnetwork (*FluentbitNetwork, optional) Default: -\nforwardOptions (*ForwardOptions, optional) Default: -\nenableUpstream (*bool, optional) Default: -\n","categories":"","description":"","excerpt":"NodeAgent NodeAgent\n(metav1.TypeMeta, required) Default: -\nmetadata …","ref":"/4.2/docs/configuration/crds/v1beta1/node_agent_types/","tags":"","title":"NodeAgent"},{"body":"OpenSearch output plugin for Fluentd Overview More info at https://github.com/fluent/fluent-plugin-opensearch\nExample Deployment: Save all logs to OpenSearch\nExample output configurations spec: opensearch: host: opensearch-cluster.default.svc.cluster.local port: 9200 scheme: https ssl_verify: false ssl_version: TLSv1_2 buffer: timekey: 1m timekey_wait: 30s timekey_use_utc: true Configuration OpenSearch Send your logs to OpenSearch\nhost (string, optional) You can specify OpenSearch host by this parameter.\nDefault: localhost\nport (int, optional) You can specify OpenSearch port by this parameter.\nDefault: 9200\nuser (string, optional) User for HTTP Basic authentication. This plugin will escape required URL encoded characters within %{} placeholders. e.g. %{demo+}\nDefault: -\npassword (*secret.Secret, optional) Password for HTTP Basic authentication. Secret\nDefault: -\npath (string, optional) Path for HTTP Basic authentication.\nDefault: -\nscheme (string, optional) Connection scheme\nDefault: http\nhosts (string, optional) You can specify multiple OpenSearch hosts with separator “,”. If you specify hosts option, host and port options are ignored.\nDefault: -\ntarget_index_key (string, optional) Tell this plugin to find the index name to write to in the record under this key in preference to other mechanisms. Key can be specified as path to nested record using dot (’.’) as a separator.\nDefault: -\ntime_key_format (string, optional) The format of the time stamp field (@timestamp or what you specify with time_key). This parameter only has an effect when logstash_format is true as it only affects the name of the index we write to.\nDefault: -\ntime_precision (string, optional) Should the record not include a time_key, define the degree of sub-second time precision to preserve from the time portion of the routed event.\nDefault: -\ninclude_timestamp (bool, optional) Adds a @timestamp field to the log, following all settings logstash_format does, except without the restrictions on index_name. This allows one to log to an alias in OpenSearch and utilize the rollover API.\nDefault: false\nlogstash_format (bool, optional) Enable Logstash log format.\nDefault: false\nlogstash_prefix (string, optional) Set the Logstash prefix.\nDefault: logstash\nlogstash_prefix_separator (string, optional) Set the Logstash prefix separator.\nDefault: -\nlogstash_dateformat (string, optional) Set the Logstash date format.\nDefault: %Y.%m.%d\nutc_index (*bool, optional) By default, the records inserted into index logstash-YYMMDD with UTC (Coordinated Universal Time). This option allows to use local time if you describe utc_index to false.(default: true)\nDefault: true\nsuppress_type_name (*bool, optional) Suppress type name to avoid warnings in OpenSearch\nDefault: -\nindex_name (string, optional) The index name to write events to\nDefault: fluentd\nid_key (string, optional) Field on your data to identify the data uniquely\nDefault: -\nwrite_operation (string, optional) The write_operation can be any of: (index,create,update,upsert)\nDefault: index\nparent_key (string, optional) parent_key\nDefault: -\nrouting_key (string, optional) routing_key\nDefault: -\nrequest_timeout (string, optional) You can specify HTTP request timeout.\nDefault: 5s\nreload_connections (*bool, optional) You can tune how the OpenSearch-transport host reloading feature works.(default: true)\nDefault: true\nreload_on_failure (bool, optional) Indicates that the OpenSearch-transport will try to reload the nodes addresses if there is a failure while making the request, this can be useful to quickly remove a dead node from the list of addresses.\nDefault: false\nretry_tag (string, optional) This setting allows custom routing of messages in response to bulk request failures. The default behavior is to emit failed records using the same tag that was provided.\nDefault: -\nresurrect_after (string, optional) You can set in the OpenSearch-transport how often dead connections from the OpenSearch-transport’s pool will be resurrected.\nDefault: 60s\ntime_key (string, optional) By default, when inserting records in Logstash format, @timestamp is dynamically created with the time at log ingestion. If you’d like to use a custom time, include an @timestamp with your record.\nDefault: -\ntime_key_exclude_timestamp (bool, optional) time_key_exclude_timestamp\nDefault: false\nssl_verify (*bool, optional) Skip ssl verification (default: true)\nDefault: true\nclient_key (*secret.Secret, optional) Client certificate key\nDefault: -\nclient_cert (*secret.Secret, optional) Client certificate\nDefault: -\nclient_key_pass (*secret.Secret, optional) Client key password\nDefault: -\nca_file (*secret.Secret, optional) CA certificate\nDefault: -\nssl_version (string, optional) If you want to configure SSL/TLS version, you can specify ssl_version parameter. [SSLv23, TLSv1, TLSv1_1, TLSv1_2]\nDefault: -\nremove_keys_on_update (string, optional) If you want to configure SSL/TLS version, you can specify ssl_version parameter. [SSLv23, TLSv1, TLSv1_1, TLSv1_2] Remove keys on update will not update the configured keys in OpenSearch when a record is being updated. This setting only has any effect if the write operation is update or upsert.\nDefault: -\nremove_keys_on_update_key (string, optional) This setting allows remove_keys_on_update to be configured with a key in each record, in much the same way as target_index_key works.\nDefault: -\nflatten_hashes (bool, optional) https://github.com/fluent/fluent-plugin-opensearch#hash-flattening\nDefault: -\nflatten_hashes_separator (string, optional) Flatten separator\nDefault: -\ntemplate_name (string, optional) The name of the template to define. If a template by the name given is already present, it will be left unchanged, unless template_overwrite is set, in which case the template will be updated.\nDefault: -\ntemplate_file (*secret.Secret, optional) The path to the file containing the template to install. Secret\nDefault: -\ntemplate_overwrite (bool, optional) Always update the template, even if it already exists.\nDefault: false\ncustomize_template (string, optional) Specify the string and its value to be replaced in form of hash. Can contain multiple key value pair that would be replaced in the specified template_file. This setting only creates template and to add rollover index please check the rollover_index configuration.\nDefault: -\nindex_date_pattern (*string, optional) Specify this to override the index date pattern for creating a rollover index.\nDefault: now/d\nindex_separator (string, optional) index_separator\nDefault: -\napplication_name (*string, optional) Specify the application name for the rollover index to be created.\nDefault: default\ntemplates (string, optional) Specify index templates in form of hash. Can contain multiple templates.\nDefault: -\nmax_retry_putting_template (string, optional) You can specify times of retry putting template.\nDefault: 10\nfail_on_putting_template_retry_exceed (*bool, optional) Indicates whether to fail when max_retry_putting_template is exceeded. If you have multiple output plugin, you could use this property to do not fail on fluentd statup.(default: true)\nDefault: true\nfail_on_detecting_os_version_retry_exceed (*bool, optional) fail_on_detecting_os_version_retry_exceed (default: true)\nDefault: true\nmax_retry_get_os_version (int, optional) max_retry_get_os_version\nDefault: 15\ninclude_tag_key (bool, optional) This will add the Fluentd tag in the JSON record.\nDefault: false\ntag_key (string, optional) This will add the Fluentd tag in the JSON record.\nDefault: tag\ntime_parse_error_tag (string, optional) With logstash_format true, OpenSearch plugin parses timestamp field for generating index name. If the record has invalid timestamp value, this plugin emits an error event to @ERROR label with time_parse_error_tag configured tag.\nDefault: -\nreconnect_on_error (bool, optional) Indicates that the plugin should reset connection on any error (reconnect on next send). By default it will reconnect only on “host unreachable exceptions”. We recommended to set this true in the presence of OpenSearch shield.\nDefault: false\npipeline (string, optional) This param is to set a pipeline id of your OpenSearch to be added into the request, you can configure ingest node.\nDefault: -\nwith_transporter_log (bool, optional) This is debugging purpose option to enable to obtain transporter layer log.\nDefault: false\nemit_error_for_missing_id (bool, optional) emit_error_for_missing_id\nDefault: false\nsniffer_class_name (string, optional) TThe default Sniffer used by the OpenSearch::Transport class works well when Fluentd has a direct connection to all of the OpenSearch servers and can make effective use of the _nodes API. This doesn’t work well when Fluentd must connect through a load balancer or proxy. The parameter sniffer_class_name gives you the ability to provide your own Sniffer class to implement whatever connection reload logic you require. In addition, there is a new Fluent::Plugin::OpenSearchSimpleSniffer class which reuses the hosts given in the configuration, which is typically the hostname of the load balancer or proxy. For example, a configuration like this would cause connections to logging-os to reload every 100 operations: https://github.com/fluent/fluent-plugin-opensearch#sniffer-class-name\nDefault: -\nselector_class_name (string, optional) selector_class_name\nDefault: -\nreload_after (string, optional) When reload_connections true, this is the integer number of operations after which the plugin will reload the connections. The default value is 10000.\nDefault: -\ninclude_index_in_url (bool, optional) With this option set to true, Fluentd manifests the index name in the request URL (rather than in the request body). You can use this option to enforce an URL-based access control.\nDefault: -\nhttp_backend (string, optional) With http_backend typhoeus, opensearch plugin uses typhoeus faraday http backend. Typhoeus can handle HTTP keepalive.\nDefault: excon\nhttp_backend_excon_nonblock (*bool, optional) http_backend_excon_nonblock (default: true)\nDefault: true\nvalidate_client_version (bool, optional) When you use mismatched OpenSearch server and client libraries, fluent-plugin-opensearch cannot send data into OpenSearch.\nDefault: false\nprefer_oj_serializer (bool, optional) With default behavior, OpenSearch client uses Yajl as JSON encoder/decoder. Oj is the alternative high performance JSON encoder/decoder. When this parameter sets as true, OpenSearch client uses Oj as JSON encoder/decoder.\nDefault: false\nunrecoverable_error_types (string, optional) Default unrecoverable_error_types parameter is set up strictly. Because rejected_execution_exception is caused by exceeding OpenSearch’s thread pool capacity. Advanced users can increase its capacity, but normal users should follow default behavior.\nDefault: -\nunrecoverable_record_types (string, optional) unrecoverable_record_types\nDefault: -\nemit_error_label_event (*bool, optional) emit_error_label_event (default: true)\nDefault: true\nverify_os_version_at_startup (*bool, optional) verify_os_version_at_startup (default: true)\nDefault: true\ndefault_opensearch_version (int, optional) max_retry_get_os_version\nDefault: 1\nlog_os_400_reason (bool, optional) log_os_400_reason\nDefault: false\ncustom_headers (string, optional) This parameter adds additional headers to request. Example: {“token”:“secret”}\nDefault: {}\nsuppress_doc_wrap (bool, optional) By default, record body is wrapped by ‘doc’. This behavior can not handle update script requests. You can set this to suppress doc wrapping and allow record body to be untouched.\nDefault: false\nignore_exceptions (string, optional) A list of exception that will be ignored - when the exception occurs the chunk will be discarded and the buffer retry mechanism won’t be called. It is possible also to specify classes at higher level in the hierarchy.\nDefault: -\nexception_backup (*bool, optional) Indicates whether to backup chunk when ignore exception occurs. (default: true)\nDefault: true\nbulk_message_request_threshold (string, optional) Configure bulk_message request splitting threshold size. Default value is 20MB. (20 * 1024 * 1024) If you specify this size as negative number, bulk_message request splitting feature will be disabled.\nDefault: 20MB\ncompression_level (string, optional) compression_level\nDefault: -\ntruncate_caches_interval (string, optional) truncate_caches_interval\nDefault: -\nuse_legacy_template (*bool, optional) use_legacy_template (default: true)\nDefault: true\ncatch_transport_exception_on_retry (*bool, optional) catch_transport_exception_on_retry (default: true)\nDefault: true\ntarget_index_affinity (bool, optional) target_index_affinity\nDefault: false\nbuffer (*Buffer, optional) Default: -\nslow_flush_log_threshold (string, optional) The threshold for chunk flush performance check. Parameter type is float, not time, default: 20.0 (seconds) If chunk flush takes longer time than this threshold, fluentd logs warning message and increases metric fluentd_output_status_slow_flush_count.\nDefault: -\ndata_stream_enable (*bool, optional) Use @type opensearch_data_stream\nDefault: -\ndata_stream_name (string, optional) You can specify Opensearch data stream name by this parameter. This parameter is mandatory for opensearch_data_stream.\nDefault: -\ndata_stream_template_name (string, optional) Specify an existing index template for the data stream. If not present, a new template is created and named after the data stream.\nDefault: data_stream_name\n","categories":"","description":"","excerpt":"OpenSearch output plugin for Fluentd Overview More info at …","ref":"/4.2/docs/configuration/plugins/outputs/opensearch/","tags":"","title":"OpenSearch"},{"body":"OutputSpec OutputSpec defines the desired state of Output\nloggingRef (string, optional) Default: -\ns3 (*output.S3OutputConfig, optional) Default: -\nazurestorage (*output.AzureStorage, optional) Default: -\ngcs (*output.GCSOutput, optional) Default: -\noss (*output.OSSOutput, optional) Default: -\nelasticsearch (*output.ElasticsearchOutput, optional) Default: -\nopensearch (*output.OpenSearchOutput, optional) Default: -\nlogz (*output.LogZOutput, optional) Default: -\nloki (*output.LokiOutput, optional) Default: -\nsumologic (*output.SumologicOutput, optional) Default: -\ndatadog (*output.DatadogOutput, optional) Default: -\nforward (*output.ForwardOutput, optional) Default: -\nfile (*output.FileOutputConfig, optional) Default: -\nnullout (*output.NullOutputConfig, optional) Default: -\nkafka (*output.KafkaOutputConfig, optional) Default: -\ncloudwatch (*output.CloudWatchOutput, optional) Default: -\nkinesisStream (*output.KinesisStreamOutputConfig, optional) Default: -\nlogdna (*output.LogDNAOutput, optional) Default: -\nnewrelic (*output.NewRelicOutputConfig, optional) Default: -\nsplunkHec (*output.SplunkHecOutput, optional) Default: -\nhttp (*output.HTTPOutputConfig, optional) Default: -\nawsElasticsearch (*output.AwsElasticsearchOutputConfig, optional) Default: -\nredis (*output.RedisOutputConfig, optional) Default: -\nsyslog (*output.SyslogOutputConfig, optional) Default: -\ngelf (*output.GELFOutputConfig, optional) Default: -\nsqs (*output.SQSOutputConfig, optional) Default: -\nmattermost (*output.MattermostOutputConfig, optional) Default: -\nrelabel (*output.RelabelOutputConfig, optional) Default: -\nOutputStatus OutputStatus defines the observed state of Output\nactive (*bool, optional) Default: -\nproblems ([]string, optional) Default: -\nproblemsCount (int, optional) Default: -\nOutput Output is the Schema for the outputs API\n(metav1.TypeMeta, required) Default: -\nmetadata (metav1.ObjectMeta, optional) Default: -\nspec (OutputSpec, optional) Default: -\nstatus (OutputStatus, optional) Default: -\nOutputList OutputList contains a list of Output\n(metav1.TypeMeta, required) Default: -\nmetadata (metav1.ListMeta, optional) Default: -\nitems ([]Output, required) Default: -\n","categories":"","description":"","excerpt":"OutputSpec OutputSpec defines the desired state of Output\nloggingRef …","ref":"/4.2/docs/configuration/crds/v1beta1/output_types/","tags":"","title":"OutputSpec"},{"body":"Parser Filter Overview Parses a string field in event records and mutates its event record with the parsed result.\nConfiguration ParserConfig key_name (string, optional) Specify field name in the record to parse. If you leave empty the Container Runtime default will be used.\nDefault: -\nreserve_time (bool, optional) Keep original event time in parsed result.\nDefault: -\nreserve_data (bool, optional) Keep original key-value pair in parsed result.\nDefault: -\nremove_key_name_field (bool, optional) Remove key_name field when parsing is succeeded\nDefault: -\nreplace_invalid_sequence (bool, optional) If true, invalid string is replaced with safe characters and re-parse it.\nDefault: -\ninject_key_prefix (string, optional) Store parsed values with specified key name prefix.\nDefault: -\nhash_value_field (string, optional) Store parsed values as a hash value in a field.\nDefault: -\nemit_invalid_record_to_error (*bool, optional) Emit invalid record to @ERROR label. Invalid cases are: key not exist, format is not matched, unexpected error\nDefault: -\nparse (ParseSection, optional) Parse Section\nDefault: -\nparsers ([]ParseSection, optional) Deprecated, use parse instead\nDefault: -\nParse Section type (string, optional) Parse type: apache2, apache_error, nginx, syslog, csv, tsv, ltsv, json, multiline, none, logfmt, grok, multiline_grok\nDefault: -\nexpression (string, optional) Regexp expression to evaluate\nDefault: -\ntime_key (string, optional) Specify time field for event time. If the event doesn’t have this field, current time is used.\nDefault: -\nkeys (string, optional) Names for fields on each line. (seperated by coma)\nDefault: -\nnull_value_pattern (string, optional) Specify null value pattern.\nDefault: -\nnull_empty_string (bool, optional) If true, empty string field is replaced with nil\nDefault: -\nestimate_current_event (bool, optional) If true, use Fluent::EventTime.now(current time) as a timestamp when time_key is specified.\nDefault: -\nkeep_time_key (bool, optional) If true, keep time field in the record.\nDefault: -\ntypes (string, optional) Types casting the fields to proper types example: field1:type, field2:type\nDefault: -\ntime_format (string, optional) Process value using specified format. This is available only when time_type is string\nDefault: -\ntime_type (string, optional) Parse/format value according to this type available values: float, unixtime, string\nDefault: string\nlocal_time (bool, optional) Ff true, use local time. Otherwise, UTC is used. This is exclusive with utc.\nDefault: true\nutc (bool, optional) If true, use UTC. Otherwise, local time is used. This is exclusive with localtime\nDefault: false\ntimezone (string, optional) Use specified timezone. one can parse/format the time value in the specified timezone.\nDefault: nil\nformat (string, optional) Only available when using type: multi_format\nDefault: -\nformat_firstline (string, optional) Only available when using type: multi_format\nDefault: -\ndelimiter (string, optional) Only available when using type: ltsv\nDefault: “\\t”\ndelimiter_pattern (string, optional) Only available when using type: ltsv\nDefault: -\nlabel_delimiter (string, optional) Only available when using type: ltsv\nDefault: “:”\nmultiline ([]string, optional) The multiline parser plugin parses multiline logs.\nDefault: -\npatterns ([]SingleParseSection, optional) Only available when using type: multi_format Parse Section\nDefault: -\ngrok_pattern (string, optional) Only available when using type: grok, multiline_grok. The pattern of grok. You cannot specify multiple grok pattern with this.\nDefault: -\ncustom_pattern_path (*secret.Secret, optional) Only available when using type: grok, multiline_grok. File that includes custom grok patterns.\nDefault: -\ngrok_failure_key (string, optional) Only available when using type: grok, multiline_grok. The key has grok failure reason.\nDefault: -\ngrok_name_key (string, optional) Only available when using type: grok, multiline_grok. The key name to store grok section’s name.\nDefault: -\nmultiline_start_regexp (string, optional) Only available when using type: multiline_grok The regexp to match beginning of multiline.\nDefault: -\ngrok_patterns ([]GrokSection, optional) Only available when using type: grok, multiline_grok. Grok Section Specify grok pattern series set.\nDefault: -\nParse Section (single) type (string, optional) {#parse section-(single)-type} Parse type: apache2, apache_error, nginx, syslog, csv, tsv, ltsv, json, multiline, none, logfmt, grok, multiline_grok\nDefault: -\nexpression (string, optional) {#parse section-(single)-expression} Regexp expression to evaluate\nDefault: -\ntime_key (string, optional) {#parse section-(single)-time_key} Specify time field for event time. If the event doesn’t have this field, current time is used.\nDefault: -\nnull_value_pattern (string, optional) {#parse section-(single)-null_value_pattern} Specify null value pattern.\nDefault: -\nnull_empty_string (bool, optional) {#parse section-(single)-null_empty_string} If true, empty string field is replaced with nil\nDefault: -\nestimate_current_event (bool, optional) {#parse section-(single)-estimate_current_event} If true, use Fluent::EventTime.now(current time) as a timestamp when time_key is specified.\nDefault: -\nkeep_time_key (bool, optional) {#parse section-(single)-keep_time_key} If true, keep time field in the record.\nDefault: -\ntypes (string, optional) {#parse section-(single)-types} Types casting the fields to proper types example: field1:type, field2:type\nDefault: -\ntime_format (string, optional) {#parse section-(single)-time_format} Process value using specified format. This is available only when time_type is string\nDefault: -\ntime_type (string, optional) {#parse section-(single)-time_type} Parse/format value according to this type available values: float, unixtime, string\nDefault: string\nlocal_time (bool, optional) {#parse section-(single)-local_time} Ff true, use local time. Otherwise, UTC is used. This is exclusive with utc.\nDefault: true\nutc (bool, optional) {#parse section-(single)-utc} If true, use UTC. Otherwise, local time is used. This is exclusive with localtime\nDefault: false\ntimezone (string, optional) {#parse section-(single)-timezone} Use specified timezone. one can parse/format the time value in the specified timezone.\nDefault: nil\nformat (string, optional) {#parse section-(single)-format} Only available when using type: multi_format\nDefault: -\ngrok_pattern (string, optional) {#parse section-(single)-grok_pattern} Only available when using format: grok, multiline_grok. The pattern of grok. You cannot specify multiple grok pattern with this.\nDefault: -\ncustom_pattern_path (*secret.Secret, optional) {#parse section-(single)-custom_pattern_path} Only available when using format: grok, multiline_grok. File that includes custom grok patterns.\nDefault: -\ngrok_failure_key (string, optional) {#parse section-(single)-grok_failure_key} Only available when using format: grok, multiline_grok. The key has grok failure reason.\nDefault: -\ngrok_name_key (string, optional) {#parse section-(single)-grok_name_key} Only available when using format: grok, multiline_grok. The key name to store grok section’s name.\nDefault: -\nmultiline_start_regexp (string, optional) {#parse section-(single)-multiline_start_regexp} Only available when using format: multiline_grok The regexp to match beginning of multiline.\nDefault: -\ngrok_patterns ([]GrokSection, optional) {#parse section-(single)-grok_patterns} Only available when using format: grok, multiline_grok. Grok Section Specify grok pattern series set.\nDefault: -\nGrok Section name (string, optional) The name of grok section.\nDefault: -\npattern (string, required) The pattern of grok.\nDefault: -\nkeep_time_key (bool, optional) If true, keep time field in the record.\nDefault: -\ntime_key (string, optional) Specify time field for event time. If the event doesn’t have this field, current time is used.\nDefault: time\ntime_format (string, optional) Process value using specified format. This is available only when time_type is string.\nDefault: -\ntimezone (string, optional) Use specified timezone. one can parse/format the time value in the specified timezone.\nDefault: -\nExample Parser filter configurations apiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: demo-flow spec: filters: - parser: remove_key_name_field: true reserve_data: true parse: type: multi_format patterns: - format: nginx - format: regexp expression: /foo/ - format: none selectors: {} localOutputRefs: - demo-output Fluentd Config Result \u003cfilter **\u003e @type parser @id test_parser key_name message remove_key_name_field true reserve_data true \u003cparse\u003e @type multi_format \u003cpattern\u003e format nginx \u003c/pattern\u003e \u003cpattern\u003e expression /foo/ format regexp \u003c/pattern\u003e \u003cpattern\u003e format none \u003c/pattern\u003e \u003c/parse\u003e \u003c/filter\u003e ","categories":"","description":"","excerpt":"Parser Filter Overview Parses a string field in event records and …","ref":"/4.2/docs/configuration/plugins/filters/parser/","tags":"","title":"Parser"},{"body":"Parser filters can be used to extract key-value pairs from message data. Logging operator currently supports the following parsers:\nregexp syslog-parser Regexp parser The regexp parser can use regular expressions to parse fields from a message.\nfilters: - parser: regexp: patterns: - \".*test_field -\u003e (?\u003ctest_field\u003e.*)$\" prefix: .regexp. For details, see the syslog-ng documentation.\nSyslog parser The syslog parser can parse syslog messages. For details, see the syslog-ng documentation.\nfilters: - parser: syslog-parser: {} Configuration Parser regexp (*RegexpParser, optional) {#parser-regexp} Default: -\nsyslog-parser (*SyslogParser, optional) {#parser-syslog-parser} Default: -\nRegexp parser patterns ([]string, required) {#regexp parser-patterns} The regular expression patterns that you want to find a match. regexp-parser() supports multiple patterns, and stops the processing at the first successful match.\nDefault: -\nprefix (string, optional) {#regexp parser-prefix} Insert a prefix before the name part of the parsed name-value pairs to help further processing.\nDefault: -\ntemplate (string, optional) {#regexp parser-template} Specify a template of the record fields to match against.\nDefault: -\nflags ([]string, optional) {#regexp parser-flags} Pattern flags\nDefault: -\nSyslogParser flags ([]string, optional) Pattern flags\nDefault: -\n","categories":"","description":"","excerpt":"Parser filters can be used to extract key-value pairs from message …","ref":"/4.2/docs/configuration/plugins/syslog-ng-filters/parser/","tags":"","title":"Parser"},{"body":"Prometheus Filter Overview Prometheus Filter Plugin to count Incoming Records\nConfiguration PrometheusConfig metrics ([]MetricSection, optional) Metrics Section\nDefault: -\nlabels (Label, optional) Default: -\nMetrics Section name (string, required) Metrics name\nDefault: -\ntype (string, required) Metrics type counter, gauge, summary, histogram\nDefault: -\ndesc (string, required) Description of metric\nDefault: -\nkey (string, optional) Key name of record for instrumentation.\nDefault: -\nbuckets (string, optional) Buckets of record for instrumentation\nDefault: -\nlabels (Label, optional) Additional labels for this metric\nDefault: -\nExample Prometheus filter configurations apiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: demo-flow spec: filters: - tag_normaliser: {} - parser: remove_key_name_field: true reserve_data: true parse: type: nginx - prometheus: metrics: - name: total_counter desc: The total number of foo in message. type: counter labels: foo: bar labels: host: ${hostname} tag: ${tag} namespace: $.kubernetes.namespace selectors: {} localOutputRefs: - demo-output Fluentd Config Result \u003cfilter **\u003e @type prometheus @id logging-demo-flow_2_prometheus \u003cmetric\u003e desc The total number of foo in message. name total_counter type counter \u003clabels\u003e foo bar \u003c/labels\u003e \u003c/metric\u003e \u003clabels\u003e host ${hostname} namespace $.kubernetes.namespace tag ${tag} \u003c/labels\u003e \u003c/filter\u003e ","categories":"","description":"","excerpt":"Prometheus Filter Overview Prometheus Filter Plugin to count Incoming …","ref":"/4.2/docs/configuration/plugins/filters/prometheus/","tags":"","title":"Prometheus"},{"body":"Try out Logging Operator with these quick start guides!\n","categories":"","description":"","excerpt":"Try out Logging Operator with these quick start guides!\n","ref":"/4.2/docs/quickstarts/","tags":"","title":"Quick start guides"},{"body":"Record Modifier Overview Modify each event record.\nConfiguration RecordModifier prepare_value (string, optional) Prepare values for filtering in configure phase. Prepared values can be used in . You can write any ruby code.\nDefault: -\nchar_encoding (string, optional) Fluentd including some plugins treats logs as a BINARY by default to forward. To overide that, use a target encoding or a from:to encoding here.\nDefault: -\nremove_keys (string, optional) A comma-delimited list of keys to delete\nDefault: -\nwhitelist_keys (string, optional) This is exclusive with remove_keys\nDefault: -\nreplaces ([]Replace, optional) Replace specific value for keys\nDefault: -\nrecords ([]Record, optional) Add records docs at: https://github.com/repeatedly/fluent-plugin-record-modifier Records are represented as maps: key: value\nDefault: -\nExample Record Modifier filter configurations apiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: demo-flow spec: filters: - record_modifier: records: - foo: \"bar\" selectors: {} localOutputRefs: - demo-output Fluentd Config Result \u003cfilter **\u003e @type record_modifier @id test_record_modifier \u003crecord\u003e foo bar \u003c/record\u003e \u003c/filter\u003e Replace Directive Specify replace rule. This directive contains three parameters.\nkey (string, required) {#replace directive-key} Key to search for\nDefault: -\nexpression (string, required) {#replace directive-expression} Regular expression\nDefault: -\nreplace (string, required) {#replace directive-replace} Value to replace with\nDefault: -\n","categories":"","description":"","excerpt":"Record Modifier Overview Modify each event record.\nConfiguration …","ref":"/4.2/docs/configuration/plugins/filters/record_modifier/","tags":"","title":"Record Modifier"},{"body":"Record Transformer Overview Mutates/transforms incoming event streams.\nConfiguration RecordTransformer remove_keys (string, optional) A comma-delimited list of keys to delete\nDefault: -\nkeep_keys (string, optional) A comma-delimited list of keys to keep.\nDefault: -\nrenew_record (bool, optional) Create new Hash to transform incoming data\nDefault: false\nrenew_time_key (string, optional) Specify field name of the record to overwrite the time of events. Its value must be unix time.\nDefault: -\nenable_ruby (bool, optional) When set to true, the full Ruby syntax is enabled in the ${…} expression.\nDefault: false\nauto_typecast (bool, optional) Use original value type.\nDefault: true\nrecords ([]Record, optional) Add records docs at: https://docs.fluentd.org/filter/record_transformer Records are represented as maps: key: value\nDefault: -\nExample Record Transformer filter configurations apiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: demo-flow spec: filters: - record_transformer: records: - foo: \"bar\" selectors: {} localOutputRefs: - demo-output Fluentd Config Result \u003cfilter **\u003e @type record_transformer @id test_record_transformer \u003crecord\u003e foo bar \u003c/record\u003e \u003c/filter\u003e ","categories":"","description":"","excerpt":"Record Transformer Overview Mutates/transforms incoming event streams. …","ref":"/4.2/docs/configuration/plugins/filters/record_transformer/","tags":"","title":"Record Transformer"},{"body":"Redis plugin for Fluentd Overview Sends logs to Redis endpoints. More info at https://github.com/fluent-plugins-nursery/fluent-plugin-redis\nExample output configurations spec: redis: host: redis-master.prod.svc.cluster.local buffer: tags: \"[]\" flush_interval: 10s Configuration Output Config host (string, optional) Host Redis endpoint\nDefault: localhost\nport (int, optional) Port of the Redis server\nDefault: 6379\ndb_number (int, optional) DbNumber database number is optional.\nDefault: 0\npassword (*secret.Secret, optional) Redis Server password\nDefault: -\ninsert_key_prefix (string, optional) insert_key_prefix\nDefault: “${tag}”\nstrftime_format (string, optional) strftime_format Users can set strftime format.\nDefault: “%s”\nallow_duplicate_key (bool, optional) allow_duplicate_key Allow insert key duplicate. It will work as update values.\nDefault: false\nttl (int, optional) ttl If 0 or negative value is set, ttl is not set in each key.\nDefault: -\nformat (*Format, optional) Format\nDefault: -\nbuffer (*Buffer, optional) Buffer\nDefault: -\nslow_flush_log_threshold (string, optional) The threshold for chunk flush performance check. Parameter type is float, not time, default: 20.0 (seconds) If chunk flush takes longer time than this threshold, fluentd logs warning message and increases metric fluentd_output_status_slow_flush_count.\nDefault: -\n","categories":"","description":"","excerpt":"Redis plugin for Fluentd Overview Sends logs to Redis endpoints. More …","ref":"/4.2/docs/configuration/plugins/outputs/redis/","tags":"","title":"Redis"},{"body":"Available in Logging Operator version 4.2 and later.\nThe relabel output uses the relabel output plugin of Fluentd to route events back to a specific Flow, where they can be processed again.\nThis is useful, for example, if you need to preprocess a subset of logs differently, but then do the same processing on all messages at the end. In this case, you can create multiple flows for preprocessing based on specific log matchers and then aggregate everything into a single final flow for postprocessing.\nThe value of the label parameter of the relabel output must be the same as the value of the flowLabel parameter of the Flow (or ClusterFlow) where you want to send the messages.\nFor example:\napiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterOutput metadata: name: final-relabel spec: relabel: label: '@final-flow' --- apiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: serviceFlow1 namespace: namespace1 spec: filters: [] globalOutputRefs: - final-relabel match: - select: labels: app: service1 --- apiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: serviceFlow2 namespace: namespace2 spec: filters: [] globalOutputRefs: - final-relabel match: - select: labels: app: service2 --- apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterFlow metadata: name: final-flow spec: flowLabel: '@final-flow' includeLabelInRouter: false filters: [] Using the relabel output also makes it possible to pass the messages emitted by the Concat plugin in case of a timeout. Set the timeout_label of the concat plugin to the flowLabel of the flow where you want to send the timeout messages.\nOutput Config label (string, required) {#output config-label} Specifies new label for events\nDefault: -\n","categories":"","description":"","excerpt":"Available in Logging Operator version 4.2 and later.\nThe relabel …","ref":"/4.2/docs/configuration/plugins/outputs/relabel/","tags":"","title":"Relabel"},{"body":"Rewrite filters can be used to modify record contents. Logging operator currently supports the following rewrite functions:\ngroup_unset rename set substitute unset Note: All rewrite functions support an optional condition which has the same syntax as the match filter.\nGroup unset The group_unset function removes from the record a group of fields matching a pattern.\nfilters: - rewrite: - group_unset: pattern: \"json.kubernetes.annotations.*\" Rename The rename function changes the name of an existing field name.\nfilters: - rewrite: - rename: oldName: \"json.kubernetes.labels.app\" newName: \"json.kubernetes.labels.app.kubernetes.io/name\" Set The set function sets the value of a field.\nfilters: - rewrite: - set: field: \"json.kubernetes.cluster\" value: \"prod-us\" Substitute (subst) The subst function replaces parts of a field with a replacement value based on a pattern.\nfilters: - rewrite: - subst: pattern: \"\\d\\d\\d\\d-\\d\\d\\d\\d-\\d\\d\\d\\d-\\d\\d\\d\\d\" replace: \"[redacted bank card number]\" field: \"MESSAGE\" The function also supports the type and flags fields for specifying pattern type and flags as described in the match expression regexp function.\nUnset You can unset macros or fields of the message.\nNote: Unsetting a field completely deletes any previous value of the field.\nfilters: - rewrite: - unset: field: \"json.kubernetes.cluster\" Configuration RewriteConfig group_unset (*GroupUnsetConfig, optional) Default: -\nrename (*RenameConfig, optional) Default: -\nset (*SetConfig, optional) Default: -\nsubst (*SubstituteConfig, optional) Default: -\nunset (*UnsetConfig, optional) Default: -\nRenameConfig https://www.syslog-ng.com/technical-documents/doc/syslog-ng-open-source-edition/3.37/administration-guide/78#TOPIC-1829213\noldName (string, required) Default: -\nnewName (string, required) Default: -\ncondition (*MatchExpr, optional) Default: -\nSetConfig https://www.syslog-ng.com/technical-documents/doc/syslog-ng-open-source-edition/3.37/administration-guide/77#TOPIC-1829207\nfield (string, required) Default: -\nvalue (string, required) Default: -\ncondition (*MatchExpr, optional) Default: -\nSubstituteConfig https://www.syslog-ng.com/technical-documents/doc/syslog-ng-open-source-edition/3.37/administration-guide/77#TOPIC-1829206\npattern (string, required) Default: -\nreplace (string, required) Default: -\nfield (string, required) Default: -\nflags ([]string, optional) Default: -\ntype (string, optional) Default: -\ncondition (*MatchExpr, optional) Default: -\nUnsetConfig https://www.syslog-ng.com/technical-documents/doc/syslog-ng-open-source-edition/3.37/administration-guide/78#TOPIC-1829212\nfield (string, required) Default: -\ncondition (*MatchExpr, optional) Default: -\nGroupUnsetConfig https://www.syslog-ng.com/technical-documents/doc/syslog-ng-open-source-edition/3.37/administration-guide/78#TOPIC-1829212\npattern (string, required) Default: -\ncondition (*MatchExpr, optional) Default: -\n","categories":"","description":"","excerpt":"Rewrite filters can be used to modify record contents. Logging …","ref":"/4.2/docs/configuration/plugins/syslog-ng-filters/rewrite/","tags":"","title":"Rewrite"},{"body":"Security self_hostname (string, required) Hostname\nDefault: -\nshared_key (string, required) Shared key for authentication.\nDefault: -\nuser_auth (bool, optional) If true, use user based authentication.\nDefault: -\nallow_anonymous_source (bool, optional) Allow anonymous source. sections are required if disabled.\nDefault: -\n","categories":"","description":"","excerpt":"Security self_hostname (string, required) Hostname\nDefault: - …","ref":"/4.2/docs/configuration/plugins/common/security/","tags":"","title":"Security"},{"body":"Splunk via Hec output plugin for Fluentd Overview More info at https://github.com/splunk/fluent-plugin-splunk-hec\nExample output configurations spec: splunkHec: hec_host: splunk.default.svc.cluster.local hec_port: 8088 protocol: http Configuration SplunkHecOutput SplunkHecOutput sends your logs to Splunk via Hec\ndata_type (string, optional) The type of data that will be sent to Sumo Logic, either event or metric\nDefault: event\nhec_host (string, required) You can specify SplunkHec host by this parameter.\nDefault: -\nhec_port (int, optional) The port number for the Hec token or the Hec load balancer.\nDefault: 8088\nprotocol (string, optional) This is the protocol to use for calling the Hec API. Available values are: http, https.\nDefault: https\nhec_token (*secret.Secret, required) Identifier for the Hec token. Secret\nDefault: -\nmetrics_from_event (*bool, optional) When data_type is set to “metric”, the ingest API will treat every key-value pair in the input event as a metric name-value pair. Set metrics_from_event to false to disable this behavior and use metric_name_key and metric_value_key to define metrics. (Default:true)\nDefault: -\nmetric_name_key (string, optional) Field name that contains the metric name. This parameter only works in conjunction with the metrics_from_event parameter. When this prameter is set, the metrics_from_event parameter is automatically set to false.\nDefault: true\nmetric_value_key (string, optional) Field name that contains the metric value, this parameter is required when metric_name_key is configured.\nDefault: -\ncoerce_to_utf8 (*bool, optional) Indicates whether to allow non-UTF-8 characters in user logs. If set to true, any non-UTF-8 character is replaced by the string specified in non_utf8_replacement_string. If set to false, the Ingest API errors out any non-UTF-8 characters. .\nDefault: true\nnon_utf8_replacement_string (string, optional) If coerce_to_utf8 is set to true, any non-UTF-8 character is replaced by the string you specify in this parameter. .\nDefault: ’ '\nindex (string, optional) Identifier for the Splunk index to be used for indexing events. If this parameter is not set, the indexer is chosen by HEC. Cannot set both index and index_key parameters at the same time.\nDefault: -\nindex_key (string, optional) The field name that contains the Splunk index name. Cannot set both index and index_key parameters at the same time.\nDefault: -\nhost (string, optional) The host location for events. Cannot set both host and host_key parameters at the same time. (Default:hostname)\nDefault: -\nhost_key (string, optional) Key for the host location. Cannot set both host and host_key parameters at the same time.\nDefault: -\nsource (string, optional) The source field for events. If this parameter is not set, the source will be decided by HEC. Cannot set both source and source_key parameters at the same time.\nDefault: -\nsource_key (string, optional) Field name to contain source. Cannot set both source and source_key parameters at the same time.\nDefault: -\nsourcetype (string, optional) The sourcetype field for events. When not set, the sourcetype is decided by HEC. Cannot set both source and source_key parameters at the same time.\nDefault: -\nsourcetype_key (string, optional) Field name that contains the sourcetype. Cannot set both source and source_key parameters at the same time.\nDefault: -\nkeep_keys (bool, optional) By default, all the fields used by the *_key parameters are removed from the original input events. To change this behavior, set this parameter to true. This parameter is set to false by default. When set to true, all fields defined in index_key, host_key, source_key, sourcetype_key, metric_name_key, and metric_value_key are saved in the original event.\nDefault: -\nidle_timeout (int, optional) If a connection has not been used for this number of seconds it will automatically be reset upon the next use to avoid attempting to send to a closed connection. nil means no timeout.\nDefault: -\nread_timeout (int, optional) The amount of time allowed between reading two chunks from the socket.\nDefault: -\nopen_timeout (int, optional) The amount of time to wait for a connection to be opened.\nDefault: -\nclient_cert (*secret.Secret, optional) The path to a file containing a PEM-format CA certificate for this client. Secret\nDefault: -\nclient_key (*secret.Secret, optional) The private key for this client.’ Secret\nDefault: -\nca_file (*secret.Secret, optional) The path to a file containing a PEM-format CA certificate. Secret\nDefault: -\nca_path (*secret.Secret, optional) The path to a directory containing CA certificates in PEM format. Secret\nDefault: -\nssl_ciphers (string, optional) List of SSL ciphers allowed.\nDefault: -\ninsecure_ssl (*bool, optional) Indicates if insecure SSL connection is allowed\nDefault: false\nfields (Fields, optional) In this case, parameters inside are used as indexed fields and removed from the original input events\nDefault: -\nformat (*Format, optional) Format\nDefault: -\nbuffer (*Buffer, optional) Buffer\nDefault: -\nslow_flush_log_threshold (string, optional) The threshold for chunk flush performance check. Parameter type is float, not time, default: 20.0 (seconds) If chunk flush takes longer time than this threshold, fluentd logs warning message and increases metric fluentd_output_status_slow_flush_count.\nDefault: -\n","categories":"","description":"","excerpt":"Splunk via Hec output plugin for Fluentd Overview More info at …","ref":"/4.2/docs/configuration/plugins/outputs/splunk_hec/","tags":"","title":"Splunk"},{"body":"SQS Output Overview Fluentd output plugin for SQS.\nConfiguration Output Config sqs_url (string, optional) SQS queue url e.g. https://sqs.us-west-2.amazonaws.com/123456789012/myqueue\nDefault: -\nqueue_name (string, optional) SQS queue name - required if sqs_url is not set\nDefault: -\naws_key_id (*secret.Secret, optional) AWS access key id\nDefault: -\naws_sec_key (*secret.Secret, optional) AWS secret key\nDefault: -\ncreate_queue (*bool, optional) Create SQS queue\nDefault: true\nregion (string, optional) AWS region\nDefault: ap-northeast-1\nmessage_group_id (string, optional) Message group id for FIFO queue\nDefault: -\ndelay_seconds (int, optional) Delivery delay seconds\nDefault: 0\ninclude_tag (*bool, optional) Include tag\nDefault: true\ntag_property_name (string, optional) Tags property name in json\nDefault: ‘__tag’\nbuffer (*Buffer, optional) Buffer\nDefault: -\nslow_flush_log_threshold (string, optional) The threshold for chunk flush performance check. Parameter type is float, not time, default: 20.0 (seconds) If chunk flush takes longer time than this threshold, fluentd logs warning message and increases metric fluentd_output_status_slow_flush_count.\nDefault: -\nExample SQS output configurations apiVersion: logging.banzaicloud.io/v1beta1 kind: Output metadata: name: sqs-output-sample spec: sqs: queue_name: some-aws-sqs-queue create_queue: false region: us-east-1 Fluentd Config Result \u003cmatch **\u003e @type sqs @id test_sqs queue_name some-aws-sqs-queue create_queue false region us-east-1 \u003c/match\u003e ","categories":"","description":"","excerpt":"SQS Output Overview Fluentd output plugin for SQS.\nConfiguration …","ref":"/4.2/docs/configuration/plugins/outputs/sqs/","tags":"","title":"SQS"},{"body":"Stdout Filter Overview Fluentd Filter plugin to print events to stdout\nConfiguration StdOutFilterConfig output_type (string, optional) This is the option of stdout format.\nDefault: -\nExample StdOut filter configurations apiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: demo-flow spec: filters: - stdout: output_type: json selectors: {} localOutputRefs: - demo-output Fluentd Config Result \u003cfilter **\u003e @type stdout @id test_stdout output_type json \u003c/filter\u003e ","categories":"","description":"","excerpt":"Stdout Filter Overview Fluentd Filter plugin to print events to stdout …","ref":"/4.2/docs/configuration/plugins/filters/stdout/","tags":"","title":"StdOut"},{"body":"The sumologic-http output sends log records over HTTP to Sumo Logic.\nPrerequisites You need a Sumo Logic account to use this output. For details, see the syslog-ng documentation.\nExample apiVersion: logging.banzaicloud.io/v1beta1 kind: SyslogNGOutput metadata: name: test-sumo namespace: default spec: sumologic-http: batch-lines: 1000 disk_buffer: disk_buf_size: 512000000 dir: /buffers reliable: true body: \"$(format-json --subkeys json. --exclude json.kubernetes.annotations.* json.kubernetes.annotations=literal($(format-flat-json --subkeys json.kubernetes.annotations.)) --exclude json.kubernetes.labels.* json.kubernetes.labels=literal($(format-flat-json --subkeys json.kubernetes.labels.)))\" collector: valueFrom: secretKeyRef: key: token name: sumo-collector deployment: us2 headers: - 'X-Sumo-Name: source-name' - 'X-Sumo-Category: source-category' tls: use-system-cert-store: true Configuration SumologicHTTPOutput collector (*secret.Secret, optional) The Cloud Syslog Cloud Token that you received from the Sumo Logic service while configuring your cloud syslog source.\nDefault: empty\ndeployment (string, optional) This option specifies your Sumo Logic deployment.https://help.sumologic.com/APIs/General-API-Information/Sumo-Logic-Endpoints-by-Deployment-and-Firewall-Security\nDefault: empty\nheaders ([]string, optional) Custom HTTP headers to include in the request, for example, headers(“HEADER1: header1”, “HEADER2: header2”).\nDefault: empty\ntime_reopen (int, optional) The time to wait in seconds before a dead connection is reestablished.\nDefault: 60\ntls (*TLS, optional) This option sets various options related to TLS encryption, for example, key/certificate files and trusted CA locations. TLS can be used only with tcp-based transport protocols. For details, see TLS for syslog-ng outputs and the syslog-ng documentation.\nDefault: -\ndisk_buffer (*DiskBuffer, optional) This option enables putting outgoing messages into the disk buffer of the destination to avoid message loss in case of a system failure on the destination side. For details, see the Syslog-ng DiskBuffer options.\nDefault: false\nbody (string, optional) Default: -\nurl (*secret.Secret, optional) Default: -\nbatch-lines (int, optional) Default: -\nbatch-bytes (int, optional) Default: -\nbatch-timeout (int, optional) Default: -\npersist_name (string, optional) Default: -\n","categories":"","description":"","excerpt":"The sumologic-http output sends log records over HTTP to Sumo Logic. …","ref":"/4.2/docs/configuration/plugins/syslog-ng-outputs/sumologic_http/","tags":"","title":"Sumo Logic HTTP"},{"body":"Storing messages in Sumo Logic over syslog Overview More info at https://www.syslog-ng.com/technical-documents/doc/syslog-ng-open-source-edition/3.37/administration-guide/56#TOPIC-1829122\nConfiguration SumologicSyslogOutput port (int, optional) This option sets the port number of the Sumo Logic server to connect to.\nDefault: 6514\ndeployment (string, optional) This option specifies your Sumo Logic deployment.https://help.sumologic.com/APIs/General-API-Information/Sumo-Logic-Endpoints-by-Deployment-and-Firewall-Security\nDefault: empty\ntag (string, optional) This option specifies the list of tags to add as the tags fields of Sumo Logic messages. If not specified, syslog-ng OSE automatically adds the tags already assigned to the message. If you set the tag() option, only the tags you specify will be added to the messages.\nDefault: tag\ntoken (int, optional) The Cloud Syslog Cloud Token that you received from the Sumo Logic service while configuring your cloud syslog source. https://help.sumologic.com/03Send-Data/Sources/02Sources-for-Hosted-Collectors/Cloud-Syslog-Source#configure-a-cloud%C2%A0syslog%C2%A0source\nDefault: -\ntls (*TLS, optional) This option sets various options related to TLS encryption, for example, key/certificate files and trusted CA locations. TLS can be used only with tcp-based transport protocols. For details, see TLS for syslog-ng outputs and the syslog-ng documentation.\nDefault: -\ndisk_buffer (*DiskBuffer, optional) This option enables putting outgoing messages into the disk buffer of the destination to avoid message loss in case of a system failure on the destination side. For details, see the Syslog-ng DiskBuffer options.\nDefault: false\npersist_name (string, optional) Default: -\n","categories":"","description":"","excerpt":"Storing messages in Sumo Logic over syslog Overview More info at …","ref":"/4.2/docs/configuration/plugins/syslog-ng-outputs/sumologic_syslog/","tags":"","title":"Sumo Logic Syslog"},{"body":"Sumo Logic collection solution for Kubernetes Overview More info at https://github.com/SumoLogic/sumologic-kubernetes-collection\nConfiguration SumoLogic source_category (string, optional) Source Category\nDefault: “%{namespace}/%{pod_name}”\nsource_category_replace_dash (string, optional) Source Category Replace Dash\nDefault: “/”\nsource_category_prefix (string, optional) Source Category Prefix\nDefault: kubernetes/\nsource_name (string, optional) Source Name\nDefault: “%{namespace}.%{pod}.%{container}”\nlog_format (string, optional) Log Format\nDefault: json\nsource_host (string, optional) Source Host\nDefault: \"\"\nexclude_container_regex (string, optional) Exclude Container Regex\nDefault: \"\"\nexclude_facility_regex (string, optional) Exclude Facility Regex\nDefault: \"\"\nexclude_host_regex (string, optional) Exclude Host Regex\nDefault: \"\"\nexclude_namespace_regex (string, optional) Exclude Namespace Regex\nDefault: \"\"\nexclude_pod_regex (string, optional) Exclude Pod Regex\nDefault: \"\"\nexclude_priority_regex (string, optional) Exclude Priority Regex\nDefault: \"\"\nexclude_unit_regex (string, optional) Exclude Unit Regex\nDefault: \"\"\ntracing_format (*bool, optional) Tracing Format\nDefault: false\ntracing_namespace (string, optional) Tracing Namespace\nDefault: “namespace”\ntracing_pod (string, optional) Tracing Pod\nDefault: “pod”\ntracing_pod_id (string, optional) Tracing Pod ID\nDefault: “pod_id”\ntracing_container_name (string, optional) Tracing Container Name\nDefault: “container_name”\ntracing_host (string, optional) Tracing Host\nDefault: “hostname”\ntracing_label_prefix (string, optional) Tracing Label Prefix\nDefault: “pod_label_”\ntracing_annotation_prefix (string, optional) Tracing Annotation Prefix\nDefault: “pod_annotation_”\nsource_host_key_name (string, optional) Source HostKey Name\nDefault: “_sourceHost”\nsource_category_key_name (string, optional) Source CategoryKey Name\nDefault: “_sourceCategory”\nsource_name_key_name (string, optional) Source NameKey Name\nDefault: “_sourceName”\ncollector_key_name (string, optional) CollectorKey Name\nDefault: “_collector”\ncollector_value (string, optional) Collector Value\nDefault: “undefined”\nExample Parser filter configurations apiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: demo-flow spec: filters: - sumologic: source_name: \"elso\" selectors: {} localOutputRefs: - demo-output Fluentd Config Result \u003cfilter **\u003e @type kubernetes_sumologic @id test_sumologic source_name elso \u003c/filter\u003e ","categories":"","description":"","excerpt":"Sumo Logic collection solution for Kubernetes Overview More info at …","ref":"/4.2/docs/configuration/plugins/filters/sumologic/","tags":"","title":"SumoLogic"},{"body":"SumoLogic output plugin for Fluentd Overview This plugin has been designed to output logs or metrics to SumoLogic via a HTTP collector endpoint More info at https://github.com/SumoLogic/fluentd-output-sumologic\nExample secret for HTTP input URL\nexport URL='https://endpoint1.collection.eu.sumologic.com/receiver/v1/http/.......' kubectl create secret generic sumo-output --from-literal \"endpoint=$URL\" Example ClusterOutput\napiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterOutput metadata: name: sumo-output spec: sumologic: buffer: flush_interval: 10s flush_mode: interval compress: true endpoint: valueFrom: secretKeyRef: key: endpoint name: sumo-output source_name: test1 Configuration Output Config data_type (string, optional) The type of data that will be sent to Sumo Logic, either logs or metrics\nDefault: logs\nendpoint (*secret.Secret, required) SumoLogic HTTP Collector URL\nDefault: -\nverify_ssl (bool, optional) Verify ssl certificate.\nDefault: true\nmetric_data_format (string, optional) The format of metrics you will be sending, either graphite or carbon2 or prometheus\nDefault: graphite\nlog_format (string, optional) Format to post logs into Sumo.\nDefault: json\nlog_key (string, optional) Used to specify the key when merging json or sending logs in text format\nDefault: message\nsource_category (string, optional) Set _sourceCategory metadata field within SumoLogic\nDefault: nil\nsource_name (string, required) Set _sourceName metadata field within SumoLogic - overrides source_name_key (default is nil)\nDefault: -\nsource_name_key (string, optional) Set as source::path_key’s value so that the source_name can be extracted from Fluentd’s buffer\nDefault: source_name\nsource_host (string, optional) Set _sourceHost metadata field within SumoLogic\nDefault: nil\nopen_timeout (int, optional) Set timeout seconds to wait until connection is opened.\nDefault: 60\nadd_timestamp (bool, optional) Add timestamp (or timestamp_key) field to logs before sending to sumologic\nDefault: true\ntimestamp_key (string, optional) Field name when add_timestamp is on\nDefault: timestamp\nproxy_uri (string, optional) Add the uri of the proxy environment if present.\nDefault: -\ndisable_cookies (bool, optional) Option to disable cookies on the HTTP Client.\nDefault: false\ndelimiter (string, optional) Delimiter\nDefault: .\ncustom_fields ([]string, optional) Comma-separated key=value list of fields to apply to every log. more information\nDefault: -\nsumo_client (string, optional) Name of sumo client which is send as X-Sumo-Client header\nDefault: fluentd-output\ncompress (*bool, optional) Compress payload\nDefault: false\ncompress_encoding (string, optional) Encoding method of compression (either gzip or deflate)\nDefault: gzip\ncustom_dimensions (string, optional) Dimensions string (eg “cluster=payment, service=credit_card”) which is going to be added to every metric record.\nDefault: -\nbuffer (*Buffer, optional) Buffer\nDefault: -\nslow_flush_log_threshold (string, optional) The threshold for chunk flush performance check. Parameter type is float, not time, default: 20.0 (seconds) If chunk flush takes longer time than this threshold, fluentd logs warning message and increases metric fluentd_output_status_slow_flush_count.\nDefault: -\n","categories":"","description":"","excerpt":"SumoLogic output plugin for Fluentd Overview This plugin has been …","ref":"/4.2/docs/configuration/plugins/outputs/sumologic/","tags":"","title":"SumoLogic"},{"body":"Syslog Output Overview Fluentd output plugin for remote syslog with RFC5424 headers logs.\nConfiguration SyslogOutputConfig host (string, required) Destination host address\nDefault: -\nport (int, optional) Destination host port\nDefault: “514”\ntransport (string, optional) Transport Protocol\nDefault: “tls”\ninsecure (*bool, optional) skip ssl validation\nDefault: false\nverify_fqdn (*bool, optional) verify_fqdn\nDefault: nil\nenable_system_cert_store (*bool, optional) cert_store to set ca_certificate for ssl context\nDefault: -\ntrusted_ca_path (*secret.Secret, optional) file path to ca to trust\nDefault: -\nclient_cert_path (*secret.Secret, optional) file path for private_key_path\nDefault: -\nprivate_key_path (*secret.Secret, optional) file path for private_key_path\nDefault: -\nprivate_key_passphrase (*secret.Secret, optional) PrivateKeyPassphrase for private key\nDefault: “nil”\nallow_self_signed_cert (*bool, optional) allow_self_signed_cert for mutual tls\nDefault: false\nfqdn (string, optional) Fqdn\nDefault: “nil”\nversion (string, optional) TLS Version\nDefault: “TLSv1_2”\nformat (*FormatRfc5424, optional) Format\nDefault: -\nbuffer (*Buffer, optional) Buffer\nDefault: -\nslow_flush_log_threshold (string, optional) The threshold for chunk flush performance check. Parameter type is float, not time, default: 20.0 (seconds) If chunk flush takes longer time than this threshold, fluentd logs warning message and increases metric fluentd_output_status_slow_flush_count.\nDefault: -\nExample File output configurations apiVersion: logging.banzaicloud.io/v1beta1 kind: Output metadata: name: demo-output spec: syslog: host: SYSLOG-HOST port: 123 format: app_name_field: example.custom_field_1 proc_id_field: example.custom_field_2 buffer: timekey: 1m timekey_wait: 10s timekey_use_utc: true Fluentd Config Result \u003cmatch **\u003e @type syslog_rfc5424 @id test_syslog host SYSLOG-HOST port 123 \u003cformat\u003e @type syslog_rfc5424 app_name_field example.custom_field_1 proc_id_field example.custom_field_2 \u003c/format\u003e \u003cbuffer tag,time\u003e @type file path /buffers/test_file.*.buffer retry_forever true timekey 1m timekey_use_utc true timekey_wait 30s \u003c/buffer\u003e \u003c/match\u003e ","categories":"","description":"","excerpt":"Syslog Output Overview Fluentd output plugin for remote syslog with …","ref":"/4.2/docs/configuration/plugins/outputs/syslog/","tags":"","title":"Syslog"},{"body":"The syslog output sends log records over a socket using the Syslog protocol (RFC 5424).\nspec: syslog: host: 10.12.34.56 transport: tls tls: ca_file: mountFrom: secretKeyRef: name: tls-secret key: ca.crt cert_file: mountFrom: secretKeyRef: name: tls-secret key: tls.crt key_file: mountFrom: secretKeyRef: name: tls-secret key: tls.key The following example also configures disk-based buffering for the output. For details, see the Syslog-ng DiskBuffer options.\napiVersion: logging.banzaicloud.io/v1beta1 kind: SyslogNGOutput metadata: name: test namespace: default spec: syslog: host: 10.20.9.89 port: 601 disk_buffer: disk_buf_size: 512000000 dir: /buffer reliable: true template: \"$(format-json --subkeys json. --exclude json.kubernetes.labels.* json.kubernetes.labels=literal($(format-flat-json --subkeys json.kubernetes.labels.)))\\n\" tls: ca_file: mountFrom: secretKeyRef: key: ca.crt name: syslog-tls-cert cert_file: mountFrom: secretKeyRef: key: tls.crt name: syslog-tls-cert key_file: mountFrom: secretKeyRef: key: tls.key name: syslog-tls-cert transport: tls For details on the available options of the output, see the syslog-ng documentation.\nhost (string, required) Address of the destination host\nDefault: -\nport (int, optional) The port number to connect to. more information\nDefault: -\ntransport (string, optional) Specifies the protocol used to send messages to the destination server. more information more information\nDefault: -\nclose_on_input (*bool, optional) By default, syslog-ng OSE closes destination sockets if it receives any input from the socket (for example, a reply). If this option is set to no, syslog-ng OSE just ignores the input, but does not close the socket. more information\nDefault: -\nflags ([]string, optional) Flags influence the behavior of the destination driver. more information\nDefault: -\nflush_lines (int, optional) Specifies how many lines are flushed to a destination at a time. more information\nDefault: -\nso_keepalive (*bool, optional) Enables keep-alive messages, keeping the socket open. more information\nDefault: -\nsuppress (int, optional) Specifies the number of seconds syslog-ng waits for identical messages. more information\nDefault: -\ntemplate (string, optional) Specifies a template defining the logformat to be used in the destination. more information\nDefault: 0\ntemplate_escape (*bool, optional) Turns on escaping for the ‘, “, and backspace characters in templated output files. more information\nDefault: -\ntls (*TLS, optional) Sets various options related to TLS encryption, for example, key/certificate files and trusted CA locations. more information\nDefault: -\nts_format (string, optional) Override the global timestamp format (set in the global ts-format() parameter) for the specific destination. more information\nDefault: -\ndisk_buffer (*DiskBuffer, optional) Enables putting outgoing messages into the disk buffer of the destination to avoid message loss in case of a system failure on the destination side. For details, see the Syslog-ng DiskBuffer options.\nDefault: -\npersist_name (string, optional) Unique name for the syslog-ng driver more information\nDefault: -\n","categories":"","description":"","excerpt":"The syslog output sends log records over a socket using the Syslog …","ref":"/4.2/docs/configuration/plugins/syslog-ng-outputs/syslog/","tags":"","title":"Syslog (RFC5424) output"},{"body":"The following sections help you troubleshoot the syslog-ng statefulset component of the Logging operator.\nCheck syslog-ng pod status (statefulset) Verify that the syslog-ng statefulset is available using the following command: kubectl get statefulsets\nExpected output:\nNAME READY AGE logging-demo-syslogng 1/1 1m ConfigCheck The Logging operator has a builtin mechanism that validates the generated syslog-ng configuration before applying it to syslog-ng. You should be able to see the configcheck pod and its log output. The result of the check is written into the status field of the corresponding Logging resource.\nIn case the operator is stuck in an error state caused by a failed configcheck, restore the previous configuration by modifying or removing the invalid resources to the point where the configcheck pod is finally able to complete successfully.\nCheck syslog-ng configuration Use the following command to display the configuration of syslog-ng: kubectl get secret logging-demo-syslogng-app -o jsonpath=\"{.data['syslogng\\.conf']}\" | base64 --decode\nGetting Support If you encounter any problems that the documentation does not address, file an issue or talk to us on Discord or Slack.\nCommercial support is also available for the Logging operator.\nBefore asking for help, prepare the following information to make troubleshooting faster:\nLogging operator version kubernetes version helm/chart version (if you installed the Logging operator with helm) Logging operator logs fluentd configuration fluentd logs fluentbit configuration fluentbit logs Do not forget to remove any sensitive information (for example, passwords and private keys) before sharing.\n","categories":"","description":"","excerpt":"The following sections help you troubleshoot the syslog-ng statefulset …","ref":"/4.2/docs/operation/troubleshooting/syslog-ng/","tags":"","title":"Troubleshooting syslog-ng"},{"body":"SyslogNGClusterFlow SyslogNGClusterFlow is the Schema for the syslog-ng clusterflows API\n(metav1.TypeMeta, required) Default: -\nmetadata (metav1.ObjectMeta, optional) Default: -\nspec (SyslogNGClusterFlowSpec, optional) Default: -\nstatus (SyslogNGFlowStatus, optional) Default: -\nSyslogNGClusterFlowSpec SyslogNGClusterFlowSpec is the Kubernetes spec for Flows\nmatch (*SyslogNGMatch, optional) Default: -\nfilters ([]SyslogNGFilter, optional) Default: -\nloggingRef (string, optional) Default: -\nglobalOutputRefs ([]string, optional) Default: -\nSyslogNGClusterFlowList SyslogNGClusterFlowList contains a list of SyslogNGClusterFlow\n(metav1.TypeMeta, required) Default: -\nmetadata (metav1.ListMeta, optional) Default: -\nitems ([]SyslogNGClusterFlow, required) Default: -\n","categories":"","description":"","excerpt":"SyslogNGClusterFlow SyslogNGClusterFlow is the Schema for the …","ref":"/4.2/docs/configuration/crds/v1beta1/syslogng_clusterflow_types/","tags":"","title":"SyslogNGClusterFlow"},{"body":"SyslogNGClusterOutput SyslogNGClusterOutput is the Schema for the syslog-ng clusteroutputs API\n(metav1.TypeMeta, required) Default: -\nmetadata (metav1.ObjectMeta, optional) Default: -\nspec (SyslogNGClusterOutputSpec, required) Default: -\nstatus (SyslogNGOutputStatus, optional) Default: -\nSyslogNGClusterOutputSpec SyslogNGClusterOutputSpec contains Kubernetes spec for SyslogNGClusterOutput\n(SyslogNGOutputSpec, required) Default: -\nenabledNamespaces ([]string, optional) Default: -\nSyslogNGClusterOutputList SyslogNGClusterOutputList contains a list of SyslogNGClusterOutput\n(metav1.TypeMeta, required) Default: -\nmetadata (metav1.ListMeta, optional) Default: -\nitems ([]SyslogNGClusterOutput, required) Default: -\n","categories":"","description":"","excerpt":"SyslogNGClusterOutput SyslogNGClusterOutput is the Schema for the …","ref":"/4.2/docs/configuration/crds/v1beta1/syslogng_clusteroutput_types/","tags":"","title":"SyslogNGClusterOutput"},{"body":"SyslogNGFlowSpec SyslogNGFlowSpec is the Kubernetes spec for SyslogNGFlows\nmatch (*SyslogNGMatch, optional) Default: -\nfilters ([]SyslogNGFilter, optional) Default: -\nloggingRef (string, optional) Default: -\nglobalOutputRefs ([]string, optional) Default: -\nlocalOutputRefs ([]string, optional) Default: -\nSyslogNGFilter Filter definition for SyslogNGFlowSpec\nid (string, optional) Default: -\nmatch (*filter.MatchConfig, optional) Default: -\nrewrite ([]filter.RewriteConfig, optional) Default: -\nparser (*filter.ParserConfig, optional) Default: -\nSyslogNGFlow Flow Kubernetes object\n(metav1.TypeMeta, required) Default: -\nmetadata (metav1.ObjectMeta, optional) Default: -\nspec (SyslogNGFlowSpec, optional) Default: -\nstatus (SyslogNGFlowStatus, optional) Default: -\nSyslogNGFlowList FlowList contains a list of Flow\n(metav1.TypeMeta, required) Default: -\nmetadata (metav1.ListMeta, optional) Default: -\nitems ([]SyslogNGFlow, required) Default: -\n","categories":"","description":"","excerpt":"SyslogNGFlowSpec SyslogNGFlowSpec is the Kubernetes spec for …","ref":"/4.2/docs/configuration/crds/v1beta1/syslogng_flow_types/","tags":"","title":"SyslogNGFlowSpec"},{"body":"SyslogNGOutputSpec SyslogNGOutputSpec defines the desired state of SyslogNGOutput\nloggingRef (string, optional) Default: -\nloggly (*output.Loggly, optional) Default: -\nsyslog (*output.SyslogOutput, optional) Default: -\nfile (*output.FileOutput, optional) Default: -\nmqtt (*output.MQTT, optional) Default: -\nsumologic-http (*output.SumologicHTTPOutput, optional) Default: -\nsumologic-syslog (*output.SumologicSyslogOutput, optional) Default: -\nhttp (*output.HTTPOutput, optional) Default: -\nlogscale (*output.LogScaleOutput, optional) Default: -\nSyslogNGOutput SyslogNGOutput is the Schema for the syslog-ng outputs API\n(metav1.TypeMeta, required) Default: -\nmetadata (metav1.ObjectMeta, optional) Default: -\nspec (SyslogNGOutputSpec, optional) Default: -\nstatus (SyslogNGOutputStatus, optional) Default: -\nSyslogNGOutputList SyslogNGOutputList contains a list of SyslogNGOutput\n(metav1.TypeMeta, required) Default: -\nmetadata (metav1.ListMeta, optional) Default: -\nitems ([]SyslogNGOutput, required) Default: -\n","categories":"","description":"","excerpt":"SyslogNGOutputSpec SyslogNGOutputSpec defines the desired state of …","ref":"/4.2/docs/configuration/crds/v1beta1/syslogng_output_types/","tags":"","title":"SyslogNGOutputSpec"},{"body":"SyslogNGSpec SyslogNGSpec defines the desired state of SyslogNG\ntls (SyslogNGTLS, optional) Default: -\nreadinessDefaultCheck (ReadinessDefaultCheck, optional) Default: -\nskipRBACCreate (bool, optional) Default: -\nstatefulSet (*typeoverride.StatefulSet, optional) Default: -\nservice (*typeoverride.Service, optional) Default: -\nserviceAccount (*typeoverride.ServiceAccount, optional) Default: -\nconfigCheckPod (*typeoverride.PodSpec, optional) Default: -\nmetrics (*Metrics, optional) Default: -\nmetricsService (*typeoverride.Service, optional) Default: -\nbufferVolumeMetrics (*BufferMetrics, optional) Default: -\nbufferVolumeMetricsService (*typeoverride.Service, optional) Default: -\nglobalOptions (*GlobalOptions, optional) Default: -\njsonKeyPrefix (string, optional) Default: -\njsonKeyDelim (string, optional) Default: -\nmaxConnections (int, optional) Default: -\nlogIWSize (int, optional) Default: -\nSyslogNGTLS SyslogNGTLS defines the TLS configs\nenabled (bool, required) Default: -\nsecretName (string, optional) Default: -\nsharedKey (string, optional) Default: -\nGlobalOptions stats_level (*int, optional) Deprecated. Use stats/level from 4.1+\nDefault: -\nstats_freq (*int, optional) Deprecated. Use stats/level from 4.1+\nDefault: -\nstats (*Stats, optional) TODO switch to this by default\nDefault: -\nStats level (*int, optional) Default: -\nfreq (*int, optional) Default: -\n","categories":"","description":"","excerpt":"SyslogNGSpec SyslogNGSpec defines the desired state of SyslogNG\ntls …","ref":"/4.2/docs/configuration/crds/v1beta1/syslogng_types/","tags":"","title":"SyslogNGSpec"},{"body":"Fluentd Plugin to re-tag based on log metadata Overview More info at https://github.com/kube-logging/fluent-plugin-tag-normaliser\nAvailable kubernetes metadata\nParameter Description Example ${pod_name} Pod name understood-butterfly-logging-demo-7dcdcfdcd7-h7p9n ${container_name} Container name inside the Pod logging-demo ${namespace_name} Namespace name default ${pod_id} Kubernetes UUID for Pod 1f50d309-45a6-11e9-b795-025000000001 ${labels} Kubernetes Pod labels. This is a nested map. You can access nested attributes via . {“app”:“logging-demo”, “pod-template-hash”:“7dcdcfdcd7” } ${host} Node hostname the Pod runs on docker-desktop ${docker_id} Docker UUID of the container 3a38148aa37aa3… Configuration Tag Normaliser parameters format (string, optional) Re-Tag log messages info at github\nDefault: ${namespace_name}.${pod_name}.${container_name}\nmatch_tag (string, optional) Tag used in match directive.\nDefault: kubernetes.**\nExample Parser filter configurations apiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: demo-flow spec: filters: - tag_normaliser: format: cluster1.${namespace_name}.${pod_name}.${labels.app} selectors: {} localOutputRefs: - demo-output Fluentd Config Result \u003cmatch kubernetes.**\u003e @type tag_normaliser @id test_tag_normaliser format cluster1.${namespace_name}.${pod_name}.${labels.app} \u003c/match\u003e ","categories":"","description":"","excerpt":"Fluentd Plugin to re-tag based on log metadata Overview More info at …","ref":"/4.2/docs/configuration/plugins/filters/tagnormaliser/","tags":"","title":"Tag Normaliser"},{"body":"Throttle Filter Overview A sentry plugin to throttle logs. Logs are grouped by a configurable key. When a group exceeds a configuration rate, logs are dropped for this group.\nConfiguration Throttle group_key (string, optional) Used to group logs. Groups are rate limited independently\nDefault: kubernetes.container_name\ngroup_bucket_period_s (int, optional) This is the period of of time over which group_bucket_limit applies\nDefault: 60\ngroup_bucket_limit (int, optional) Maximum number logs allowed per groups over the period of group_bucket_period_s\nDefault: 6000\ngroup_drop_logs (bool, optional) When a group reaches its limit, logs will be dropped from further processing if this value is true\nDefault: true\ngroup_reset_rate_s (int, optional) After a group has exceeded its bucket limit, logs are dropped until the rate per second falls below or equal to group_reset_rate_s.\nDefault: group_bucket_limit/group_bucket_period_s\ngroup_warning_delay_s (int, optional) When a group reaches its limit and as long as it is not reset, a warning message with the current log rate of the group is emitted repeatedly. This is the delay between every repetition.\nDefault: 10 seconds\nExample Throttle filter configurations apiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: demo-flow spec: filters: - throttle: group_key: \"$.kubernetes.container_name\" selectors: {} localOutputRefs: - demo-output Fluentd Config Result \u003cfilter **\u003e @type throttle @id test_throttle group_key $.kubernetes.container_name \u003c/filter\u003e ","categories":"","description":"","excerpt":"Throttle Filter Overview A sentry plugin to throttle logs. Logs are …","ref":"/4.2/docs/configuration/plugins/filters/throttle/","tags":"","title":"Throttle"},{"body":"TLS config for syslog-ng outputs Overview More info at https://www.syslog-ng.com/technical-documents/doc/syslog-ng-open-source-edition/3.37/administration-guide/32#kanchor2338\nConfiguration TLS ca_dir (*secret.Secret, optional) The name of a directory that contains a set of trusted CA certificates in PEM format. more information\nDefault: -\nca_file (*secret.Secret, optional) The name of a file that contains a set of trusted CA certificates in PEM format. (Optional) more information\nDefault: -\nkey_file (*secret.Secret, optional) The name of a file that contains an unencrypted private key in PEM format, suitable as a TLS key. more information\nDefault: -\ncert_file (*secret.Secret, optional) Name of a file, that contains an X.509 certificate (or a certificate chain) in PEM format, suitable as a TLS certificate, matching the private key set in the key-file() option. more information\nDefault: -\npeer_verify (string, optional) Verification method of the peer. more information\nDefault: -\nuse-system-cert-store (*bool, optional) Use the certificate store of the system for verifying HTTPS certificates. more information\nDefault: -\ncipher-suite (string, optional) Description: Specifies the cipher, hash, and key-exchange algorithms used for the encryption, for example, ECDHE-ECDSA-AES256-SHA384. The list of available algorithms depends on the version of OpenSSL used to compile syslog-ng OSE\nDefault: -\n","categories":"","description":"","excerpt":"TLS config for syslog-ng outputs Overview More info at …","ref":"/4.2/docs/configuration/plugins/syslog-ng-outputs/tls/","tags":"","title":"TLS config for syslog-ng outputs"},{"body":"Transport protocol (string, optional) Protocol Default: :tcp\nDefault: -\nversion (string, optional) Version Default: ‘TLSv1_2’\nDefault: -\nciphers (string, optional) Ciphers Default: “ALL:!aNULL:!eNULL:!SSLv2”\nDefault: -\ninsecure (bool, optional) Use secure connection when use tls) Default: false\nDefault: -\nca_path (string, optional) Specify path to CA certificate file\nDefault: -\ncert_path (string, optional) Specify path to Certificate file\nDefault: -\nprivate_key_path (string, optional) Specify path to private Key file\nDefault: -\nprivate_key_passphrase (string, optional) public CA private key passphrase contained path\nDefault: -\nclient_cert_auth (bool, optional) When this is set Fluentd will check all incoming HTTPS requests for a client certificate signed by the trusted CA, requests that don’t supply a valid client certificate will fail.\nDefault: -\nca_cert_path (string, optional) Specify private CA contained path\nDefault: -\nca_private_key_path (string, optional) private CA private key contained path\nDefault: -\nca_private_key_passphrase (string, optional) private CA private key passphrase contained path\nDefault: -\n","categories":"","description":"","excerpt":"Transport protocol (string, optional) Protocol Default: :tcp\nDefault: …","ref":"/4.2/docs/configuration/plugins/common/transport/","tags":"","title":"Transport"},{"body":"The Logging extensions part of the Logging operator solves the following problems:\nCollect Kubernetes events to provide insight into what is happening inside a cluster, such as decisions made by the scheduler, or why some pods were evicted from the node. Collect logs from the nodes like kubelet logs. Collect logs from files on the nodes, for example, audit logs, or the systemd journal. Collect logs from legacy application log files. Starting with Logging operator version 3.17.0, logging-extensions are open source and part of Logging operator.\nFeatures Logging-operator handles the new features the well-known way: it uses custom resources to access the features. This way a simple kubectl apply with a particular parameter set initiates a new feature. Extensions supports three different custom resource types:\nLogging-operator handles the new features the well-known way: it uses custom resources to access the features. This way a simple kubectl apply with a particular parameter set initiates a new feature. Extensions supports three different custom resource types:\nEvent-tailer listens for Kubernetes events and transmits their changes to stdout, so the Logging operator can process them.\nHost-tailer tails custom files and transmits their changes to stdout. This way the Logging operator can process them. Kubernetes host tailer allows you to tail logs like kubelet, audit logs, or the systemd journal from the nodes.\nTailer-webhook is a different approach for the same problem: parsing legacy application’s log file. Instead of running a host-tailer instance on every node, tailer-webhook attaches a sidecar container to the pod, and reads the specified file(s).\nCheck our configuration snippets for examples.\n","categories":"","description":"","excerpt":"The Logging extensions part of the Logging operator solves the …","ref":"/4.2/docs/configuration/extensions/","tags":"","title":"Kubernetes events, node logs, and logfiles"},{"body":"This document contains detailed information about the Custom Resource Definitions that the Logging operator uses.\nYou can find example yamls in our GitHub repository.\nNamespace separation A logging pipeline consist of two types of resources.\nNamespaced resources: Flow, Output, SyslogNGFlow, SyslogNGOutput Global resources: ClusterFlow, ClusterOutput, SyslogNGClusterFlow, SyslogNGClusterOutput The namespaced resources are only effective in their own namespace. Global resources are cluster wide.\nYou can create ClusterFlow, ClusterOutput, SyslogNGClusterFlow, and SyslogNGClusterOutput resources only in the controlNamespace, unless the allowClusterResourcesFromAllNamespaces option is enabled in the logging resource. This namespace MUST be a protected namespace so that only administrators can access it.\nAvailable CRDs ","categories":"","description":"","excerpt":"This document contains detailed information about the Custom Resource …","ref":"/4.2/docs/configuration/crds/","tags":"","title":"Custom Resource Definitions"},{"body":"\nThis guide describes how to collect application and container logs in Kubernetes using the Logging operator, and how to send them to Elasticsearch.\nThe following figure gives you an overview about how the system works. The Logging operator collects the logs from the application, selects which logs to forward to the output, and sends the selected log messages to the output. For more details about the Logging operator, see the Logging operator overview.\nDeploy Elasticsearch First, deploy Elasticsearch in your Kubernetes cluster. The following procedure is based on the Elastic Cloud on Kubernetes quickstart, but there are some minor configuration changes, and we install everything into the logging namespace.\nInstall the Elasticsearch operator.\nkubectl apply -f https://download.elastic.co/downloads/eck/1.3.0/all-in-one.yaml Create the logging Namespace.\nkubectl create ns logging Install the Elasticsearch cluster into the logging namespace.\ncat \u003c\u003cEOF | kubectl apply -n logging -f - apiVersion: elasticsearch.k8s.elastic.co/v1 kind: Elasticsearch metadata: name: quickstart spec: version: 7.10.0 nodeSets: - name: default count: 1 config: node.master: true node.data: true node.ingest: true node.store.allow_mmap: false EOF Install Kibana into the logging namespace.\ncat \u003c\u003cEOF | kubectl apply -n logging -f - apiVersion: kibana.k8s.elastic.co/v1 kind: Kibana metadata: name: quickstart spec: version: 7.10.0 count: 1 elasticsearchRef: name: quickstart EOF Deploy the Logging operator and a demo Application Install the Logging operator and a demo application to provide sample log messages.\nDeploy the Logging operator with Helm To install the Logging operator using Helm, complete these steps.\nNote: For the Helm-based installation you need Helm v3.2.1 or later.\nAdd the chart repository of the Logging operator using the following commands:\nhelm repo add kube-logging https://kube-logging.dev/helm-charts helm repo update Install the Logging operator into the logging namespace:\nhelm upgrade --install --wait --create-namespace --namespace logging logging-operator kube-logging/logging-operator Validate your deployment.\nConfigure the Logging operator Create the logging resource.\nkubectl -n logging apply -f - \u003c\u003c\"EOF\" apiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: default-logging-simple spec: fluentd: {} fluentbit: {} controlNamespace: logging EOF Note: You can use the ClusterOutput and ClusterFlow resources only in the controlNamespace.\nCreate an Elasticsearch output definition.\nkubectl -n logging apply -f - \u003c\u003c\"EOF\" apiVersion: logging.banzaicloud.io/v1beta1 kind: Output metadata: name: es-output spec: elasticsearch: host: quickstart-es-http.logging.svc.cluster.local port: 9200 scheme: https ssl_verify: false ssl_version: TLSv1_2 user: elastic password: valueFrom: secretKeyRef: name: quickstart-es-elastic-user key: elastic buffer: timekey: 1m timekey_wait: 30s timekey_use_utc: true EOF Note: In production environment, use a longer timekey interval to avoid generating too many objects.\nCreate a flow resource. (Mind the label selector in the match that selects a set of pods that we will install in the next step)\nkubectl -n logging apply -f - \u003c\u003c\"EOF\" apiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: es-flow spec: filters: - tag_normaliser: {} - parser: remove_key_name_field: true reserve_data: true parse: type: nginx match: - select: labels: app.kubernetes.io/name: log-generator localOutputRefs: - es-output EOF Install the demo application.\nhelm upgrade --install --wait --create-namespace --namespace logging log-generator kube-logging/log-generator Validate your deployment.\nValidate the deployment To validate that the deployment was successful, complete the following steps.\nCheck fluentd logs:\nkubectl exec -ti -n logging default-logging-simple-fluentd-0 -- tail -f /fluentd/log/out Use the following command to retrieve the password of the elastic user:\nkubectl -n logging get secret quickstart-es-elastic-user -o=jsonpath='{.data.elastic}' | base64 --decode; echo Enable port forwarding to the Kibana Dashboard Service.\nkubectl -n logging port-forward svc/quickstart-kb-http 5601 Open the Kibana dashboard in your browser at https://localhost:5601 and login as elastic using the retrieved password.\nBy default, the Logging operator sends the incoming log messages into an index called fluentd. Create an Index Pattern that includes this index (for example, fluentd*), then select Menu \u003e Kibana \u003e Discover. You should see the dashboard and some sample log messages from the demo application.\nIf you don’t get the expected result you can find help in the troubleshooting section.\n","categories":"","description":"","excerpt":"\nThis guide describes how to collect application and container logs in …","ref":"/4.2/docs/quickstarts/es-nginx/","tags":"","title":"Store NGINX access logs in Elasticsearch with Logging operator"},{"body":"You can configure the various features and parameters of the Logging operator using Custom Resource Definitions (CRDs).\nThe Logging operator manages the log collectors and log forwarders of your logging infrastructure, and the routing rules that specify where you want to send your different log messages.\nThe log collectors are endpoint agents that collect the logs of your Kubernetes nodes and send them to the log forwarders. Logging operator currently uses Fluent Bit as log collector agents.\nThe log forwarder instance receives, filters, and transforms the incoming logs, and transfers them to one or more destination outputs. The Logging operator supports Fluentd and syslog-ng as log forwarders. Which log forwarder is best for you depends on your logging requirements. For tips, see Which log forwarder to use.\nYou can filter and process the incoming log messages using the flow custom resource of the log forwarder to route them to the appropriate output. The outputs are the destinations where you want to send your log messages, for example, Elasticsearch, or an Amazon S3 bucket. You can also define cluster-wide outputs and flows, for example, to use a centralized output that namespaced users can reference but cannot modify. Note that flows and outputs are specific to the type of log forwarder you use (Fluentd or syslog-ng).\nYou can configure the Logging operator using the following Custom Resource Definitions.\nlogging - The logging resource defines the logging infrastructure (the log collectors and forwarders) for your cluster that collects and transports your log messages. It also contains configurations for Fluent Bit, Fluentd, and syslog-ng. CRDs for Fluentd: output - Defines a Fluentd Output for a logging flow, where the log messages are sent using Fluentd. This is a namespaced resource. See also clusteroutput. To configure syslog-ng outputs, see SyslogNGOutput. flow - Defines a Fluentd logging flow using filters and outputs. Basically, the flow routes the selected log messages to the specified outputs. This is a namespaced resource. See also clusterflow. To configure syslog-ng flows, see SyslogNGFlow. clusteroutput - Defines a Fluentd output that is available from all flows and clusterflows. The operator evaluates clusteroutputs in the controlNamespace only unless allowClusterResourcesFromAllNamespaces is set to true. clusterflow - Defines a Fluentd logging flow that collects logs from all namespaces by default. The operator evaluates clusterflows in the controlNamespace only unless allowClusterResourcesFromAllNamespaces is set to true. To configure syslog-ng clusterflows, see SyslogNGClusterFlow. CRDs for syslog-ng (these resources like their Fluentd counterparts, but are tailored to features available via syslog-ng): SyslogNGOutput - Defines a syslog-ng Output for a logging flow, where the log messages are sent using Fluentd. This is a namespaced resource. See also SyslogNGClusterOutput. To configure Fluentd outputs, see output. SyslogNGFlow - Defines a syslog-ng logging flow using filters and outputs. Basically, the flow routes the selected log messages to the specified outputs. This is a namespaced resource. See also SyslogNGClusterFlow. To configure Fluentd flows, see flow. SyslogNGClusterOutput - Defines a syslog-ng output that is available from all flows and clusterflows. The operator evaluates clusteroutputs in the controlNamespace only unless allowClusterResourcesFromAllNamespaces is set to true. SyslogNGClusterFlow - Defines a syslog-ng logging flow that collects logs from all namespaces by default. The operator evaluates clusterflows in the controlNamespace only unless allowClusterResourcesFromAllNamespaces is set to true. To configure Fluentd clusterflows, see clusterflow. The following sections show examples on configuring the various components to configure outputs and to filter and route your log messages to these outputs. For a list of available CRDs, see Custom Resource Definitions.\n","categories":"","description":"","excerpt":"You can configure the various features and parameters of the Logging …","ref":"/4.2/docs/configuration/","tags":"","title":"Configure log routing"},{"body":"\nThis guide describes how to collect application and container logs in Kubernetes using the Logging operator, and how to send them to Splunk.\nLogging operator collects the logs from the application, selects which logs to forward to the output, and sends the selected log messages to the output (in this case, to Splunk). For more details about the Logging operator, see the Logging operator overview.\nDeploy Splunk First, deploy Splunk Standalone in your Kubernetes cluster. The following procedure is based on the Splunk on Kubernetes quickstart.\nCreate the logging Namespace.\nkubectl create ns logging Install the Splunk operator.\nkubectl apply -n logging -f https://tiny.cc/splunk-operator-install Install the Splunk cluster.\nkubectl apply -n logging -f - \u003c\u003c\"EOF\" apiVersion: enterprise.splunk.com/v1alpha2 kind: Standalone metadata: name: single spec: config: splunkPassword: helloworld456 splunkStartArgs: --accept-license topology: standalones: 1 EOF Deploy the Logging operator and a demo Application Install the Logging operator and a demo application to provide sample log messages.\nDeploy the Logging operator with Helm To install the Logging operator using Helm, see Deploy the Logging operator with Helm.\nCreate the logging resource.\nkubectl -n logging apply -f - \u003c\u003c\"EOF\" apiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: default-logging-simple spec: fluentd: {} fluentbit: {} controlNamespace: logging EOF Note: You can use the ClusterOutput and ClusterFlow resources only in the controlNamespace.\nGet a Splunk HEC Token.\nHEC_TOKEN=$(kubectl get secret -n logging splunk-single-standalone-secrets -o jsonpath='{.data.hec_token}' | base64 --decode) Create a Splunk output secret from the token.\nkubectl create secret generic splunk-token -n logging --from-literal \"SplunkHecToken=${HEC_TOKEN}\" Define a Splunk output.\nkubectl -n logging apply -f - \u003c\u003c\"EOF\" apiVersion: logging.banzaicloud.io/v1beta1 kind: Output metadata: name: splunk-output spec: splunkHec: hec_host: splunk-single-standalone-headless insecure_ssl: true hec_port: 8088 hec_token: valueFrom: secretKeyRef: name: splunk-token key: SplunkHecToken index: main format: type: json EOF Create a flow resource.\nkubectl -n logging apply -f - \u003c\u003c\"EOF\" apiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: splunk-flow spec: filters: - tag_normaliser: {} - parser: remove_key_name_field: true reserve_data: true parse: type: nginx match: - select: labels: app.kubernetes.io/name: log-generator localOutputRefs: - splunk-output EOF Install log-generator to produce logs with the label app.kubernetes.io/name: log-generator\nhelm upgrade --install --wait --create-namespace --namespace logging log-generator kube-logging/log-generator Validate your deployment.\nValidate the deployment To validate that the deployment was successful, complete the following steps.\nUse the following command to retrieve the password of the admin user:\nkubectl -n logging get secret splunk-single-standalone-secrets -o jsonpath='{.data.password}' | base64 --decode Enable port forwarding to the Splunk Dashboard Service.\nkubectl -n logging port-forward svc/splunk-single-standalone-headless 8000 Open the Splunk dashboard in your browser: http://localhost:8000. You should see the dashboard and some sample log messages from the demo application.\nIf you don’t get the expected result you can find help in the troubleshooting section.\n","categories":"","description":"","excerpt":"\nThis guide describes how to collect application and container logs in …","ref":"/4.2/docs/examples/splunk/","tags":"","title":"Splunk operator with Logging operator"},{"body":"This guide walks you through a simple Sumo Logic setup using the Logging Operator. Sumo Logic has Prometheus and logging capabilities as well. Now we only focus on the logging part.\nConfiguration There are 3 crucial plugins needed for a proper Sumo Logic setup.\nKubernetes metadata enhancer Sumo Logic filter Sumo Logic output Let’s setup the logging first.\nGlobalFilters The first thing we need to ensure is that the EnhanceK8s filter is present in the globalFilters section of the Logging spec. This adds additional data to the log lines (like deployment and service names).\nkubectl apply -f - \u003c\u003c\"EOF\" apiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: sumologic spec: controlNamespace: logging enableRecreateWorkloadOnImmutableFieldChange: true globalFilters: - enhanceK8s: {} fluentbit: bufferStorage: storage.backlog.mem_limit: 256KB inputTail: Mem_Buf_Limit: 256KB storage.type: filesystem metrics: serviceMonitor: true serviceMonitorConfig: {} fluentd: disablePvc: true metrics: serviceMonitor: true serviceMonitorConfig: {} EOF ClusterFlow Now we can create a ClusterFlow. Add the Sumo Logic filter to the filters section of the ClusterFlow spec. It will use the Kubernetes metadata and moves them to a special field called _sumo_metadata. All those moved fields will be sent as HTTP Header to the Sumo Logic endpoint.\nNote: As we are using Fluent Bit to enrich Kubernetes metadata, we need to specify the field names where this data is stored.\nkubectl -n logging apply -f - \u003c\u003c\"EOF\" apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterFlow metadata: name: sumologic spec: filters: - sumologic: source_name: kubernetes log_format: fields tracing_namespace: namespace_name tracing_pod: pod_name match: - select: {} globalOutputRefs: - sumo EOF ClusterOutput Create a Sumo Logic output secret from the URL.\nkubectl create secret generic logging-sumo -n logging --from-literal \"sumoURL=https://endpoint1.collection.eu.sumologic.com/......\" Finally create the Sumo Logic output.\nkubectl -n logging apply -f - \u003c\u003c\"EOF\" apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterOutput metadata: name: sumo spec: sumologic: buffer: flush_interval: 10s flush_mode: interval endpoint: valueFrom: secretKeyRef: name: logging-sumo key: sumoURL source_name: kubernetes EOF ","categories":"","description":"","excerpt":"This guide walks you through a simple Sumo Logic setup using the …","ref":"/4.2/docs/examples/sumologic/","tags":"","title":"Sumo Logic with Logging operator and Fluentd"},{"body":"This guide helps you install and configure the Logging operator and syslog-ng to forward logs to your Sumo Logic account.\nPrerequisites We assume that you already have:\nA Sumo Logic account.\nA HTTP Hosted Collector configured in the Sumo Logic service.\nTo configure a Hosted Collector, complete the steps in the Configure a Hosted Collector section on the official Sumo Logic website.\nThe unique HTTP collector code you receive while configuring your Host Collector for HTTP requests.\nDeploy the Logging operator and a demo Application Install the Logging operator and a demo application to provide sample log messages.\nDeploy the Logging operator with Helm To install the Logging operator using Helm, complete these steps.\nNote: For the Helm-based installation you need Helm v3.2.1 or later.\nAdd the chart repository of the Logging operator using the following commands:\nhelm repo add kube-logging https://kube-logging.dev/helm-charts helm repo update Install the logging-operator\nhelm upgrade --install --wait --create-namespace --namespace logging logging-operator kube-logging/logging-operator Configure the Logging operator Create the logging resource with a persistent syslog-ng installation.\nkubectl apply -f - \u003c\u003c\"EOF\" apiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: demo spec: controlNamespace: logging fluentbit: {} syslogNG: statefulSet: spec: template: spec: containers: - name: syslog-ng volumeMounts: - mountPath: /buffers name: buffer volumeClaimTemplates: - metadata: name: buffer spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi EOF Note: You can use the ClusterOutput and ClusterFlow resources only in the controlNamespace.\nCreate a Sumo Logic output secret from the URL of your Sumo Logic collection.\nkubectl create secret generic sumo-collector -n logging --from-literal \"token=XYZ\" Create a SyslogNGOutput resource.\nkubectl -n logging apply -f - \u003c\u003c\"EOF\" apiVersion: logging.banzaicloud.io/v1beta1 kind: SyslogNGOutput metadata: name: sumologic-syslog-ng-output spec: sumologic-http: collector: valueFrom: secretKeyRef: key: token name: sumo-collector deployment: us2 batch-lines: 1000 disk_buffer: disk_buf_size: 512000000 dir: /buffers reliable: true body: \"$(format-json --subkeys json. --exclude json.kubernetes.annotations.* json.kubernetes.annotations=literal($(format-flat-json --subkeys json.kubernetes.annotations.)) --exclude json.kubernetes.labels.* json.kubernetes.labels=literal($(format-flat-json --subkeys json.kubernetes.labels.)))\" headers: - 'X-Sumo-Name: source-name' - 'X-Sumo-Category: source-category' tls: use-system-cert-store: true EOF Create a SyslogNGFlow resource.\nkubectl -n logging apply -f - \u003c\u003c\"EOF\" apiVersion: logging.banzaicloud.io/v1beta1 kind: SyslogNGFlow metadata: name: log-generator spec: match: and: - regexp: value: json.kubernetes.labels.app.kubernetes.io/instance pattern: log-generator type: string - regexp: value: json.kubernetes.labels.app.kubernetes.io/name pattern: log-generator type: string filters: - parser: regexp: patterns: - '^(?\u003cremote\u003e[^ ]*) (?\u003chost\u003e[^ ]*) (?\u003cuser\u003e[^ ]*) \\[(?\u003ctime\u003e[^\\]]*)\\] \"(?\u003cmethod\u003e\\S+)(?: +(?\u003cpath\u003e[^\\\"]*?)(?: +\\S*)?)?\" (?\u003ccode\u003e[^ ]*) (?\u003csize\u003e[^ ]*)(?: \"(?\u003creferer\u003e[^\\\"]*)\" \"(?\u003cagent\u003e[^\\\"]*)\"(?:\\s+(?\u003chttp_x_forwarded_for\u003e[^ ]+))?)?$' template: ${json.message} prefix: json. - rewrite: - set: field: json.cluster value: xxxxx - unset: field: json.message - set: field: json.source value: /var/log/log-generator condition: regexp: value: json.kubernetes.container_name pattern: log-generator type: string localOutputRefs: - sumologic-syslog-ng-output EOF Install log-generator to produce logs with the label app.kubernetes.io/name: log-generator\nhelm upgrade --install --wait --create-namespace --namespace logging log-generator kube-logging/log-generator If you don’t get the expected result you can find help in the troubleshooting section.\n","categories":"","description":"","excerpt":"This guide helps you install and configure the Logging operator and …","ref":"/4.2/docs/quickstarts/syslog-ng-sumologic/","tags":"","title":"Sumo Logic with Logging operator and syslog-ng"},{"body":" syslog-ng is supported only in Logging operator 4.0 or newer.\nYou can configure the deployment of the syslog-ng log forwarder via the syslogNG section of the The Logging custom resource. For the detailed list of available parameters, see SyslogNGSpec.\nThe following example sets a volume mount that syslog-ng can use for buffering messages on the disk (if Disk buffer is configured in the output).\napiVersion: logging.banzaicloud.io/v1beta1 kind: Logging name: test spec: syslogNG: statefulSet: spec: template: spec: containers: - name: syslog-ng volumeMounts: - mountPath: /buffers name: buffer volumeClaimTemplates: - metadata: name: buffer spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi CPU and memory requirements To adjust the CPU and memory limits and requests of the pods managed by Logging operator, see CPU and memory requirements.\nProbe A Probe is a diagnostic performed periodically by the kubelet on a Container. To perform a diagnostic, the kubelet calls a Handler implemented by the Container. You can configure a probe for syslog-ng in the livenessProbe section of the The Logging custom resource. For example:\napiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: default-logging-simple spec: syslogNG: livenessProbe: periodSeconds: 60 initialDelaySeconds: 600 exec: command: - \"/usr/sbin/syslog-ng-ctl\" - \"--control=/tmp/syslog-ng/syslog-ng.ctl\" - \"query\" - \"get\" - \"global.sdata_updates.processed\" controlNamespace: logging You can use the following parameters:\nName Type Default Description initialDelaySeconds int 30 Number of seconds after the container has started before liveness probes are initiated. timeoutSeconds int 0 Number of seconds after which the probe times out. periodSeconds int 10 How often (in seconds) to perform the probe. successThreshold int 0 Minimum consecutive successes for the probe to be considered successful after having failed. failureThreshold int 3 Minimum consecutive failures for the probe to be considered failed after having succeeded. exec array {} Exec specifies the action to take. More info Note: To configure readiness probes, see Readiness probe.\n","categories":"","description":"","excerpt":" syslog-ng is supported only in Logging operator 4.0 or newer.\nYou can …","ref":"/4.2/docs/logging-infrastructure/syslog-ng/","tags":"","title":"Configure syslog-ng"},{"body":"Flow examples The following examples show some simple flows. For more examples that use filters, see Filter examples in Flows.\nFlow with a single output This Flow sends every message with the app: nginx label to the output called forward-output-sample.\nFlow with multiple outputs This Flow sends every message with the app: nginx label to the gcs-output-sample and s3-output-example outputs.\nLogging examples Simple Logging definition with default values.\nLogging with TLS Simple Logging definition with TLS encryption enabled.\nOutput examples Simple file output Defines a file output with timestamped log files.\nDrop messages into dev/null output Creates a dev/null output that can be the destination of messages you want to drop explicitly.\nCAUTION:\nMessages sent to this output are irrevocably lost forever. S3 output Defines an Amazon S3 output to store your logs in a bucket.\nGCS output Defines a Google Cloud Storage output to store your logs.\n","categories":"","description":"","excerpt":"Flow examples The following examples show some simple flows. For more …","ref":"/4.2/docs/examples/","tags":"","title":"Examples"},{"body":"The following sections describe how to change the configuration of your logging infrastructure, that is, how to configure your log collectors and forwarders.\nNote: Log routing is covered in Logging infrastructure setup.\n","categories":"","description":"","excerpt":"The following sections describe how to change the configuration of …","ref":"/4.2/docs/logging-infrastructure/","tags":"","title":"Logging infrastructure setup"},{"body":"","categories":"","description":"","excerpt":"","ref":"/4.2/docs/operation/","tags":"","title":"Operation"},{"body":"\nFluent Bit is an open source and multi-platform Log Processor and Forwarder which allows you to collect data/logs from different sources, unify and send them to multiple destinations.\nLogging operator uses Fluent Bit as a log collector agent: Logging operator deploys Fluent Bit to your Kubernetes nodes where it collects and enriches the local logs and transfers them to a log forwarder instance.\nWays to configure Fluent Bit There are three ways to configure the Fluent Bit daemonset:\nUsing the spec.fluentbit section of The Logging custom resource. This method is deprecated and will be removed in the next major release. Using the standalone FluentbitAgent CRD. This method is only available in Logging operator version 4.2 and newer, and the specification of the CRD is compatible with the spec.fluentbit configuration method. Using the spec.nodeagents section of The Logging custom resource. This method is deprecated and will be removed from the Logging operator. (Note that this configuration isn’t compatible with the FluentbitAgent CRD.) Migrating from spec.fluentbit to FluentbitAgent The standalone FluentbitAgent CRD is only available in Logging operator version 4.2 and newer. Its specification and logic is identical with the spec.fluentbit configuration method. Using the FluentbitAgent CRD allows you to remove the spec.fluentbit section from the Logging CRD, which has the following benefits.\nRBAC control over the FluentbitAgent CRD, so you can have separate roles that can manage the Logging resource and the FluentbitAgent resource (that is, the Fluent Bit deployment). It reduces the size of the Logging resource, which can grow big enough to reach the annotation size limit in certain scenarios (e.g. when using kubectl apply). It allows you to use multiple different Fluent Bit configurations within the same cluster. For details, see Multiple Fluent Bit agents in the cluster. To migrate your spec.fluentbit configuration from the Logging resource to a separate FluentbitAgent CRD, complete the following steps.\nOpen your Logging resource and find the spec.fluentbit section. For example:\napiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: example-logging-resource spec: fluentd: {} fluentbit: positiondb: hostPath: path: \"\" bufferStorageVolume: hostPath: path: \"\" controlNamespace: default Create a new FluentbitAgent CRD. For the value of metadata.name, use the name of the Logging resource, for example:\napiVersion: logging.banzaicloud.io/v1beta1 kind: FluentbitAgent metadata: # Use the name of the logging resource name: example-logging-resource Copy the the spec.fluentbit section from the Logging resource into the spec section of the FluentbitAgent CRD, then fix the indentation.\nSpecify the paths for the positiondb and the bufferStorageVolume. If you used the default settings in the spec.fluentbit configuration, set empty strings as paths, like in the following example. This is needed to retain the existing buffers of the deployment, otherwise data loss may occur.\napiVersion: logging.banzaicloud.io/v1beta1 kind: FluentbitAgent metadata: # Use the name of the logging resource name: example-logging-resource spec: positiondb: hostPath: path: \"\" bufferStorageVolume: hostPath: path: \"\" Delete the spec.fluentbit section from the Logging resource, then apply the Logging and the FluentbitAgent CRDs.\nExamples The following sections show you some examples on configuring Fluent Bit. For the detailed list of available parameters, see FluentbitSpec.\nNote: These examples use the traditional method that configures the Fluent Bit deployment using spec.fluentbit section of The Logging custom resource.\nFilters Kubernetes (filterKubernetes) Fluent Bit Kubernetes Filter allows you to enrich your log files with Kubernetes metadata. For example:\napiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: default-logging-simple spec: fluentd: {} fluentbit: filterKubernetes: Kube_URL: \"https://kubernetes.default.svc:443\" Match: \"kube.*\" controlNamespace: logging For the detailed list of available parameters for this plugin, see FilterKubernetes. More info\nTail input The tail input plugin allows to monitor one or several text files. It has a similar behavior like tail -f shell command. For example:\napiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: default-logging-simple spec: fluentd: {} fluentbit: inputTail: Refresh_Interval: \"60\" Rotate_Wait: \"5\" controlNamespace: logging For the detailed list of available parameters for this plugin, see InputTail. More Info.\nBuffering Buffering in Fluent Bit places the processed data into a temporal location until is sent to Fluentd. By default, the Logging operator sets storage.path to /buffers and leaves fluent-bit defaults for the other options.\napiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: default-logging-simple spec: fluentd: {} fluentbit: bufferStorage: storage.path: /buffers controlNamespace: logging For the detailed list of available parameters for this plugin, see BufferStorage. More Info.\nHostPath volumes for buffers and positions apiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: default-logging-simple spec: fluentd: {} fluentbit: bufferStorageVolume: hostPath: path: \"\" # leave it empty to automatically generate positiondb: hostPath: path: \"\" # leave it empty to automatically generate controlNamespace: logging Custom Fluent Bit image You can deploy custom images by overriding the default images using the following parameters in the fluentd or fluentbit sections of the logging resource.\nThe following example deploys a custom fluentd image:\napiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: default-logging-simple spec: fluentd: image: repository: banzaicloud/fluentd tag: v1.10.4-alpine-1 pullPolicy: IfNotPresent fluentbit: {} controlNamespace: logging Volume Mount Defines a pod volume mount. For example:\napiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: default-logging-tls spec: fluentd: {} fluentbit: extraVolumeMounts: - source: /opt/docker destination: /opt/docker readOnly: true controlNamespace: logging For the detailed list of available parameters for this plugin, see VolumeMount.\nCustom Fluent Bit annotations apiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: default-logging-simple spec: fluentd: {} fluentbit: annotations: my-annotations/enable: true controlNamespace: logging KubernetesStorage Define Kubernetes storage.\nName Type Default Description hostPath HostPathVolumeSource - Represents a host path mapped into a pod. If path is empty, it will automatically be set to /opt/logging-operator/\u003cname of the logging CR\u003e/\u003cname of the volume\u003e emptyDir EmptyDirVolumeSource - Represents an empty directory for a pod. CPU and memory requirements To adjust the CPU and memory limits and requests of the pods managed by Logging operator, see CPU and memory requirements.\nProbe A Probe is a diagnostic performed periodically by the kubelet on a Container. To perform a diagnostic, the kubelet calls a Handler implemented by the Container. You can configure a probe for Fluent Bit in the livenessProbe section of the The Logging custom resource. For example:\napiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: default-logging-simple spec: fluentbit: livenessProbe: periodSeconds: 60 initialDelaySeconds: 600 exec: command: - \"/bin/sh\" - \"-c\" - \u003e LIVENESS_THRESHOLD_SECONDS=${LIVENESS_THRESHOLD_SECONDS:-300}; if [ ! -e /buffers ]; then exit 1; fi; touch -d \"${LIVENESS_THRESHOLD_SECONDS} seconds ago\" /tmp/marker-liveness; if [ -z \"$(find /buffers -type d -newer /tmp/marker-liveness -print -quit)\" ]; then exit 1; fi; controlNamespace: logging You can use the following parameters:\nName Type Default Description initialDelaySeconds int 10 Number of seconds after the container has started before liveness probes are initiated. timeoutSeconds int 0 Number of seconds after which the probe times out. periodSeconds int 10 How often (in seconds) to perform the probe. successThreshold int 0 Minimum consecutive successes for the probe to be considered successful after having failed. failureThreshold int 3 Minimum consecutive failures for the probe to be considered failed after having succeeded. exec array {} Exec specifies the action to take. More info httpGet array {} HTTPGet specifies the http request to perform. More info tcpSocket array {} TCPSocket specifies an action involving a TCP port. More info Note: To configure readiness probes, see Readiness probe.\n","categories":"","description":"","excerpt":"\nFluent Bit is an open source and multi-platform Log Processor and …","ref":"/4.2/docs/logging-infrastructure/fluentbit/","tags":"","title":"Fluent Bit log collector"},{"body":"\nThis guide describes how to collect application and container logs in Kubernetes using the Logging operator, and how to send them to Kafka.\nThe following figure gives you an overview about how the system works. The Logging operator collects the logs from the application, selects which logs to forward to the output, and sends the selected log messages to the output. For more details about the Logging operator, see the Logging operator overview.\nDeploy Kafka This demo uses Koperator to create an Apache Kafka cluster in Kubernetes. For details on installing it, see the Koperator installation guide.\nDeploy the Logging operator and a demo Application Install the Logging operator and a demo application to provide sample log messages.\nDeploy the Logging operator with Helm To install the Logging operator using Helm, complete these steps.\nNote: For the Helm-based installation you need Helm v3.2.1 or later.\nAdd the chart repository of the Logging operator using the following commands:\nhelm repo add kube-logging https://kube-logging.dev/helm-charts helm repo update Install the Logging operator into the logging namespace:\nhelm upgrade --install --wait --create-namespace --namespace logging logging-operator kube-logging/logging-operator Validate your deployment.\nCreate the logging resource.\nkubectl -n logging apply -f - \u003c\u003c\"EOF\" apiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: default-logging-simple spec: fluentd: {} fluentbit: {} controlNamespace: logging EOF Note: You can use the ClusterOutput and ClusterFlow resources only in the controlNamespace.\nCreate a Kafka output definition.\nkubectl -n logging apply -f - \u003c\u003c\"EOF\" apiVersion: logging.banzaicloud.io/v1beta1 kind: Output metadata: name: kafka-output spec: kafka: brokers: kafka-headless.kafka.svc.cluster.local:29092 default_topic: topic format: type: json buffer: tags: topic timekey: 1m timekey_wait: 30s timekey_use_utc: true EOF Note: In production environment, use a longer timekey interval to avoid generating too many objects.\nCreate a flow resource.\nkubectl -n logging apply -f - \u003c\u003c\"EOF\" apiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: kafka-flow spec: filters: - tag_normaliser: {} - parser: remove_key_name_field: true reserve_data: true parse: type: nginx match: - select: labels: app.kubernetes.io/name: log-generator localOutputRefs: - kafka-output EOF Install log-generator to produce logs with the label app.kubernetes.io/name: log-generator\nhelm upgrade --install --wait --create-namespace --namespace logging log-generator kube-logging/log-generator Validate your deployment.\nValidate the deployment Run the following command to consume some log messages from Kafka:\nkubectl -n kafka run kafka-consumer -it --image=banzaicloud/kafka:2.13-2.4.0 --rm=true --restart=Never -- /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka-headless:29092 --topic topic --from-beginning Expected output:\n{\"stream\":\"stdout\",\"logtag\":\"F\",\"kubernetes\":{\"pod_name\":\"logging-demo-log-generator-5f9f9cdb9f-z76wr\",\"namespace_name\":\"logging\",\"pod_id\":\"a7174256-31bf-4ace-897b-77899873d9ad\",\"labels\":{\"app.kubernetes.io/instance\":\"logging-demo\",\"app.kubernetes.io/name\":\"log-generator\",\"pod-template-hash\":\"5f9f9cdb9f\"},\"host\":\"ip-192-168-3-189.eu-west-2.compute.internal\",\"container_name\":\"log-generator\",\"docker_id\":\"7349e6bb2926b8c93cb054a60f171a3f2dd1f6751c07dd389da7f28daf4d70c5\",\"container_hash\":\"ghcr.io/banzaicloud/log-generator@sha256:814a69be8ab8a67aa6b009d83f6fa6c4776beefbe629a869ff16690fde8ac362\",\"container_image\":\"ghcr.io/banzaicloud/log-generator:0.3.3\"},\"remote\":\"79.104.42.168\",\"host\":\"-\",\"user\":\"-\",\"method\":\"PUT\",\"path\":\"/products\",\"code\":\"302\",\"size\":\"18136\",\"referer\":\"-\",\"agent\":\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/33.0.1750.166 Safari/537.36 OPR/20.0.1396.73172\",\"http_x_forwarded_for\":\"-\"} {\"stream\":\"stdout\",\"logtag\":\"F\",\"kubernetes\":{\"pod_name\":\"logging-demo-log-generator-5f9f9cdb9f-mpp98\",\"namespace_name\":\"logging\",\"pod_id\":\"e2822c26-961c-4be8-99a2-b17517494ca1\",\"labels\":{\"app.kubernetes.io/instance\":\"logging-demo\",\"app.kubernetes.io/name\":\"log-generator\",\"pod-template-hash\":\"5f9f9cdb9f\"},\"host\":\"ip-192-168-2-102.eu-west-2.compute.internal\",\"container_name\":\"log-generator\",\"docker_id\":\"26ffbec769e52e468216fe43a331f4ce5374075f9b2717d9b9ae0a7f0747b3e2\",\"container_hash\":\"ghcr.io/banzaicloud/log-generator@sha256:814a69be8ab8a67aa6b009d83f6fa6c4776beefbe629a869ff16690fde8ac362\",\"container_image\":\"ghcr.io/banzaicloud/log-generator:0.3.3\"},\"remote\":\"26.220.126.5\",\"host\":\"-\",\"user\":\"-\",\"method\":\"POST\",\"path\":\"/\",\"code\":\"200\",\"size\":\"14370\",\"referer\":\"-\",\"agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:52.0) Gecko/20100101 Firefox/52.0\",\"http_x_forwarded_for\":\"-\"} If you don’t get the expected result you can find help in the troubleshooting section.\n","categories":"","description":"","excerpt":"\nThis guide describes how to collect application and container logs in …","ref":"/4.2/docs/examples/kafka-nginx/","tags":"","title":"Transport Nginx Access Logs into Kafka with Logging operator"},{"body":"Persistent Volumes do not respect the fsGroup value on Kind so disable using a PVC for fluentd:\napiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: example-on-kind spec: fluentd: disablePvc: true Getting Support If you encounter any problems that the documentation does not address, file an issue or talk to us on Discord or Slack.\nCommercial support is also available for the Logging operator.\nBefore asking for help, prepare the following information to make troubleshooting faster:\nLogging operator version kubernetes version helm/chart version (if you installed the Logging operator with helm) Logging operator logs fluentd configuration fluentd logs fluentbit configuration fluentbit logs Do not forget to remove any sensitive information (for example, passwords and private keys) before sharing.\n","categories":"","description":"","excerpt":"Persistent Volumes do not respect the fsGroup value on Kind so disable …","ref":"/4.2/docs/operation/troubleshooting/kind/","tags":"","title":"Running on KinD"},{"body":"Welcome to the Logging operator documentation!\nOverview The Logging operator solves your logging-related problems in Kubernetes environments by automating the deployment and configuration of a Kubernetes logging pipeline.\nThe operator deploys and configures a log collector (currently a Fluent Bit DaemonSet) on every node to collect container and application logs from the node file system. Fluent Bit queries the Kubernetes API and enriches the logs with metadata about the pods, and transfers both the logs and the metadata to a log forwarder instance. The log forwarder instance receives, filters, and transforms the incoming the logs, and transfers them to one or more destination outputs. The Logging operator supports Fluentd and syslog-ng as log forwarders. Your logs are always transferred on authenticated and encrypted channels.\nThis operator helps you bundle logging information with your applications: you can describe the behavior of your application in its charts, the Logging operator does the rest.\nFeature highlights Namespace isolation Native Kubernetes label selectors Secure communication (TLS) Configuration validation Multiple flow support (multiply logs for different transformations) Multiple output support (store the same logs in multiple storage: S3, GCS, ES, Loki and more…) Multiple logging system support (multiple Fluentd, Fluent Bit deployment on the same cluster) Support for both syslog-ng and Fluentd as the central log routing component Architecture The Logging operator manages the log collectors and log forwarders of your logging infrastructure, and the routing rules that specify where you want to send your different log messages.\nThe log collectors are endpoint agents that collect the logs of your Kubernetes nodes and send them to the log forwarders. Logging operator currently uses Fluent Bit as log collector agents.\nThe log forwarder instance receives, filters, and transforms the incoming logs, and transfers them to one or more destination outputs. The Logging operator supports Fluentd and syslog-ng as log forwarders. Which log forwarder is best for you depends on your logging requirements. For tips, see Which log forwarder to use.\nYou can filter and process the incoming log messages using the flow custom resource of the log forwarder to route them to the appropriate output. The outputs are the destinations where you want to send your log messages, for example, Elasticsearch, or an Amazon S3 bucket. You can also define cluster-wide outputs and flows, for example, to use a centralized output that namespaced users can reference but cannot modify. Note that flows and outputs are specific to the type of log forwarder you use (Fluentd or syslog-ng).\nYou can configure the Logging operator using the following Custom Resource Definitions.\nlogging - The logging resource defines the logging infrastructure (the log collectors and forwarders) for your cluster that collects and transports your log messages. It also contains configurations for Fluent Bit, Fluentd, and syslog-ng. CRDs for Fluentd: output - Defines a Fluentd Output for a logging flow, where the log messages are sent using Fluentd. This is a namespaced resource. See also clusteroutput. To configure syslog-ng outputs, see SyslogNGOutput. flow - Defines a Fluentd logging flow using filters and outputs. Basically, the flow routes the selected log messages to the specified outputs. This is a namespaced resource. See also clusterflow. To configure syslog-ng flows, see SyslogNGFlow. clusteroutput - Defines a Fluentd output that is available from all flows and clusterflows. The operator evaluates clusteroutputs in the controlNamespace only unless allowClusterResourcesFromAllNamespaces is set to true. clusterflow - Defines a Fluentd logging flow that collects logs from all namespaces by default. The operator evaluates clusterflows in the controlNamespace only unless allowClusterResourcesFromAllNamespaces is set to true. To configure syslog-ng clusterflows, see SyslogNGClusterFlow. CRDs for syslog-ng (these resources like their Fluentd counterparts, but are tailored to features available via syslog-ng): SyslogNGOutput - Defines a syslog-ng Output for a logging flow, where the log messages are sent using Fluentd. This is a namespaced resource. See also SyslogNGClusterOutput. To configure Fluentd outputs, see output. SyslogNGFlow - Defines a syslog-ng logging flow using filters and outputs. Basically, the flow routes the selected log messages to the specified outputs. This is a namespaced resource. See also SyslogNGClusterFlow. To configure Fluentd flows, see flow. SyslogNGClusterOutput - Defines a syslog-ng output that is available from all flows and clusterflows. The operator evaluates clusteroutputs in the controlNamespace only unless allowClusterResourcesFromAllNamespaces is set to true. SyslogNGClusterFlow - Defines a syslog-ng logging flow that collects logs from all namespaces by default. The operator evaluates clusterflows in the controlNamespace only unless allowClusterResourcesFromAllNamespaces is set to true. To configure Fluentd clusterflows, see clusterflow. For the detailed CRD documentation, see List of CRDs.\nQuickstart See our Quickstart guides.\nSupport If you encounter problems while using the Logging operator the documentation does not address, open an issue or talk to us on Discord or Slack.\n","categories":"","description":"","excerpt":"Welcome to the Logging operator documentation!\nOverview The Logging …","ref":"/4.2/docs/","tags":"","title":"Logging operator"},{"body":"\nThe following tips and commands can help you to troubleshoot your Logging operator installation.\nFirst things to do Check that the necessary CRDs are installed. Issue the following command: kubectl get crd The output should include the following CRDs:\nclusterflows.logging.banzaicloud.io 2019-12-05T15:11:48Z clusteroutputs.logging.banzaicloud.io 2019-12-05T15:11:48Z flows.logging.banzaicloud.io 2019-12-05T15:11:48Z loggings.logging.banzaicloud.io 2019-12-05T15:11:48Z outputs.logging.banzaicloud.io 2019-12-05T15:11:48Z Verify that the Logging operator pod is running. Issue the following command: kubectl get pods |grep logging-operator The output should include the a running pod, for example:\nNAME READY STATUS RESTARTS AGE logging-demo-log-generator-6448d45cd9-z7zk8 1/1 Running 0 24m Check the status of your resources. Beginning with Logging Operator 3.8, all custom resources have a Status and a Problems field. In a healthy system, the Problems field of the resources is empty, for example:\nkubectl get clusteroutput -A Sample output:\nNAMESPACE NAME ACTIVE PROBLEMS default nullout true The ACTIVE column indicates that the ClusterOutput has successfully passed the configcheck and presented it in the current fluentd configuration. When no errors are reported the PROBLEMS column is empty.\nTake a look at another example, in which we have an incorrect ClusterFlow.\nkubectl get clusterflow -o wide Sample output:\nNAME ACTIVE PROBLEMS all-log true nullout false 1 You can see that the nullout Clusterflow is inactive and there is 1 problem with the configuration. To display the problem, check the status field of the object, for example:\nkubectl get clusterflow nullout -o=jsonpath='{.status}' | jq Sample output:\n{ \"active\": false, \"problems\": [ \"dangling global output reference: nullout2\" ], \"problemsCount\": 1 } After that, check the following sections for further tips.\nGetting Support If you encounter any problems that the documentation does not address, file an issue or talk to us on Discord or Slack.\nCommercial support is also available for the Logging operator.\nBefore asking for help, prepare the following information to make troubleshooting faster:\nLogging operator version kubernetes version helm/chart version (if you installed the Logging operator with helm) Logging operator logs fluentd configuration fluentd logs fluentbit configuration fluentbit logs Do not forget to remove any sensitive information (for example, passwords and private keys) before sharing.\n","categories":"","description":"","excerpt":"\nThe following tips and commands can help you to troubleshoot your …","ref":"/4.2/docs/operation/troubleshooting/","tags":"","title":"Logging operator troubleshooting"},{"body":"\nThis guide describes how to collect application and container logs in Kubernetes using the Logging operator, and how to send them to Grafana Loki.\nThe following figure gives you an overview about how the system works. The Logging operator collects the logs from the application, selects which logs to forward to the output, and sends the selected log messages to the output. For more details about the Logging operator, see the Logging operator overview.\nDeploy Loki and Grafana Add the chart repositories of Loki and Grafana using the following commands:\nhelm repo add grafana https://grafana.github.io/helm-charts helm repo add loki https://grafana.github.io/loki/charts helm repo update Install Loki into the logging namespace:\nhelm upgrade --install --create-namespace --namespace logging loki loki/loki Grafana Loki Documentation\nInstall Grafana into the logging namespace:\nhelm upgrade --install --create-namespace --namespace logging grafana grafana/grafana \\ --set \"datasources.datasources\\\\.yaml.apiVersion=1\" \\ --set \"datasources.datasources\\\\.yaml.datasources[0].name=Loki\" \\ --set \"datasources.datasources\\\\.yaml.datasources[0].type=loki\" \\ --set \"datasources.datasources\\\\.yaml.datasources[0].url=http://loki:3100\" \\ --set \"datasources.datasources\\\\.yaml.datasources[0].access=proxy\" Deploy the Logging operator and a demo application Install the Logging operator and a demo application to provide sample log messages.\nDeploy the Logging operator with Helm To install the Logging operator using Helm, complete these steps.\nNote: For the Helm-based installation you need Helm v3.2.1 or later.\nAdd the chart repository of the Logging operator using the following commands:\nhelm repo add kube-logging https://kube-logging.dev/helm-charts helm repo update Install the Logging operator into the logging namespace:\nhelm upgrade --install --wait --create-namespace --namespace logging logging-operator kube-logging/logging-operator Create the logging resource.\nkubectl -n logging apply -f - \u003c\u003c\"EOF\" apiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: default-logging-simple spec: fluentd: {} fluentbit: {} controlNamespace: logging EOF Note: You can use the ClusterOutput and ClusterFlow resources only in the controlNamespace.\nCreate a Loki output definition.\nkubectl -n logging apply -f - \u003c\u003c\"EOF\" apiVersion: logging.banzaicloud.io/v1beta1 kind: Output metadata: name: loki-output spec: loki: url: http://loki:3100 configure_kubernetes_labels: true buffer: timekey: 1m timekey_wait: 30s timekey_use_utc: true EOF Note: In production environment, use a longer timekey interval to avoid generating too many objects.\nCreate a flow resource.\nkubectl -n logging apply -f - \u003c\u003c\"EOF\" apiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: loki-flow spec: filters: - tag_normaliser: {} - parser: remove_key_name_field: true reserve_data: true parse: type: nginx match: - select: labels: app.kubernetes.io/name: log-generator localOutputRefs: - loki-output EOF Install log-generator to produce logs with the label app.kubernetes.io/name: log-generator\nhelm upgrade --install --wait --create-namespace --namespace logging log-generator kube-logging/log-generator Validate your deployment.\nValidate the deployment Grafana Dashboard Use the following command to retrieve the password of the Grafana admin user:\nkubectl get secret --namespace logging grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo Enable port forwarding to the Grafana Service.\nkubectl -n logging port-forward svc/grafana 3000:80 Open the Grafana Dashboard: http://localhost:3000\nUse the admin username and the password retrieved in Step 1 to log in.\nSelect Menu \u003e Explore, select Data source \u003e Loki, then select Log labels \u003e namespace \u003e logging. A list of logs should appear.\nIf you don’t get the expected result you can find help in the troubleshooting section.\n","categories":"","description":"","excerpt":"\nThis guide describes how to collect application and container logs in …","ref":"/4.2/docs/examples/loki-nginx/","tags":"","title":"Store Nginx Access Logs in Grafana Loki with Logging operator"},{"body":"There can be at least two different use cases where one might need multiple sets of node agents running with different configuration while still forwarding logs to the same aggregator.\nOne specific example is when there is a need for a configuration change in a rolling upgrade manner. As new nodes come up, they need to run with a new configuration, while old nodes use the previous configuration.\nThe other use case is when there are different node groups in a cluster for multitenancy reasons for example. You might need different Fluent Bit configurations on the separate node groups in that case.\nStarting with Logging operator version 4.2, you can do that by using the FluentbitAgent CRD. This allows you to implement hard multitenancy on the node group level.\nFor details on using the FluentbitAgent CRD, see Fluent Bit log collector.\nTo configure multiple FluentbitAgent CRDs for a cluster, complete the following steps.\nNote: The examples refer to a scenario where you have two node groups that have the Kubernetes label nodeGroup=A and nodeGroup=B. These labels are fictional and are used only as examples. Node labels are not available in the log metadata, to have similar labels, you have to apply the node labels directly to the pods. How to do that is beyond the scope of this guide (for example, you can use a policy engine, like Kyverno).\nIf you are updating an existing deployment, make sure that it already uses a Logging configuration based on FluentbitAgent CRD. If not, first migrate your configuration to use a FluentbitAgent CRD.\nEdit your existing FluentbitAgent CRD, and set the spec.nodeSelector field so it applies only to the node group you want to apply this Fluent Bit configuration on, for example, nodes that have the label nodeGroup=A. For details, see nodeSelector in the Kubernetes documentation.\napiVersion: logging.banzaicloud.io/v1beta1 kind: FluentbitAgent metadata: # Use the same name as the logging resource does name: multi spec: nodeSelector: nodeGroup: \"A\" Note: If your Logging resource has its spec.loggingRef parameter set, set the same value in the spec.loggingRef parameter of the FluentbitAgent resource.\nSet other FluentbitAgent parameters as needed for your environment.\nCreate a new FluentbitAgent CRD, and set the spec.nodeSelector field so it applies only to the node group you want to apply this Fluent Bit configuration on, for example, nodes that have the label nodeGroup=B. For details, see nodeSelector in the Kubernetes documentation.\napiVersion: logging.banzaicloud.io/v1beta1 kind: FluentbitAgent metadata: name: multi-B spec: nodeSelector: nodeGroup: \"B\" Note: If your Logging resource has its spec.loggingRef parameter set, set the same value in the spec.loggingRef parameter of the FluentbitAgent resource.\nSet other FluentbitAgent parameters as needed for your environment.\nCreate the Flow resources to route the log messages to the outputs. For example, you can select and exclude logs based on their node group labels.\napiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: \"flow-for-nodegroup-A\" spec: match: - select: labels: nodeGroup: \"A\" localOutputRefs: - \"output-for-nodegroup-A\" apiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: \"flow-for-nodegroup-B\" spec: match: - select: labels: nodeGroup: \"B\" localOutputRefs: - \"output-for-nodegroup-B\" Note: If your Logging resource has its spec.loggingRef parameter set, set the same value in the spec.loggingRef parameter of the Flow resource.\nSet other Flow parameters as needed for your environment.\nCreate the outputs (called \"output-for-nodegroup-A\" and \"output-for-nodegroup-B\") for the Flows.\n","categories":"","description":"","excerpt":"There can be at least two different use cases where one might need …","ref":"/4.2/docs/logging-infrastructure/fluentbit-multiple/","tags":"","title":"Multiple Fluent Bit agents in the cluster"},{"body":"This documentation helps to set-up a developer environment and writing plugins for the Logging operator.\nSetting up Kind Install Kind on your computer\ngo get sigs.k8s.io/kind@v0.5.1 Create cluster\nkind create cluster --name logging Install prerequisites (this is a Kubebuilder makefile that will generate and install crds)\nmake install Run the Operator\ngo run main.go Writing a plugin To add a plugin to the logging operator you need to define the plugin struct.\nNote: Place your plugin in the corresponding directory pkg/sdk/model/filter or pkg/sdk/model/output\ntype MyExampleOutput struct { // Path that is required for the plugin Path string `json:\"path,omitempty\"` } The plugin uses the JSON tags to parse and validate configuration. Without tags the configuration is not valid. The fluent parameter name must match with the JSON tag. Don’t forget to use omitempty for non required parameters.\nImplement ToDirective To render the configuration you have to implement the ToDirective function.\nfunc (c *S3OutputConfig) ToDirective(secretLoader secret.SecretLoader) (types.Directive, error) { ... } For simple Plugins you can use the NewFlatDirective function.\nfunc (c *ExampleOutput) ToDirective(secretLoader secret.SecretLoader) (types.Directive, error) { return types.NewFlatDirective(types.PluginMeta{ Type: \"example\", Directive: \"output\", Tags: \"**\", }, c, secretLoader) } For more example please check the available plugins.\nReuse existing Plugin sections You can embed existing configuration for your plugins. For example modern Output plugins have Buffer section.\n// +docLink:\"Buffer,./buffer.md\" Buffer *Buffer `json:\"buffer,omitempty\"` If you are using embedded sections you must call its ToDirective method manually and append it as a SubDirective\nif c.Buffer != nil { if buffer, err := c.Buffer.ToDirective(secretLoader); err != nil { return nil, err } else { s3.SubDirectives = append(s3.SubDirectives, buffer) } } Special plugin tags To document the plugins logging-operator uses the Go tags (like JSON tags). Logging operator uses plugin named tags for special instructions.\nSpecial tag default The default tag helps to give default values for parameters. These parameters are explicitly set in the generated fluentd configuration.\nRetryForever bool `json:\"retry_forever\" plugin:\"default:true\"` Special tag required The required tag ensures that the attribute cannot be empty\nRetryForever bool `json:\"retry_forever\" plugin:\"required\"` Generate documentation for Plugin The operator parse the docstrings for the documentation.\n... // AWS access key id AwsAccessKey *secret.Secret `json:\"aws_key_id,omitempty\"` ... Will generate the following Markdown\nVariable Name Default Applied function AwsAccessKey AWS access key id You can hint default values in docstring via (default: value). This is useful if you don’t want to set default explicitly with tag. However during rendering defaults in tags have priority over docstring.\n... // The format of S3 object keys (default: %{path}%{time_slice}_%{index}.%{file_extension}) S3ObjectKeyFormat string `json:\"s3_object_key_format,omitempty\"` ... Special docstrings +docName:\"Title for the plugin section\" +docLink:\"Buffer,./buffer.md\" You can declare document title and description above the type _doc* interface{} variable declaration.\nExample Document headings:\n// +docName:\"Amazon S3 plugin for Fluentd\" // **s3** output plugin buffers event logs in local file and upload it to S3 periodically. This plugin splits files exactly by using the time of event logs (not the time when the logs are received). For example, a log '2011-01-02 message B' is reached, and then another log '2011-01-03 message B' is reached in this order, the former one is stored in \"20110102.gz\" file, and latter one in \"20110103.gz\" file. type _docS3 interface{} Example Plugin headings:\n// +kubebuilder:object:generate=true // +docName:\"Shared Credentials\" type S3SharedCredentials struct { ... Example linking embedded sections\n// +docLink:\"Buffer,./buffer.md\" Buffer *Buffer `json:\"buffer,omitempty\"` Generate docs for your Plugin make docs ","categories":"","description":"","excerpt":"This documentation helps to set-up a developer environment and writing …","ref":"/4.2/docs/developers/","tags":"","title":"For developers"},{"body":"\nYou can configure the Logging operator to expose metrics endpoints for Fluentd, Fluent Bit, and syslog-ng using ServiceMonitor resources. That way, a Prometheus operator running in the same cluster can automatically fetch your logging metrics.\nMetrics Variables You can configure the following metrics-related options in the spec.fluentd.metrics, spec.syslogNG.metrics, and spec.fluentbit.metrics sections of your Logging resource.\nVariable Name Type Required Default Description interval string No “15s” Scrape Interval timeout string No “5s” Scrape Timeout port int No - Metrics Port. path int No - Metrics Path. serviceMonitor bool No false Enable to create ServiceMonitor for Prometheus operator prometheusAnnotations bool No false Add prometheus labels to fluent pods. For example:\nspec: fluentd: metrics: serviceMonitor: true fluentbit: metrics: serviceMonitor: true syslogNG: metrics: serviceMonitor: true For more details on installing the Prometheus operator and configuring and accessing metrics, see the following procedures.\nInstall Prometheus Operator with Helm Create logging namespace\nkubectl create namespace logging Install Prometheus Operator\nhelm upgrade --install --wait --create-namespace --namespace logging monitor stable/prometheus-operator \\ --set \"grafana.dashboardProviders.dashboardproviders\\\\.yaml.apiVersion=1\" \\ --set \"grafana.dashboardProviders.dashboardproviders\\\\.yaml.providers[0].orgId=1\" \\ --set \"grafana.dashboardProviders.dashboardproviders\\\\.yaml.providers[0].type=file\" \\ --set \"grafana.dashboardProviders.dashboardproviders\\\\.yaml.providers[0].disableDeletion=false\" \\ --set \"grafana.dashboardProviders.dashboardproviders\\\\.yaml.providers[0].options.path=/var/lib/grafana/dashboards/default\" \\ --set \"grafana.dashboards.default.logging.gnetId=7752\" \\ --set \"grafana.dashboards.default.logging.revision=5\" \\ --set \"grafana.dashboards.default.logging.datasource=Prometheus\" \\ --set \"prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=False\" Prometheus Operator Documentation The prometheus-operator install may take a few more minutes. Please be patient. The logging-operator metrics function depends on the prometheus-operator’s resources. If those do not exist in the cluster it may cause the logging-operator’s malfunction.\nInstall Logging Operator with Helm Add operator chart repository:\nhelm repo add kube-logging https://kube-logging.dev/helm-charts Logging Operator\nhelm upgrade --install --wait --create-namespace --namespace logging-operator logging kube-logging/logging-operator Install Minio Create Minio Credential Secret\nkubectl -n logging create secret generic logging-s3 --from-literal=accesskey='AKIAIOSFODNN7EXAMPLE' --from-literal=secretkey='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY' Deploy Minio\nkubectl -n logging apply -f - \u003c\u003c\"EOF\" apiVersion: apps/v1 kind: Deployment metadata: name: minio-deployment namespace: logging spec: selector: matchLabels: app: minio strategy: type: Recreate template: metadata: labels: app: minio spec: containers: - name: minio image: minio/minio args: - server - /storage readinessProbe: httpGet: path: /minio/health/ready port: 9000 initialDelaySeconds: 10 periodSeconds: 5 env: - name: MINIO_REGION value: 'test_region' - name: MINIO_ACCESS_KEY valueFrom: secretKeyRef: name: logging-s3 key: accesskey - name: MINIO_SECRET_KEY valueFrom: secretKeyRef: name: logging-s3 key: secretkey ports: - containerPort: 9000 volumes: - name: logging-s3 secret: secretName: logging-s3 --- kind: Service apiVersion: v1 metadata: name: nginx-demo-minio namespace: logging spec: selector: app: minio ports: - protocol: TCP port: 9000 targetPort: 9000 EOF Create logging resource\nkubectl apply -f - \u003c\u003c\"EOF\" apiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: default-logging-simple spec: fluentd: metrics: serviceMonitor: true fluentbit: metrics: serviceMonitor: true controlNamespace: logging EOF Note: ClusterOutput and ClusterFlow resource will only be accepted in the controlNamespace\nCreate Minio output definition\nkubectl -n logging apply -f - \u003c\u003c\"EOF\" apiVersion: logging.banzaicloud.io/v1beta1 kind: Output metadata: name: demo-output spec: s3: aws_key_id: valueFrom: secretKeyRef: key: accesskey name: logging-s3 aws_sec_key: valueFrom: secretKeyRef: key: secretkey name: logging-s3 buffer: timekey: 10s timekey_use_utc: true timekey_wait: 0s force_path_style: \"true\" path: logs/${tag}/%Y/%m/%d/ s3_bucket: demo s3_endpoint: http://nginx-demo-minio.logging.svc.cluster.local:9000 s3_region: test_region EOF Note: For production set-up we recommend using longer timekey interval to avoid generating too many object.\nCreate flow resource\nkubectl -n logging apply -f - \u003c\u003c\"EOF\" apiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: demo-flow spec: filters: - tag_normaliser: {} - parser: remove_key_name_field: true reserve_data: true parse: type: nginx match: - select: labels: app.kubernetes.io/instance: log-generator app.kubernetes.io/name: log-generator localOutputRefs: - demo-output EOF Install log-generator to produce logs with the label app.kubernetes.io/name: log-generator\nhelm upgrade --install --wait --create-namespace --namespace logging log-generator kube-logging/log-generator Validation Minio Get Minio login credentials\nkubectl -n logging get secrets logging-s3 -o json | jq '.data | map_values(@base64d)' Forward Service\nkubectl -n logging port-forward svc/nginx-demo-minio 9000 Open the Minio Dashboard: http://localhost:9000\nPrometheus Forward Service\nkubectl port-forward svc/monitor-prometheus-operato-prometheus 9090 Open the Prometheus Dashboard: http://localhost:9090\nGrafana Get Grafana login credentials\nkubectl get secret --namespace logging monitor-grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo Default username: admin\nForward Service\nkubectl -n logging port-forward svc/monitor-grafana 3000:80 Open Gradana Dashboard: http://localhost:3000\n","categories":"","description":"","excerpt":"\nYou can configure the Logging operator to expose metrics endpoints …","ref":"/4.2/docs/operation/logging-operator-monitoring/","tags":"","title":"Monitor your logging pipeline with Prometheus Operator"},{"body":"To use TLS encryption in your logging infrastructure, you have to configure encryption:\nfor the log collection part of your logging pipeline (between Fluent Bit and Fluentd or Fluent bit and syslog-ng), and for the output plugin (between Fluentd or syslog-ng and the output backend). For configuring the output, see the documentation of the output plugin you want to use at Outputs.\nFor Fluentd and Fluent Bit, you can configure encryption in the logging resource using the following parameters:\nName Type Default Description enabled bool “Yes” Enable TLS encryption secretName string \"\" Kubernetes secret that contains: tls.crt, tls.key, ca.crt sharedKey string \"\" Shared secret for fluentd authentication For example:\napiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: default-logging-tls spec: fluentd: tls: enabled: true secretName: fluentd-tls sharedKey: example-secret fluentbit: tls: enabled: true secretName: fluentbit-tls sharedKey: example-secret controlNamespace: logging For other parameters of the logging resource, see LoggingSpec.\n","categories":"","description":"","excerpt":"To use TLS encryption in your logging infrastructure, you have to …","ref":"/4.2/docs/logging-infrastructure/tls/","tags":"","title":"TLS encryption"},{"body":"This section describes how to set alerts for your logging infrastructure. Alternatively, you can enable the default alerting rules that are provided by the Logging operator.\nNote: Alerting based on the contents of the collected log messages is not covered here.\nPrerequisites Using alerting rules requires the following:\nLogging operator 3.14.0 or newer installed on the cluster. Prometheus operator installed on the cluster. For details, see Monitor your logging pipeline with Prometheus Operator. syslog-ng is supported only in Logging operator 4.0 or newer.\nEnable the default alerting rules Logging operator comes with a number of default alerting rules that help you monitor your logging environment and ensure that it’s working properly. To enable the default rules, complete the following steps.\nVerify that your cluster meets the Prerequisites.\nEnable the alerting rules in your logging CR. You can enable alerting separately for Fluentd, syslog-ng, and Fluent Bit. For example:\napiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: default-logging-simple namespace: logging spec: fluentd: metrics: prometheusRules: true fluentbit: metrics: prometheusRules: true syslogNG: metrics: prometheusRules: true controlNamespace: logging If needed you can add custom alerting rules.\nOverview of default alerting rules The default alerting rules trigger alerts when:\nFor the Fluent Bit log collector:\nThe number of Fluent Bit errors or retries is high For the Fluentd and syslog-ng log forwarders:\nPrometheus cannot access the log forwarder node The buffers of the log forwarder are filling up quickly Traffic to the log forwarder is increasing at a high rate The number of errors or retries is high on the log forwarder The buffers are over 90% full Currently, you cannot modify the default alerting rules, because they are generated from the source files. For the detailed list of alerts, see the source code:\nFor Fluentd For syslog-ng For Fluent Bit To enable these alerts on your cluster, see Enable the default alerting rules.\nAdd custom alerting rules Although you cannot modify the default alerting rules, you can add your own custom rules to the cluster by creating and applying AlertmanagerConfig resources to the Prometheus Operator.\nFor example, the Logging operator creates the following alerting rule to detect if a Fluentd node is down:\napiVersion: monitoring.coreos.com/v1 kind: PrometheusRule name: logging-demo-fluentd-metrics namespace: logging spec: groups: - name: fluentd rules: - alert: FluentdNodeDown annotations: description: Prometheus could not scrape {{ \"{{ $labels.job }}\" }} for more than 30 minutes summary: fluentd cannot be scraped expr: up{job=\"logging-demo-fluentd-metrics\", namespace=\"logging\"} == 0 for: 10m labels: service: fluentd severity: critical On the Prometheus web interface, this rule looks like:\n","categories":"","description":"","excerpt":"This section describes how to set alerts for your logging …","ref":"/4.2/docs/operation/alerting/","tags":"","title":"Alerting"},{"body":"How can I run the unreleased master version? Clone the logging-operator repo.\ngit clone git@github.com:kube-logging/logging-operator.git Navigate to the logging-operator folder.\ncd logging-operator Install with helm\nHelm v3\nhelm upgrade --install --wait --create-namespace --namespace logging logging ./charts/logging-operator --set image.tag=master How can I support the project? Give a star to this repository Add your company to the adopters list ","categories":"","description":"","excerpt":"How can I run the unreleased master version? Clone the …","ref":"/4.2/docs/faq/","tags":"","title":"Frequently asked questions"},{"body":"Define secret value Secrets can be used in logging-operator Output definitions.\nSecrets MUST be in the SAME namespace as the Output or ClusterOutput custom resource\nExample secret definition\naws_key_id: valueFrom: secretKeyRef: name: \u003ckubernetes-secret-name\u003e key: \u003ckubernetes-secret-key\u003e For debug purposes you can define secret values directly. However this is NOT recommended in production.\naws_key_id: value: \"secretvalue\" Define secret mount There are cases when you can’t inject secret into the configuration because the plugin need a file to read from. For this cases you can use mountFrom.\ntls_cert_path: mountFrom: secretKeyRef: name: \u003ckubernetes-secret-name\u003e key: \u003ckubernetes-secret-key\u003e The operator will collect the secret and copy it to the fluentd-output secret. The fluentd configuration will contain the secret path.\nExample rendered configuration\n\u003cmatch **\u003e @type forward tls_cert_path /fluentd/etc/secret/default-fluentd-tls-tls.crt ... \u003c/match\u003e How it works? Behind the scene the operator marks the secret with an annotation and watches it for changes as long as the annotation is present.\nExample annotated secret\napiVersion: v1 kind: Secret type: Opaque metadata: annotations: logging.banzaicloud.io/default: watched name: fluentd-tls namespace: default data: tls.crt: SGVsbG8gV29ybGQ= The annotation format is logging.banzaicloud.io/\u003cloggingRef\u003e: watched. Since the name part of the an annotation can’t be empty the default applies to empty loggingRef value as well.\nThe mount path is generated from the secret information\n/fluentd/etc/secret/$namespace-$secret_name-$secret_key ","categories":"","description":"","excerpt":"Define secret value Secrets can be used in logging-operator Output …","ref":"/4.2/docs/configuration/plugins/outputs/secret/","tags":"","title":"Secret definition"},{"body":"The tailer-webhook is a different approach for the same problem: parsing legacy application’s log file. As an alternative to using a host file tailer service, you can use a file tailer webhook service. While the containers of the host file tailers run in a separated pod, file tailer webhook uses a different approach: if a pod has a specific annotation, the webhook injects a sidecar container for every tailed file into the pod.\nThe tailer-webhook behaves differently compared to the host-tailer:\nPros:\nA simple annotation on the pod initiates the file tailing. There is no need to use mounted volumes, Logging operator will manage the volumes and mounts between your containers. Cons:\nRequired to start the Logging operator with webhooks service enabled. This requires additional configuration, especially on certificates since webhook services are allowed over TLS only. Possibly uses more resources, since every tailed file attaches a new sidecar container to the pod. Enable webhooks in Logging operator We recommend using cert-manager to manage your certificates. Since using cert-manager is not part of this article, we assume you already have valid certs.\nYou will require the following things:\na valid client certificate, a CA certificate, and a custom value.yaml file for your helm chart. The following example refers to a Kubernetes secret named webhook-tls which is a self-signed certificate generated by cert-manager.\nAdd the following lines to your custom values.yaml or create a new file if needed:\nenv: - name: ENABLE_WEBHOOKS value: \"true\" volumes: - name: webhook-tls secret: secretName: webhook-tls volumeMounts: - name: webhook-tls mountPath: /tmp/k8s-webhook-server/serving-certs This will:\nSet ENABLE_WEBHOOKS environment variable to true. This is the official way to enable webhooks in Logging operator. Create a volume from the webhook-tls Kubernetes secret. Mount the webhook-tls secret volume to the /tmp/k8s-webhook-server/serving-certs path where Logging operator will search for it. Now you are ready to install Logging operator with the new custom values:\nhelm upgrade --install --wait --create-namespace --namespace logging -f operator_values.yaml logging-operator ./charts/logging-operator Alternatively, instead of using the values.yaml file, you can run the installation from command line also by passing the values with the set and set-string parameters:\nhelm upgrade --install --wait --create-namespace --namespace logging --set \"env[0].name=ENABLE_WEBHOOKS\" --set-string \"env[0].value=true\" --set \"volumes[0].name=webhook-tls\" --set \"volumes[0].secret.secretName=webhook-tls\" --set \"volumeMounts[0].name=webhook-tls\" --set \"volumeMounts[0].mountPath=/tmp/k8s-webhook-server/serving-certs\" logging-operator ./charts/logging-operator You also need a service which points to the webhook port (9443) of Logging operator, and where the mutatingwebhookconfiuration will point to. Running the following command in shell will create the required service:\nkubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: Service metadata: name: logging-webhooks namespace: logging spec: ports: - name: logging-webhooks port: 443 targetPort: 9443 protocol: TCP selector: app.kubernetes.io/instance: logging-operator type: ClusterIP EOF Furthermore, you need to tell Kubernetes to send admission requests to our webhook service. To do that, create a mutatingwebhookconfiguration Kubernetes resource, and:\nSet the configuration to call /tailer-webhook path on your logging-webhooks service when v1.Pod is created. Set failurePolicy to ignore, which means that the original pod will be created on webhook errors. Set sideEffects to none, because we won’t cause any out-of-band changes in Kubernetes. Unfortunately, mutatingwebhookconfiguration requires the caBundle field to be filled because we used a self-signed certificate, and the certificate cannot be validated through the system trust roots. If your certificate was generated with a system trust root CA, remove the caBundle line, because the certificate will be validated automatically. There are more sophisticated ways to load the CA into this field, but this solution requires no further components.\nFor example: you can inject the CA with a simple cert-manager cert-manager.io/inject-ca-from: logging/webhook-tls annotation on the mutatingwebhookconfiguration resource.\nkubectl apply -f - \u003c\u003cEOF apiVersion: admissionregistration.k8s.io/v1 kind: MutatingWebhookConfiguration metadata: name: sample-webhook-cfg namespace: logging labels: app: sample-webhook webhooks: - name: sample-webhook.banzaicloud.com clientConfig: service: name: logging-webhooks namespace: logging path: \"/tailer-webhook\" caBundle: $(kubectl get secret webhook-tls -n logging -o json | jq -r '.data[\"ca.crt\"]') rules: - operations: [ \"CREATE\" ] apiGroups: [\"\"] apiVersions: [\"v1\"] resources: [\"pods\"] scope: \"*\" failurePolicy: Ignore sideEffects: None admissionReviewVersions: [v1] EOF Triggering the webhook CAUTION:\nTo use the webhook, you must first enable webhooks in the Logging operator. File tailer webhook is based on a Mutating Admission Webhook. It is called every time when a pod starts.\nTo trigger the webhook, add the following annotation to the pod metadata:\nAnnotation key: sidecar.logging-extensions.banzaicloud.io/tail\nValue of the annotation: the filename (including path, and optionally the container) you want to tail, for example:\nannotations: {\"sidecar.logging-extensions.banzaicloud.io/tail\": \"/var/log/date\"} To tail multiple files, add only one annotation, and separate the filenames with commas, for example:\n... metadata: name: test-pod annotations: {\"sidecar.logging-extensions.banzaicloud.io/tail\": \"/var/log/date,/var/log/mycustomfile\"} spec: ... If the pod contains multiple containers, see Multi-container pods.\nNote: If the pod with the sidecar annotation is in the default namespace, Logging operator handles tailer-webhook annotations clusterwide. To restrict the webhook callbacks to the current namespace, change the scope of the mutatingwebhookconfiguration to namespaced.\nFile tailer example The following example creates a pod that is running a shell in infinite loop that appends the date command’s output to a file every second. The annotation sidecar.logging-extensions.banzaicloud.io/tail notifies Logging operator to attach a sidecar container to the pod. The sidecar tails the /legacy-logs/date.log file and sends its output to the stdout.\napiVersion: v1 kind: Pod metadata: name: test-pod annotations: {\"sidecar.logging-extensions.banzaicloud.io/tail\": \"/var/log/date\"} spec: containers: - image: debian name: sample-container command: [\"/bin/sh\", \"-c\"] args: - while true; do date \u003e\u003e /var/log/date; sleep 1; done - image: debian name: sample-container2 ... After you have created the pod with the required annotation, make sure that the test-pod contains two containers by running kubectl get pod\nExpected output:\nNAME READY STATUS RESTARTS AGE test-pod 2/2 Running 0 29m Check the container names in the pod to see that the Logging operator has created the sidecar container called legacy-logs-date-log. The sidecar containers’ name is always built from the path and name of the tailed file. Run the following command:\nkubectl get pod test-pod -o json | jq '.spec.containers | map(.name)' Expected output:\n[ \"test\", \"legacy-logs-date-log\" ] Check the logs of the test container. Since it writes the logs into a file, it does not produce any logs on stdout.\nkubectl logs test-pod test; echo $? Expected output:\n0 Check the logs of the legacy-logs-date-log container. This container exposes the logs of the test container on its stdout.\nkubectl logs test-pod legacy-logs-date-log Expected output:\nFluent Bit v1.9.5 * Copyright (C) 2015-2022 The Fluent Bit Authors * Fluent Bit is a CNCF sub-project under the umbrella of Fluentd * https://fluentbit.io [2022/09/15 11:26:11] [ info] [fluent bit] version=1.9.5, commit=9ec43447b6, pid=1 [2022/09/15 11:26:11] [ info] [storage] version=1.2.0, type=memory-only, sync=normal, checksum=disabled, max_chunks_up=128 [2022/09/15 11:26:11] [ info] [cmetrics] version=0.3.4 [2022/09/15 11:26:11] [ info] [sp] stream processor started [2022/09/15 11:26:11] [ info] [input:tail:tail.0] inotify_fs_add(): inode=938627 watch_fd=1 name=/legacy-logs/date.log [2022/09/15 11:26:11] [ info] [output:file:file.0] worker #0 started Thu Sep 15 11:26:11 UTC 2022 Thu Sep 15 11:26:12 UTC 2022 ... Multi-container pods In some cases you have multiple containers in your pod and you want to distinguish which file annotation belongs to which container. You can order every file annotations to particular container by prefixing the annotation with a ${ContainerName}: container key. For example:\n... metadata: name: test-pod annotations: {\"sidecar.logging-extensions.banzaicloud.io/tail\": \"sample-container:/var/log/date,sample-container2:/var/log/anotherfile,/var/log/mycustomfile,foobarbaz:/foo/bar/baz\"} spec: ... CAUTION:\nAnnotations without containername prefix: the file gets tailed on the default container (container 0) Annotations with invalid containername: file tailer annotation gets discarded Annotation Explanation sample-container:/var/log/date tails file /var/log/date in sample-container sample-container2:/var/log/anotherfile tails file /var/log/anotherfile in sample-container2 /var/log/mycustomfile tails file /var/log/mycustomfile in default container (sample-container) foobarbaz:/foo/bar/baz will be discarded due to non-existing container name ","categories":"","description":"","excerpt":"The tailer-webhook is a different approach for the same problem: …","ref":"/4.2/docs/configuration/extensions/tailer-webhook/","tags":"","title":"Tail logfiles with a webhook"},{"body":"This section describes how to configure readiness probes for your Fluentd and syslog-ng pods. If you don’t configure custom readiness probes, Logging operator uses the default probes.\nPrerequisites Configuring readiness probes requires Logging operator 3.14.0 or newer installed on the cluster. syslog-ng is supported only in Logging operator 4.0 or newer.\nOverview of default readiness probes By default, Logging operator performs the following readiness checks:\nNumber of buffer files is too high (higher than 5000) Fluentd buffers are over 90% full syslog-ng buffers are over 90% full The parameters of the readiness probes and pod failure is set by using the usual Kubernetes probe configuration parameters. Instead of the Kubernetes defaults, the Logging operator uses the following values for these parameters:\nInitialDelaySeconds: 5 TimeoutSeconds: 3 PeriodSeconds: 30 SuccessThreshold: 3 FailureThreshold: 1 Currently, you cannot modify the default readiness probes, because they are generated from the source files. For the detailed list of readiness probes, see the Default readiness probes. However, you can customize their values in the Logging custom resource, separately for the Fluentd and syslog-ng log forwarder. For example:\nFluentd readiness probe settings apiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: logging-demo spec: controlNamespace: logging fluentd: readinessDefaultCheck: bufferFileNumber: true bufferFileNumberMax: 5000 bufferFreeSpace: true bufferFreeSpaceThreshold: 90 failureThreshold: 1 initialDelaySeconds: 5 periodSeconds: 30 successThreshold: 3 timeoutSeconds: 3 SyslogNG readiness probe settings apiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: logging-demo spec: controlNamespace: logging syslogNG: readinessDefaultCheck: bufferFileNumber: true bufferFileNumberMax: 5000 bufferFreeSpace: true bufferFreeSpaceThreshold: 90 failureThreshold: 1 initialDelaySeconds: 5 periodSeconds: 30 successThreshold: 3 timeoutSeconds: 3 Default readiness probes The Logging operator applies the following readiness probe by default:\nreadinessProbe: exec: command: - /bin/sh - -c - FREESPACE_THRESHOLD=90 - FREESPACE_CURRENT=$(df -h $BUFFER_PATH | grep / | awk '{ print $5}' | sed 's/%//g') - if [ \"$FREESPACE_CURRENT\" -gt \"$FREESPACE_THRESHOLD\" ] ; then exit 1; fi - MAX_FILE_NUMBER=5000 - FILE_NUMBER_CURRENT=$(find $BUFFER_PATH -type f -name *.buffer | wc -l) - if [ \"$FILE_NUMBER_CURRENT\" -gt \"$MAX_FILE_NUMBER\" ] ; then exit 1; fi failureThreshold: 1 initialDelaySeconds: 5 periodSeconds: 30 successThreshold: 3 timeoutSeconds: 3 Add custom readiness probes You can add your own custom readiness probes to the spec.ReadinessProbe section of the logging custom resource. For details on the format of readiness probes, see the official Kubernetes documentation.\nCAUTION:\nIf you set any custom readiness probes, they completely override the default probes. ","categories":"","description":"","excerpt":"This section describes how to configure readiness probes for your …","ref":"/4.2/docs/operation/readiness-probe/","tags":"","title":"Readiness probe"},{"body":"Security Variables Variable Name Type Required Default Description roleBasedAccessControlCreate bool No True create RBAC resources podSecurityPolicyCreate bool No False create PSP resources serviceAccount string No - Set ServiceAccount securityContext SecurityContext No {} SecurityContext holds security configuration that will be applied to a container. podSecurityContext PodSecurityContext No {} PodSecurityContext holds pod-level security attributes and common container settings. Some Using RBAC Authorization By default, RBAC is enabled.\nDeploy with Kubernetes Manifests Create logging resource with RBAC kubectl -n logging apply -f - \u003c\u003c\"EOF\" apiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: default-logging-simple spec: fluentd: security: roleBasedAccessControlCreate: true fluentbit: security: roleBasedAccessControlCreate: true controlNamespace: logging EOF Example Manifest Generated by the operator Fluentd Role \u0026 RoleBinding Output - apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: logging-demo-nginx-logging-demo-logging-fluentd namespace: logging ownerReferences: - apiVersion: logging.banzaicloud.io/v1beta1 controller: true kind: Logging rules: - apiGroups: - \"\" resources: - configmaps - secrets verbs: - '*' -- - apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: annotations: name: logging-demo-nginx-logging-demo-logging-fluentd namespace: logging ownerReferences: - apiVersion: logging.banzaicloud.io/v1beta1 controller: true kind: Logging roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: logging-demo-nginx-logging-demo-logging-fluentd subjects: - kind: ServiceAccount name: logging-demo-nginx-logging-demo-logging-fluentd namespace: logging Fluentbit ClusterRole \u0026 ClusterRoleBinding Output kind: ClusterRole metadata: annotations: name: logging-demo-nginx-logging-demo-logging-fluentbit ownerReferences: - apiVersion: logging.banzaicloud.io/v1beta1 controller: true kind: Logging rules: - apiGroups: - \"\" resources: - pods - namespaces verbs: - get - list - watch --- kind: ClusterRoleBinding metadata: annotations: name: logging-nginx-demo-nginx-logging-demo-logging-fluentbit ownerReferences: - apiVersion: logging.banzaicloud.io/v1beta1 controller: true kind: Logging roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: nginx-demo-nginx-logging-demo-logging-fluentbit subjects: - kind: ServiceAccount name: nginx-demo-nginx-logging-demo-logging-fluentbit namespace: logging Service Account (SA) Deploy with Kubernetes Manifests Create logging resource with Service Account kubectl -n logging apply -f - \u003c\u003c\"EOF\" apiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: default-logging-simple spec: fluentd: security: serviceAccount: fluentdUser1 fluentbit: security: serviceAccount: fluentbitUser1 controlNamespace: logging EOF Enabling Pod Security Policies (PSP) This option depends on the roleBasedAccessControlCreate enabled status because the psp require rbac roles also.\nDeploy with Kubernetes Manifests Create logging resource with PSP\nkubectl -n logging apply -f - \u003c\u003c\"EOF\" apiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: default-logging-simple spec: fluentd: security: podSecurityPolicyCreate: true roleBasedAccessControlCreate: true fluentbit: security: podSecurityPolicyCreate: true roleBasedAccessControlCreate: true controlNamespace: logging EOF Example Manifest Generated by the operator Fluentd PSP+Role Output apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: nginx-demo-nginx-logging-demo-logging-fluentd-psp rules: - apiGroups: - policy - extensions resources: - podsecuritypolicies resourceNames: - nginx-demo-nginx-logging-demo-logging-fluentd verbs: - use --- apiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: name: nginx-demo-nginx-logging-demo-logging-fluentd spec: allowPrivilegeEscalation: false fsGroup: ranges: - max: 101 min: 101 rule: MustRunAs runAsUser: ranges: - max: 100 min: 100 rule: MustRunAs seLinux: rule: RunAsAny supplementalGroups: ranges: - max: 101 min: 101 rule: MustRunAs volumes: - configMap - emptyDir - secret - hostPath - persistentVolumeClaim Fluentbit PSP+ClusterRole Output apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: nginx-demo-nginx-logging-demo-logging-fluentbit-psp rules: - apiGroups: - policy resources: - nginx-demo-nginx-logging-demo-logging-fluentbit verbs: - use --- apiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: name: nginx-demo-nginx-logging-demo-logging-fluentbit spec: allowPrivilegeEscalation: false allowedHostPaths: - pathPrefix: /var/lib/docker/containers readOnly: true - pathPrefix: /var/log readOnly: true fsGroup: rule: RunAsAny readOnlyRootFilesystem: true runAsUser: rule: RunAsAny seLinux: rule: RunAsAny supplementalGroups: rule: RunAsAny volumes: - configMap - emptyDir - secret - hostPath Security Context Security Context Parameters POD Security Context Parameters Deploy with Kubernetes Manifests Create logging resource with PSP kubectl -n logging apply -f - \u003c\u003c\"EOF\" apiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: default-logging-simple spec: fluentd: security: securityContext: allowPrivilegeEscalation: false readOnlyRootFilesystem: false podSecurityContext: fsGroup: 101 fluentbit: security: securityContext: allowPrivilegeEscalation: false readOnlyRootFilesystem: true podSecurityContext: fsGroup: 101 controlNamespace: logging EOF Example Manifest Generated by the operator apiVersion: v1 kind: Pod metadata: name: nginx-demo-nginx-logging-demo-logging-fluentd-0 namespace: logging spec: containers: - image: ghcr.io/kube-logging/fluentd:v1.15 imagePullPolicy: IfNotPresent name: fluentd securityContext: allowPrivilegeEscalation: false readOnlyRootFilesystem: false ... schedulerName: default-scheduler securityContext: fsGroup: 101 serviceAccount: nginx-demo-nginx-logging-demo-logging-fluentd ... ","categories":"","description":"","excerpt":"Security Variables Variable Name Type Required Default Description …","ref":"/4.2/docs/logging-infrastructure/security/","tags":"","title":"Security"},{"body":"This section describes how to collect Fluentd error messages (messages that are sent to the @ERROR label from another plugin in Fluentd).\nNote: It depends on the specific plugin implementation what messages are sent to the @ERROR label. For example, a parsing plugin that fails to parse a line could send that line to the @ERROR label.\nPrerequisites Configuring readiness probes requires Logging operator 3.14.0 or newer installed on the cluster.\nConfigure error output To collect the error messages of Fluentd, complete the following steps.\nCreate a ClusterOutput that receives logs from every logging flow where error happens. For example, create a file output. For details on creating outputs, see Output and ClusterOutput.\napiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterOutput metadata: name: error-file namespace: default spec: file: path: /tmp/error.log Set the errorOutputRef in the Logging resource to your preferred ClusterOutput.\napiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: example spec: controlNamespace: default enableRecreateWorkloadOnImmutableFieldChange: true errorOutputRef: error-file fluentbit: bufferStorage: {} bufferStorageVolume: hostPath: path: \"\" filterKubernetes: {} # rest of the resource is omitted You cannot apply filters for this specific error flow.\nApply the ClusterOutput and Logging to your cluster.\n","categories":"","description":"","excerpt":"This section describes how to collect Fluentd error messages (messages …","ref":"/4.2/docs/operation/error-output/","tags":"","title":"Collect Fluentd errors"},{"body":"Watch specific resources The Logging operator watches resources in all namespaces, which is required because it manages cluster-scoped objects, and also objects in multiple namespaces.\nHowever, in a large-scale infrastructure, where the number of resources is large, it makes sense to limit the scope of resources monitored by the Logging operator to save considerable amount of memory and container restarts.\nStarting with Logging operator version 3.12.0, this is now available by passing the following command-line arguments to the operator.\nwatch-namespace: Watch only objects in this namespace. Note that even if the watch-namespace option is set, the operator must watch certain objects (like Flows and Outputs) in every namespace. watch-logging-name: Logging resource name to optionally filter the list of watched objects based on which logging they belong to by checking the app.kubernetes.io/managed-by label. ","categories":"","description":"","excerpt":"Watch specific resources The Logging operator watches resources in all …","ref":"/4.2/docs/operation/optimization/","tags":"","title":"Optimization"},{"body":"In a large-scale infrastructure the logging components can get high load as well. The typical sign of this is when fluentd cannot handle its buffer directory size growth for more than the configured or calculated (timekey + timekey_wait) flush interval. In this case, you can scale the fluentd statefulset.\nCAUTION:\nWhen scaling down Fluentd, the Logging operator does not flush the buffers before terminating the pod. Unless you have a good plan to get the data out from the detached PVC, we don’t recommend scaling Fluentd down directly from the Logging operator.\nTo avoid this problem, you can either write a custom readiness check to get the last pod out from the endpoints of the service, and stop the node only when its buffers are empty.\nNote: When multiple instances send logs to the same output, the output can receive chunks of messages out of order. Some outputs tolerate this (for example, Elasticsearch), some do not, some require fine tuning (for example, Loki).\n","categories":"","description":"","excerpt":"In a large-scale infrastructure the logging components can get high …","ref":"/4.2/docs/operation/scaling/","tags":"","title":"Scaling"},{"body":"The resource requirements and limits of your Logging operator deployment must match the size of your cluster and the logging workloads. By default, the Logging operator uses the following configuration.\nFor Fluent Bit:\n- Limits: - cpu: 200m - memory: 100M - Requests: - cpu: 100m - memory: 50M For Fluentd and syslog-ng:\n- Limits: - cpu: 1000m - memory: 400M - Requests: - cpu: 500m - memory: 100M You can adjust these values in the Logging custom resource, for example:\napiVersion: logging.banzaicloud.io/v1beta1 kind: Logging metadata: name: default-logging namespace: logging spec: fluentd: resources: requests: cpu: 1 memory: 1Gi limits: cpu: 2 memory: 2Gi fluentbit: resources: requests: cpu: 500m memory: 500M limits: cpu: 1 memory: 1Gi syslogNG: resources: requests: cpu: 500m memory: 500M limits: cpu: 1 memory: 1Gi ","categories":"","description":"","excerpt":"The resource requirements and limits of your Logging operator …","ref":"/4.2/docs/operation/requirements/","tags":"","title":"CPU and memory requirements"},{"body":"Copyright (c) 2017-2019 Banzai Cloud, Inc. Copyright (c) 2020-2023 Cisco Systems, Inc. Copyright (c) 2023- kube-logging authors\nLicensed under the Apache License, Version 2.0 (the “License”); you may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n","categories":"","description":"","excerpt":"Copyright (c) 2017-2019 Banzai Cloud, Inc. Copyright (c) 2020-2023 …","ref":"/4.2/docs/license/","tags":"","title":"License"},{"body":"For more information please click on the name\nName Description Version EventTailer Eventtailer’s main goal is to listen kubernetes events and transmit their changes to stdout. This way the logging-operator is able to process them. extensions HostTailer HostTailer’s main goal is to tail custom files and transmit their changes to stdout. This way the logging-operator is able to process them. extensions ClusterFlow ClusterFlow is the Schema for the clusterflows API v1beta1 ClusterOutput ClusterOutput is the Schema for the clusteroutputs API v1beta1 Common ImageSpec Metrics Security v1beta1 FlowSpec FlowSpec is the Kubernetes spec for Flows v1beta1 FluentbitSpec FluentbitSpec defines the desired state of Fluentbit v1beta1 FluentdSpec FluentdSpec defines the desired state of Fluentd v1beta1 Logging Logging system configuration v1beta1 NodeAgent v1beta1 OutputSpec OutputSpec defines the desired state of Output v1beta1 SyslogNGClusterFlow SyslogNGClusterFlow is the Schema for the syslog-ng clusterflows API v1beta1 SyslogNGClusterOutput SyslogNGClusterOutput is the Schema for the syslog-ng clusteroutputs API v1beta1 SyslogNGFlowSpec SyslogNGFlowSpec is the Kubernetes spec for SyslogNGFlows v1beta1 SyslogNGOutputSpec SyslogNGOutputSpec defines the desired state of SyslogNGOutput v1beta1 SyslogNGSpec SyslogNGSpec defines the desired state of SyslogNG v1beta1 ","categories":"","description":"","excerpt":"For more information please click on the name\nName Description Version …","ref":"/4.2/docs/configuration/crds/v1beta1/","tags":"","title":"Available CRDs"},{"body":"","categories":"","description":"","excerpt":"","ref":"/4.2/categories/","tags":"","title":"Categories"},{"body":"You can use the following Fluentd filters in your Flow and ClusterFlow CRDs.\n","categories":"","description":"","excerpt":"You can use the following Fluentd filters in your Flow and ClusterFlow …","ref":"/4.2/docs/configuration/plugins/filters/","tags":"","title":"Fluentd filters"},{"body":" Name Description Version EventTailer Eventtailer’s main goal is to listen kubernetes events and transmit their changes to stdout. This way the logging-operator is able to process them. extensions HostTailer HostTailer’s main goal is to tail custom files and transmit their changes to stdout. This way the logging-operator is able to process them. extensions ","categories":"","description":"","excerpt":" Name Description Version EventTailer Eventtailer’s main goal is to …","ref":"/4.2/docs/configuration/crds/extensions/","tags":"","title":"Logging extensions CRDs"},{"body":" Welcome to Logging operator! Learn More Install The Logging operator solves your logging-related problems in Kubernetes environments by automating the deployment and configuration of a Kubernetes logging pipeline.\nThe Logging operator manages the log collectors and log forwarders of your logging infrastructure, and the routing rules that specify where you want to send your different log messages. You can filter and process the incoming log messages using the flow custom resource of the log forwarder to route them to the appropriate output. The outputs are the destinations where you want to send your log messages, for example, Elasticsearch, or an Amazon S3 bucket. You can also define cluster-wide outputs and flows, for example, to use a centralized output that namespaced users can reference but cannot modify. Trusted and supported by Learn more about Logging operator! Read the Logging operator documentation.\nRead more …\nContributions welcome! We do a Pull Request contributions workflow on GitHub. New users and developers are always welcome!\nRead more …\nCome chat with us! In case you need help, you can find on Slack and Discord.\nJoin Discord …\n","categories":"","description":"","excerpt":" Welcome to Logging operator! Learn More Install The Logging operator …","ref":"/4.2/","tags":"","title":"Logging operator"},{"body":"","categories":"","description":"","excerpt":"","ref":"/4.2/docs/configuration/plugins/outputs/","tags":"","title":"Outputs"},{"body":"Supported Plugins For more information please click on the plugin name\nName Profile Description Status Version Security common Transport common Concat filters Fluentd Filter plugin to concatenate multiline log separated in multiple events. GA 2.5.0 Dedot filters Concatenate multiline log separated in multiple events GA 1.0.0 Exception Detector filters Exception Detector GA 0.0.14 ElasticsearchGenId filters Enhance K8s Metadata filters Fluentd output plugin to add extra Kubernetes metadata to the events. GA 2.0.0 Geo IP filters Fluentd GeoIP filter GA 1.3.2 Grep filters Grep events by the values GA more info Kubernetes Events Timestamp filters Fluentd Filter plugin to select particular timestamp into an additional field GA 0.1.4 Parser filters Parses a string field in event records and mutates its event record with the parsed result. GA more info Prometheus filters Prometheus Filter Plugin to count Incoming Records GA 2.0.2 Record Modifier filters Modify each event record. GA 2.1.0 Record Transformer filters Mutates/transforms incoming event streams. GA more info Stdout filters Prints events to stdout GA more info SumoLogic filters Sumo Logic collection solution for Kubernetes GA 2.3.1 Tag Normaliser filters Re-tag based on log metadata GA 0.1.1 Throttle filters A sentry plugin to throttle logs. Logs are grouped by a configurable key. When a group exceeds a configuration rate, logs are dropped for this group. GA 0.0.5 Amazon Elasticsearch outputs Fluent plugin for Amazon Elasticsearch Testing 2.4.1 Azure Storage outputs Store logs in Azure Storage GA 0.2.1 Buffer outputs Fluentd event buffer GA mode info Amazon CloudWatch outputs Send your logs to AWS CloudWatch GA 0.14.2 Datadog outputs Send your logs to Datadog Testing 0.14.1 Elasticsearch outputs Send your logs to Elasticsearch GA 5.1.1 File outputs Output plugin writes events to files GA more info Format outputs Specify how to format output record. GA more info Format rfc5424 outputs Specify how to format output record. GA more info Forward outputs Forwards events to other fluentd nodes. GA more info Google Cloud Storage outputs Store logs in Google Cloud Storage GA 0.4.0 Gelf outputs Output plugin writes events to GELF Testing 1.0.8 Http outputs Sends logs to HTTP/HTTPS endpoints. GA more info Kafka outputs Send your logs to Kafka GA 0.17.5 Amazon Kinesis Firehose outputs Fluent plugin for Amazon Kinesis Testing 3.4.2 Amazon Kinesis Stream outputs Fluent plugin for Amazon Kinesis GA 3.4.2 LogDNA outputs Send your logs to LogDNA GA 0.4.0 LogZ outputs Store logs in LogZ.io GA 0.0.21 Grafana Loki outputs Transfer logs to Loki GA 1.2.17 NewRelic Logs outputs Send logs to New Relic Logs GA 1.2.1 OpenSearch outputs Send your logs to OpenSearch GA 1.0.5 Alibaba Cloud Storage outputs Store logs the Alibaba Cloud Object Storage Service GA 0.0.2 Redis outputs Sends logs to Redis endpoints. GA 0.3.5 Amazon S3 outputs Store logs in Amazon S3 GA 1.6.1 Splunk Hec outputs Fluent Plugin Splunk Hec Release GA 1.2.9 SQS outputs Output plugin writes fluent-events as queue messages to Amazon SQS Testing v2.1.0 SumoLogic outputs Send your logs to Sumologic GA 1.8.0 Syslog outputs Output plugin writes events to syslog GA 0.9.0.rc.8 ","categories":"","description":"","excerpt":"Supported Plugins For more information please click on the plugin name …","ref":"/4.2/docs/configuration/plugins/","tags":"","title":"Supported Plugins"},{"body":"You can use the following syslog-ng filters in your SyslogNGFlow and SyslogNGClusterFlow resources.\n","categories":"","description":"","excerpt":"You can use the following syslog-ng filters in your SyslogNGFlow and …","ref":"/4.2/docs/configuration/plugins/syslog-ng-filters/","tags":"","title":"syslog-ng filters"},{"body":"SyslogNGOutput and SyslogNGClusterOutput resources have almost the same structure as Output and ClusterOutput resources, with the main difference being the number and kind of supported destinations.\nYou can use the following syslog-ng outputs in your SyslogNGOutput and SyslogNGClusterOutput resources.\n","categories":"","description":"","excerpt":"SyslogNGOutput and SyslogNGClusterOutput resources have almost the …","ref":"/4.2/docs/configuration/plugins/syslog-ng-outputs/","tags":"","title":"syslog-ng outputs"},{"body":"","categories":"","description":"","excerpt":"","ref":"/4.2/tags/","tags":"","title":"Tags"}]